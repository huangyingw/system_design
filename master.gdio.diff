diff --git a/.cursorignore b/.cursorignore
new file mode 100644
--- /dev/null
+++ ./.cursorignore
@@ -0,0 +1,73 @@
+*.#*
+*.bak
+*.bin
+*.diff
+*.dmg
+*.exe
+*.findresult
+*.fugitiveblame
+*.gch
+*.hg
+*.hi
+*.keystore
+*.lck
+*.loT
+*.ncb
+*.o
+*.orig
+*.pth
+*.pyc
+*.runresult
+*.sort
+*.sql.result
+*.suo
+*.sv[a-z]
+*.sw[a-z]
+*.tmp
+*.xls
+*.zip
+*devKeyStore
+.#*
+.DS_Store
+.cscopedb.lock
+.err
+.fuse_hidden*
+.gitconfig
+.svn
+.~lock.*#
+CVS
+build
+cscope.*
+cscopefile.*
+cscope.in.out
+cscope.out
+cscope.out.files
+cscope.po.out
+cscopedb.lock
+doc/tags
+doc/tags-ja
+error
+fav.log
+fdocs.list
+files.proj
+find.cc
+gbil.log
+gfvd.already
+includefile.conf
+ncscope.in.out
+ncscope.out
+ncscope.out.bak.in
+ncscope.out.bak.po
+ncscope.po.out
+prunefile.conf
+prunefix.conf
+run
+running.time
+setup.cfg
+~$*
+prunefix.rsync
+rsync.files
+ncscopefile.*
+gbra.log
+*.js_eslint_tmp_*.js
+gbr.log
\ No newline at end of file
diff --git a/.cursorrules b/.cursorrules
new file mode 100644
--- /dev/null
+++ ./.cursorrules
@@ -0,0 +1,123 @@
+{
+  "language": "Chinese",
+  "terminal": {
+    "shell": "zsh",
+    "commandExecution": {
+      "defaultShell": "/bin/zsh",
+      "environmentSetup": [
+        "Use zsh for all terminal commands",
+        "Ensure proper PATH is set for tools",
+        "Use absolute paths when necessary"
+      ],
+      "errorHandling": {
+        "captureOutput": true,
+        "checkExitStatus": true,
+        "retryStrategy": "Retry with modified command if initial attempt fails"
+      }
+    }
+  },
+  "plantUML": {
+    "basicStandards": {
+      "containerDefinition": "Use rectangle for all container definitions",
+      "componentDefinition": "Use component for internal component definitions",
+      "connectionHandling": "Components only inside containers, handle connections outside",
+      "noteRequirements": "Add notes with appropriate positioning",
+      "diagramRequirements": "Create aesthetically pleasing, compact diagrams with high information density",
+      "cacheAnnotation": "Annotate cache components with key-value pairs"
+    },
+    "flowAndPerformance": {
+      "connectionLabeling": "Label each connection with sequence numbers to show flow order",
+      "performanceNotes": "Add notes for performance bottlenecks and optimization suggestions"
+    },
+    "designAndLayout": {
+      "backgroundRequirement": "Avoid white backgrounds to reduce eye strain",
+      "designConsideration": "Consider optimization and scalability for long interview discussions",
+      "layoutRequirements": [
+        "Ensure clear layout with logical component grouping",
+        "Keep connections simple and minimize crossings",
+        "Maintain consistent note positioning",
+        "Use appropriate colors to distinguish different component types and flows",
+        "Enhance overall readability for extended viewing and discussion"
+      ]
+    },
+    "colorScheme": {
+      "primaryFlow": "Deep Orange",
+      "secondaryFlow": "Deep Blue",
+      "auxiliaryFlow": "Deep Yellow",
+      "managementFlow": "Deep Purple",
+      "monitoringFlow": "Deep Cyan",
+      "backgroundColor": "Maintain light background for better readability"
+    },
+    "designModificationPrinciples": [
+      "Avoid major changes to current version unless explicitly requested",
+      "Prioritize minor optimizations of existing components",
+      "Focus on functional enhancements when suggesting improvements",
+      "Seek confirmation before implementing significant modifications"
+    ],
+    "errorCheckingProcess": {
+      "checkingTiming": "After each generation or modification of puml files",
+      "checkItems": [
+        "Syntax error validation",
+        "Component connection integrity check",
+        "Naming convention verification",
+        "Layout rationality assessment",
+        "Diagram compilation validation",
+        "Connection consistency check"
+      ],
+      "errorHandling": [
+        "Automatic error detection",
+        "Provide specific error location and description",
+        "Implement automatic fixes for all detected errors",
+        "Repeat error checking and fixing until no errors remain",
+        "Verify fixes do not introduce new issues"
+      ],
+      "validationSteps": [
+        "Verify file compilation",
+        "Validate component relationships",
+        "Confirm annotation completeness",
+        "Check style compliance",
+        "Ensure all text is in English",
+        "Validate diagram structure integrity",
+        "Test automatic layout optimization"
+      ],
+      "autoFixProcess": {
+        "iterativeFixing": true,
+        "fixPriorities": [
+          "Syntax errors",
+          "Connection issues",
+          "Component placement",
+          "Style violations",
+          "Layout optimization"
+        ],
+        "verificationAfterFix": true,
+        "syntaxCheckCommand": "plantuml {filename} -syntax",
+        "renderCommand": "plantuml {filename}",
+        "commonSyntaxFixes": [
+          "Remove extra spaces and empty lines",
+          "Use proper package/component nesting",
+          "Fix color definitions",
+          "Ensure proper skinparam settings",
+          "Check component and connection definitions"
+        ],
+        "postEditChecks": [
+          "Run syntax check immediately after any edit",
+          "Verify diagram renders correctly",
+          "Check for any warning messages",
+          "Validate all components are properly connected",
+          "Ensure no orphaned elements exist"
+        ]
+      }
+    }
+  },
+  "rules": [
+    {
+      "pattern": "*.puml",
+      "commands": [
+        {
+          "command": "./check_puml.sh ${file}",
+          "trigger": "save"
+        }
+      ]
+    }
+  ]
+} 
diff --git a/.gitignore b/.gitignore
new file mode 100644
--- /dev/null
+++ ./.gitignore
@@ -0,0 +1,2 @@
+*.log
+*.png
diff --git a/advanced_load_balancing_system_with_health_check_and_dynamic_scaling.puml b/advanced_load_balancing_system_with_health_check_and_dynamic_scaling.puml
new file mode 100644
--- /dev/null
+++ ./advanced_load_balancing_system_with_health_check_and_dynamic_scaling.puml
@@ -0,0 +1,82 @@
+@startuml Load Balancing System
+
+!pragma layout dot
+!define PRIMARY_COLOR #4CAF50
+!define SECONDARY_COLOR #FFA726
+!define ACCENT_COLOR #29B6F6
+!define GRAY_COLOR #BDBDBD
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+
+allowmixing
+
+rectangle "Client Apps" as ClientApps PRIMARY_COLOR {
+    component "Web Browser" as WebBrowser
+    component "Mobile App" as MobileApp
+    component "API Client" as APIClient
+}
+
+rectangle "Load Balancer" as LoadBalancer SECONDARY_COLOR {
+    component "LB Algorithms" as LBAlgorithms
+    component "Health Check" as HealthCheck
+    component "Session Persistence" as SessionPersistence
+    component "SSL Termination" as SSLTermination
+    component "Dynamic Config" as DynamicConfig
+}
+
+rectangle "Backend Servers" as BackendServers ACCENT_COLOR {
+    component "Server 1" as Server1
+    component "Server 2" as Server2
+    component "Server 3" as Server3
+    component "Server N" as ServerN
+}
+
+database "Config Store" as ConfigStore GRAY_COLOR {
+    component "Server Config" as ServerConfig
+    component "Algorithm Settings" as AlgorithmSettings
+}
+
+database "Session Store" as SessionStore GRAY_COLOR {
+    component "User Sessions" as UserSessions
+}
+
+ClientApps -[#FF5722,thickness=2]down-> LoadBalancer : <back:#FFFFFF><color:#FF5722>1. Send Request</color></back>
+LoadBalancer -[#2196F3,thickness=2]right-> ConfigStore : <back:#FFFFFF><color:#2196F3>2. Read Config</color></back>
+LoadBalancer -[#4CAF50,thickness=2]down-> BackendServers : <back:#FFFFFF><color:#4CAF50>3. Distribute Request</color></back>
+LoadBalancer -[#9C27B0,thickness=2]left-> SessionStore : <back:#FFFFFF><color:#9C27B0>4. Store/Retrieve Session</color></back>
+BackendServers -[#FFC107,thickness=2]up-> LoadBalancer : <back:#FFFFFF><color:#FFC107>5. Return Response</color></back>
+LoadBalancer -[#795548,thickness=2]up-> ClientApps : <back:#FFFFFF><color:#795548>6. Send Response</color></back>
+
+note top of LBAlgorithms
+    Supports multiple algorithms:
+    - Round Robin
+    - Weighted Round Robin
+    - Least Connections
+    - IP Hash
+    - URL Hash
+end note
+
+note bottom of HealthCheck
+    Regularly checks backend server health
+    Automatically removes unhealthy servers
+end note
+
+note top of SessionPersistence
+    Ensures requests from the same client
+    are sent to the same backend server
+end note
+
+note top of SSLTermination
+    Offloads SSL processing
+    Reduces backend server load
+end note
+
+note bottom of DynamicConfig
+    Supports real-time configuration updates
+    and server pool management
+end note
+
+@enduml
diff --git a/api_gateway_and_microservices.puml b/api_gateway_and_microservices.puml
new file mode 100644
--- /dev/null
+++ ./api_gateway_and_microservices.puml
@@ -0,0 +1,41 @@
+@startuml API_Gateway_Microservices
+!theme toy
+skinparam backgroundColor E6E6FA
+skinparam shadowing false
+skinparam RoundCorner 10
+skinparam ArrowColor 454645
+skinparam DefaultFontName Arial
+skinparam DefaultFontSize 12
+
+component "API Gateway" as APIGateway #98FB98
+rectangle "Microservices" as Microservices #FFF0F5 {
+    component "User Service" as UserService
+    component "Product Service" as ProductService
+    component "Cart Service" as CartService
+    component "Order Service" as OrderService
+    component "Inventory Service" as InventoryService
+    component "Search Service" as SearchService
+    component "Recommendation Service" as RecommendationService
+}
+database "Database" as Database #B0E0E6
+
+' Relationships
+APIGateway -[#4169E1]down-> UserService : "1. Authenticate user"
+APIGateway -[#4169E1]down-> ProductService : "2. Browse products"
+APIGateway -[#4169E1]down-> SearchService : "3. Search products"
+APIGateway -[#4169E1]down-> CartService : "5. Add to cart"
+APIGateway -[#4169E1]down-> OrderService : "7. Place order"
+
+Microservices -[#228B22]down-> Database : "CRUD operations"
+
+' Notes
+note right of APIGateway
+  API Gateway:
+  - Request routing
+  - Load balancing
+  - Auth & authorization
+  - Rate limiting
+  - Response caching
+end note
+
+@enduml
diff --git a/api_gateway_design_with_routing_authentication_and_rate_limiting.puml b/api_gateway_design_with_routing_authentication_and_rate_limiting.puml
new file mode 100644
--- /dev/null
+++ ./api_gateway_design_with_routing_authentication_and_rate_limiting.puml
@@ -0,0 +1,82 @@
+@startuml API Gateway Design
+
+allowmixing
+
+skinparam {
+    backgroundColor #E0E0E0
+    handwritten false
+    defaultFontName Arial
+    defaultFontSize 12
+    roundcorner 10
+    shadowing false
+    ArrowColor #2C3E50
+    ComponentBorderColor #2C3E50
+    ComponentBackgroundColor #FFFFFF
+}
+
+title API Gateway Design
+
+component "Client Applications" as Client #E1F5FE
+
+rectangle "API Gateway" as Gateway #FFF3E0 {
+    component "Request Routing" as Routing
+    component "Authentication" as Auth
+    component "Rate Limiting" as RateLimit
+    component "Caching" as Cache
+    component "Load Balancing" as LoadBalance
+    component "Protocol Translation" as Protocol
+    component "Logging & Monitoring" as Logging
+}
+
+rectangle "Microservices" as Services #E8F5E9 {
+    component "Service A" as ServiceA
+    component "Service B" as ServiceB
+    component "Service C" as ServiceC
+}
+
+cloud "External Services" as External #FFEBEE {
+    component "Auth Service" as AuthService
+    database "Cache" as CacheDB
+    component "Monitoring Service" as MonitorService
+}
+
+Client -down-> Gateway : 1. API Request
+Gateway -down-> Services : 2. Route Request
+Gateway -right-> External : 3. Utilize External Services
+Services -up-> Gateway : 4. Service Response
+Gateway -up-> Client : 5. API Response
+
+note top of Gateway
+  API Gateway serves as:
+  1. Single entry point for all client requests
+  2. Request router to appropriate microservices
+  3. Protocol translator between clients and services
+  4. Security enforcer (authentication, authorization)
+  5. Rate limiter to prevent overload
+  6. Cache manager for improved performance
+  7. Load balancer for even distribution of requests
+  8. Monitoring and logging centralization point
+end note
+
+note right of Gateway
+  Performance bottlenecks:
+  1. Authentication: Consider caching tokens
+  2. Rate Limiting: Implement distributed rate limiting
+  3. Caching: Use in-memory cache for frequently accessed data
+end note
+
+note bottom of Services
+  Scalability:
+  1. Use auto-scaling for microservices
+  2. Implement service discovery
+  3. Consider using event-driven architecture
+end note
+
+note left of External
+  External dependencies:
+  1. Implement circuit breakers
+  2. Use fallback mechanisms
+  3. Monitor SLAs of external services
+end note
+
+@enduml
diff --git a/architecture_diagrams/cap_theory_diagram_with_database_examples_and_trade_offs.puml b/architecture_diagrams/cap_theory_diagram_with_database_examples_and_trade_offs.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/cap_theory_diagram_with_database_examples_and_trade_offs.puml
@@ -0,0 +1,39 @@
+@startuml
+!pragma layout smetana
+skinparam backgroundColor #F0F0F0
+allowmixing
+
+' 定义 CAP 理论的三个组成部分
+rectangle "Consistency" as C #E6F3FF
+rectangle "Availability" as A #FFF0E6
+rectangle "Partition Tolerance" as P #E6FFE6
+
+' 创建关联性说明
+note right of C
+  Ensures data consistency
+  across all nodes in the system
+end note
+
+note left of A
+  System's ability to respond
+  to user requests
+end note
+
+note right of P
+  System's capability to operate
+  during network partitions
+end note
+
+' 描述它们之间的关系
+C -[#FF0000,thickness=2]down-> A : <back:#FFFFFF><color:#FF0000>1. Cannot be fully satisfied simultaneously</color></back>
+A -[#00AA00,thickness=2]right-> P : <back:#FFFFFF><color:#00AA00>2. Trade-off required</color></back>
+P -[#0000FF,thickness=2]up-> C : <back:#FFFFFF><color:#0000FF>3. Must choose two out of three</color></back>
+
+note bottom of P
+  Performance bottleneck:
+  Achieving all three properties
+  simultaneously is impossible
+  in distributed systems
+end note
+
+@enduml
diff --git a/architecture_diagrams/data_storage_options_comparison_and_use_cases.puml b/architecture_diagrams/data_storage_options_comparison_and_use_cases.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/data_storage_options_comparison_and_use_cases.puml
@@ -0,0 +1,34 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+skinparam rectangle {
+    BackgroundColor<<数据库>> Wheat
+    BackgroundColor<<Elasticsearch>> LightGreen
+}
+
+package "数据存储方案选择" {
+    rectangle 数据库 <<数据库>> as DB {
+        note right of DB
+            结构化数据
+            事务支持
+            ACID属性
+            复杂查询能力
+            数据完整性和安全性
+        end note
+    }
+
+    rectangle Elasticsearch <<Elasticsearch>> as ES {
+        note right of ES
+            非结构化/半结构化数据
+            全文搜索和分析
+            扩展性和高可用性
+            实时处理
+            灵活性
+        end note
+    }
+}
+
+[应用场景] ..> DB : 需要\n- 结构化数据处理\n- 复杂事务\n- 数据完整性\n- 复杂查询
+[应用场景] ..> ES : 需要\n- 全文搜索\n- 实时分析\n- 大规模数据集\n- 灵活的数据模型
+
+@enduml
diff --git a/architecture_diagrams/database_sharding_analysis.puml b/architecture_diagrams/database_sharding_analysis.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/database_sharding_analysis.puml
@@ -0,0 +1,58 @@
+@startuml
+skinparam backgroundColor #FFFAF0
+skinparam linetype ortho
+
+|#E6E6FA|Data Analysis Layer|
+start
+:Analyze query logs;
+:Understand business requirements;
+
+|#FFE4E1|Performance Monitoring|
+if (Have performance monitoring tools?) then (Yes)
+  :Use performance monitoring tools;
+else (No)
+  :Consider introducing monitoring tools;
+endif
+
+|#E6E6FA|Optimization Analysis Layer|
+fork
+  :Review historical optimization measures;
+fork again
+  :Application code review;
+end fork
+
+fork
+  :Collect user feedback and data;
+fork again
+  :Comprehensive analysis of query patterns;
+end fork
+
+|#98FB98|Query Optimization Layer|
+:Identify common query types;
+note right
+  Example SQL query:
+  SELECT * FROM orders 
+  WHERE customer_id = ? 
+  AND order_date BETWEEN ? AND ?;
+end note
+
+|#FFD700|Architecture Design Layer|
+:Choose shard key based on query patterns;
+note right
+  Select customer_id and order_date 
+  as composite shard key
+end note
+
+:Design sharding architecture;
+note right
+  - NoSQL: Set up sharding rules
+  - RDBMS: Configure sharding middleware
+end note
+
+|#FF6347|Performance Evaluation Layer|
+:Conduct load testing;
+:Identify potential bottlenecks;
+:Develop scaling strategies;
+
+stop
+@enduml
diff --git a/architecture_diagrams/distributed_message_queue_system_with_producers_consumers_and_brokers.puml b/architecture_diagrams/distributed_message_queue_system_with_producers_consumers_and_brokers.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/distributed_message_queue_system_with_producers_consumers_and_brokers.puml
@@ -0,0 +1,78 @@
+@startuml
+!pragma layout dot
+skinparam backgroundColor #EEEEEE
+allowmixing
+
+rectangle "Kafka Cluster" as KafkaCluster #LightBlue {
+    rectangle "Broker1" as Broker1 #PaleGreen
+    rectangle "Broker2" as Broker2 #PaleGreen
+    rectangle "Broker3" as Broker3 #PaleGreen
+    
+    rectangle "Zookeeper Ensemble" as ZookeeperEnsemble #LightYellow {
+        component "Zookeeper1" as Zookeeper1
+        component "Zookeeper2" as Zookeeper2
+        component "Zookeeper3" as Zookeeper3
+    }
+
+    rectangle "Topic1" as Topic1 #LightPink {
+        component "Partition1_1" as Partition1_1
+        component "Partition1_2" as Partition1_2
+    }
+
+    rectangle "Topic2" as Topic2 #LightPink {
+        component "Partition2_1" as Partition2_1
+        component "Partition2_2" as Partition2_2
+    }
+}
+
+rectangle "Producers" as Producers #LightSkyBlue {
+    component "Producer1" as Producer1
+    component "Producer2" as Producer2
+}
+
+rectangle "Consumers" as Consumers #LightSalmon {
+    component "Consumer1" as Consumer1
+    component "Consumer2" as Consumer2
+}
+
+' Connections
+Producer1 -[#Blue,thickness=2]-> Topic1 : <color:#Blue>1. Produce messages
+Producer2 -[#Purple,thickness=2]-> Topic2 : <color:#Purple>2. Produce messages
+
+Topic1 -[#Green,thickness=2]-> Partition1_1 : <color:#Green>3. Distribute
+Topic1 -[#Green,thickness=2]-> Partition1_2 : <color:#Green>4. Distribute
+Topic2 -[#Orange,thickness=2]-> Partition2_1 : <color:#Orange>5. Distribute
+Topic2 -[#Orange,thickness=2]-> Partition2_2 : <color:#Orange>6. Distribute
+
+Partition1_1 -[#Red,thickness=2]-> Broker1 : <color:#Red>7. Leader
+Partition1_1 .[#Gray,thickness=1]-> Broker2 : <color:#Gray>8. Follower
+Partition1_1 .[#Gray,thickness=1]-> Broker3 : <color:#Gray>9. Follower
+
+Consumer1 -[#Brown,thickness=2]-> Partition1_1 : <color:#Brown>10. Consume
+Consumer1 -[#Brown,thickness=2]-> Partition1_2 : <color:#Brown>11. Consume
+Consumer2 -[#Magenta,thickness=2]-> Partition2_1 : <color:#Magenta>12. Consume
+Consumer2 -[#Magenta,thickness=2]-> Partition2_2 : <color:#Magenta>13. Consume
+
+Broker1 -[#Black,thickness=1]-> ZookeeperEnsemble : <color:#Black>14. Coordinate
+Broker2 -[#Black,thickness=1]-> ZookeeperEnsemble : <color:#Black>15. Coordinate
+Broker3 -[#Black,thickness=1]-> ZookeeperEnsemble : <color:#Black>16. Coordinate
+
+note right of ZookeeperEnsemble
+  Stores metadata:
+  - Broker list
+  - Topic configurations
+  - Partition leaders
+end note
+
+note bottom of Consumers
+  Consumer offsets stored in 
+  Kafka (modern) or Zookeeper (legacy)
+end note
+
+note "Performance bottleneck:\nZookeeper writes for metadata updates" as N1
+ZookeeperEnsemble .. N1
+
+note "Optimization:\nIncrease partitions for better parallelism" as N2
+Topic1 .. N2
+
+@enduml
diff --git a/architecture_diagrams/kafka_cluster_architecture_overview.puml b/architecture_diagrams/kafka_cluster_architecture_overview.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kafka_cluster_architecture_overview.puml
@@ -0,0 +1,89 @@
+@startuml Kafka Cluster Architecture Overview
+!pragma layout dot
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam defaultFontColor #333333
+skinparam roundcorner 20
+skinparam shadowing false
+skinparam ArrowColor #666666
+skinparam ArrowThickness 2
+allowmixing
+
+' Color definitions
+!define ZOOKEEPER_COLOR E6E6FA
+!define BROKER_COLOR E0FFFF
+!define COMPONENT_COLOR F0FFF0
+!define MONITORING_COLOR FFF0F5
+!define SECURITY_COLOR FFE4E1
+
+' Zookeeper Ensemble
+rectangle "Zookeeper Ensemble" as ZK #ZOOKEEPER_COLOR {
+    component "ZK Node 1" as ZK1
+    component "ZK Node 2" as ZK2
+    component "ZK Node 3" as ZK3
+}
+
+' Kafka Cluster
+rectangle "Kafka Cluster" as KC {
+    rectangle "Broker 1" as B1 #BROKER_COLOR {
+        component "Request Handler" as RH1 #COMPONENT_COLOR
+        component "Network Processor" as NP1 #COMPONENT_COLOR
+    }
+    rectangle "Broker 2" as B2 #BROKER_COLOR {
+        component "Request Handler" as RH2 #COMPONENT_COLOR
+        component "Network Processor" as NP2 #COMPONENT_COLOR
+    }
+    rectangle "Broker 3 (Controller)" as B3 #BROKER_COLOR {
+        component "Controller" as CTRL #COMPONENT_COLOR
+        component "Replication Manager" as RM #COMPONENT_COLOR
+        component "Log Manager" as LM #COMPONENT_COLOR
+    }
+    rectangle "Broker 4" as B4 #BROKER_COLOR {
+        component "Request Handler" as RH4 #COMPONENT_COLOR
+        component "Network Processor" as NP4 #COMPONENT_COLOR
+    }
+}
+
+' Monitoring & Management
+rectangle "Monitoring & Management" as MM #MONITORING_COLOR {
+    component "Kafka Manager" as KM
+    component "JMX Exporter" as JMX
+    component "Prometheus" as PROM
+    component "Grafana" as GRAF
+}
+
+' Security Layer
+rectangle "Security Layer" as SL #SECURITY_COLOR {
+    component "SSL/TLS" as SSL
+    component "SASL" as SASL
+    component "ACL" as ACL
+}
+
+' Connections
+ZK -[#4B0082,thickness=2]down-> KC : <color:#4B0082><b>1. Coordinates</b></color>
+B3 -[#006400,thickness=2]up-> ZK : <color:#006400><b>2. Manages</b></color>
+KC -[#4682B4,thickness=2]right-> MM : <color:#4682B4><b>3. Monitored by</b></color>
+KC -[#8B4513,thickness=2]-> SL : <color:#8B4513><b>4. Secured by</b></color>
+
+' Notes
+note right of B3 #FFFAFA
+  <b>Controller Broker:</b>
+  Manages cluster-wide operations
+  Performance bottleneck: Consider implementing
+  multi-controller for high availability
+end note
+
+note bottom of MM #FFFAFA
+  Provides insights and facilitates cluster management
+  Optimization: Implement automated alerting and
+  self-healing mechanisms
+end note
+
+note bottom of SL #FFFAFA
+  Ensures secure communication and access control
+  Consider implementing rotating keys and
+  certificate management for enhanced security
+end note
+
+@enduml
\ No newline at end of file
diff --git a/architecture_diagrams/kafka_cluster_monitoring_and_management_tools_with_prometheus_integration.puml b/architecture_diagrams/kafka_cluster_monitoring_and_management_tools_with_prometheus_integration.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kafka_cluster_monitoring_and_management_tools_with_prometheus_integration.puml
@@ -0,0 +1,14 @@
+@startuml
+skinparam backgroundColor #F0F8FF
+
+class Broker << (K,orchid) >>
+class KafkaManager << (M,orange) >>
+class KafkaMonitor << (M,orange) >>
+class Prometheus << (M,orange) >>
+
+Broker --> KafkaManager : manage
+Broker --> KafkaMonitor : monitor
+Broker --> Prometheus : collect metrics
+
+note top of Broker : 使用工具进行Kafka集群的管理和监控
+@enduml
diff --git a/architecture_diagrams/kafka_data_flow_detailed_process.puml b/architecture_diagrams/kafka_data_flow_detailed_process.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kafka_data_flow_detailed_process.puml
@@ -0,0 +1,62 @@
+@startuml
+skinparam backgroundColor #E8E8E8
+
+' Define components
+rectangle "Producer" as Producer #4CAF50
+rectangle "Broker Cluster" as BrokerCluster {
+    rectangle "Broker 1" as Broker1 #FF9800
+    rectangle "Broker 2" as Broker2 #FF9800
+    rectangle "Broker 3" as Broker3 #FF9800
+}
+rectangle "ZooKeeper Ensemble" as ZooKeeper #9C27B0
+rectangle "Consumer Group" as ConsumerGroup {
+    rectangle "Consumer 1" as Consumer1 #2196F3
+    rectangle "Consumer 2" as Consumer2 #2196F3
+}
+rectangle "Schema Registry" as SchemaRegistry #E91E63
+rectangle "Monitoring & Metrics" as Monitoring #795548
+
+' Define connections
+Producer -[#4CAF50,thickness=2]-> Broker1 : 1. Send Message
+Producer -[#4CAF50,dashed]-> SchemaRegistry : 2. Validate Schema
+Broker1 -[#FF9800,thickness=2]-> Broker2 : 3. Replicate
+Broker1 -[#FF9800,thickness=2]-> Broker3 : 4. Replicate
+Broker1 -[#2196F3,thickness=2]-> Consumer1 : 5. Fetch
+Broker2 -[#2196F3,thickness=2]-> Consumer2 : 6. Fetch
+ZooKeeper -[#9C27B0,dashed]-> BrokerCluster : 7. Manage Metadata
+ZooKeeper -[#9C27B0,dashed]-> ConsumerGroup : 8. Coordinate
+Monitoring -[#795548,dashed]-> BrokerCluster : 9. Collect Metrics
+Monitoring -[#795548,dashed]-> ConsumerGroup : 10. Collect Metrics
+
+' Add notes
+note top of Producer
+    Producers send messages to specified topics
+    Can choose synchronous or asynchronous sending
+end note
+
+note bottom of BrokerCluster
+    Messages are partitioned and replicated across brokers
+    Ensures high availability and fault tolerance
+end note
+
+note bottom of ConsumerGroup
+    Consumers fetch messages from brokers
+    Supports various consumption modes
+end note
+
+note right of SchemaRegistry
+    Manages and validates message schemas
+    Ensures data compatibility
+end note
+
+note right of ZooKeeper
+    Manages cluster metadata
+    Coordinates Brokers and Consumers
+end note
+
+note left of Monitoring
+    Monitors system performance and health
+    Collects key metrics
+end note
+
+@enduml
diff --git a/architecture_diagrams/kafka_fault_handling_and_recovery.puml b/architecture_diagrams/kafka_fault_handling_and_recovery.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kafka_fault_handling_and_recovery.puml
@@ -0,0 +1,42 @@
+@startuml
+skinparam backgroundColor #FFFAFA
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam class {
+    BackgroundColor white
+    BorderColor black
+    FontSize 14
+}
+
+class Broker1 << (K,orchid) >> {
+    Role: Leader
+    Status: Fails
+    Action: Leader election and failover via Zookeeper
+}
+class Broker2 << (K,orchid) >> {
+    Role: Leader
+    Status: Wins the election
+    Action: Leader election and failover via Zookeeper
+}
+class Broker3 << (K,orchid) >> {
+    Role: Follower
+    Status: Takes over
+    Action: Leader election and failover via Zookeeper
+}
+class Broker4 << (K,orchid) >> {
+    Role: Standby
+    Status: Standby for failover
+}
+class Zookeeper << (Z,yellow) >> {
+    Manages leader election
+}
+
+Broker1 --> Zookeeper : leader election
+Broker2 --> Zookeeper : leader election
+Broker3 --> Zookeeper : leader election
+
+Broker1 -[#red]> Broker4 : fails
+Broker2 -[#blue]> Broker4 : wins election
+Broker3 -[#blue]> Broker4 : takes over
+
+@enduml
diff --git a/architecture_diagrams/kafka_producer_consumer_detailed_structure.puml b/architecture_diagrams/kafka_producer_consumer_detailed_structure.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kafka_producer_consumer_detailed_structure.puml
@@ -0,0 +1,125 @@
+@startuml
+skinparam backgroundColor #F5F5F5
+skinparam packageStyle rectangle
+allowmixing
+skinparam linetype ortho
+
+title Kafka Producer and Consumer Detailed Architecture
+
+rectangle "Kafka System" {
+    
+    rectangle "Producer Components" as ProducerComponents {
+        component "Producer" as Producer #87CEFA
+        component "ProducerInterceptor" as ProducerInterceptor #87CEFA
+        component "Serializer" as Serializer #87CEFA
+        component "Partitioner" as Partitioner #87CEFA
+        component "RecordAccumulator" as RecordAccumulator #87CEFA
+        component "Sender" as Sender #87CEFA
+
+        Producer -[#FF6347,thickness=2]-> ProducerInterceptor : 1
+        ProducerInterceptor -[#FF6347,thickness=2]-> Serializer : 2
+        Serializer -[#FF6347,thickness=2]-> Partitioner : 3
+        Partitioner -[#FF6347,thickness=2]-> RecordAccumulator : 4
+        RecordAccumulator -[#FF6347,thickness=2]-> Sender : 5
+    }
+
+    rectangle "Consumer Components" as ConsumerComponents {
+        component "Consumer" as Consumer #FFA07A
+        component "Coordinator" as Coordinator #FFA07A
+        component "PartitionAssignor" as PartitionAssignor #FFA07A
+        component "FetchManager" as FetchManager #FFA07A
+        component "Deserializer" as Deserializer #FFA07A
+        component "ConsumerInterceptor" as ConsumerInterceptor #FFA07A
+
+        Consumer -[#4682B4,thickness=2]-> Coordinator : 1
+        Coordinator -[#4682B4,thickness=2]-> PartitionAssignor : 2
+        PartitionAssignor -[#4682B4,thickness=2]-> FetchManager : 3
+        FetchManager -[#4682B4,thickness=2]-> Deserializer : 4
+        Deserializer -[#4682B4,thickness=2]-> ConsumerInterceptor : 5
+    }
+
+    component "Broker" as Broker #98FB98
+
+    Sender -[#FF6347,thickness=2]-> Broker : <color:#FF6347>6. Send messages</color>
+    Broker -[#4682B4,thickness=2]-> FetchManager : <color:#4682B4>6. Deliver messages</color>
+}
+
+note right of Producer
+  Initiates message production process
+  <b>Bottleneck:</b> High message volume
+  <b>Optimize:</b> Increase batch size, use async sends
+end note
+
+note right of ProducerInterceptor
+  Intercepts and modifies messages
+  <b>Bottleneck:</b> Complex interception logic
+  <b>Optimize:</b> Keep interception logic simple
+end note
+
+note right of Serializer
+  Converts messages to byte arrays
+  <b>Bottleneck:</b> Inefficient serialization
+  <b>Optimize:</b> Use efficient formats (e.g., Avro)
+end note
+
+note right of Partitioner
+  Determines message partition
+  <b>Bottleneck:</b> Uneven partition distribution
+  <b>Optimize:</b> Implement custom partitioner if needed
+end note
+
+note right of RecordAccumulator
+  Batches messages for efficiency
+  <b>Bottleneck:</b> Memory usage for large batches
+  <b>Optimize:</b> Fine-tune batch.size and linger.ms
+end note
+
+note right of Sender
+  Sends batched messages to broker
+  <b>Bottleneck:</b> Network I/O
+  <b>Optimize:</b> Increase buffer.memory, use compression
+end note
+
+note left of Consumer
+  Initiates message consumption process
+  <b>Bottleneck:</b> Slow message processing
+  <b>Optimize:</b> Increase consumer threads
+end note
+
+note left of Coordinator
+  Manages consumer group membership
+  <b>Bottleneck:</b> Frequent rebalances
+  <b>Optimize:</b> Tune session.timeout.ms
+end note
+
+note left of PartitionAssignor
+  Assigns partitions to consumers
+  <b>Bottleneck:</b> Uneven partition assignment
+  <b>Optimize:</b> Use appropriate assignment strategy
+end note
+
+note left of FetchManager
+  Fetches messages from broker
+  <b>Bottleneck:</b> Fetch size too small
+  <b>Optimize:</b> Increase fetch.min.bytes
+end note
+
+note left of Deserializer
+  Converts byte arrays back to messages
+  <b>Bottleneck:</b> Inefficient deserialization
+  <b>Optimize:</b> Use efficient deserialization methods
+end note
+
+note left of ConsumerInterceptor
+  Intercepts and processes consumed messages
+  <b>Bottleneck:</b> Complex interception logic
+  <b>Optimize:</b> Keep interception logic simple
+end note
+
+note bottom of Broker
+  Stores and manages message topics and partitions
+  <b>Bottleneck:</b> Disk I/O, network bandwidth
+  <b>Optimize:</b> Use SSDs, RAID, increase num.io.threads, network capacity
+end note
+
+@enduml
diff --git a/architecture_diagrams/kafka_security_architecture_with_authentication_authorization_and_encryption.puml b/architecture_diagrams/kafka_security_architecture_with_authentication_authorization_and_encryption.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kafka_security_architecture_with_authentication_authorization_and_encryption.puml
@@ -0,0 +1,17 @@
+@startuml
+skinparam backgroundColor #FFF8DC
+
+package "Security Components" {
+    class Broker << (K,orchid) >>
+    class Client << (C,red) >>
+    class Authentication << (A,green) >>
+    class Authorization << (A,green) >>
+    class Encryption << (A,green) >>
+}
+
+Broker --> Client : authenticate
+Broker --> Authorization : authorize
+Broker --> Encryption : encrypt
+
+note top of Broker : Kafka的安全机制包括认证、授权和加密
+@enduml
diff --git a/architecture_diagrams/kubernetes_persistent_storage_lifecycle_with_pv_pvc_and_pod_interaction.puml b/architecture_diagrams/kubernetes_persistent_storage_lifecycle_with_pv_pvc_and_pod_interaction.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/kubernetes_persistent_storage_lifecycle_with_pv_pvc_and_pod_interaction.puml
@@ -0,0 +1,22 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+actor 管理员
+actor 用户
+participant "PersistentVolume\n(PV)" as PV
+participant "PersistentVolumeClaim\n(PVC)" as PVC
+participant "Pod"
+
+管理员 -> PV : 创建PV
+用户 -> PVC : 创建PVC
+PVC -> PV : 匹配到PV
+
+用户 -> Pod : 创建并配置Pod
+Pod -> PVC : 请求挂载PVC
+
+alt Pod崩溃
+    用户 -> Pod : Pod重启
+    Pod -> PVC : 重新挂载PVC
+end
+
+@enduml
diff --git a/architecture_diagrams/microservices_with_databases.puml b/architecture_diagrams/microservices_with_databases.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/microservices_with_databases.puml
@@ -0,0 +1,105 @@
+@startuml MongoDB Sharded Cluster Architecture
+
+skinparam backgroundColor #F0F0F0
+skinparam handwritten false
+skinparam monochrome false
+skinparam packageStyle rectangle
+skinparam packageBackgroundColor #EEEBDC
+skinparam componentStyle uml2
+
+!define MICROSERVICE(x) component x <<Microservice>> #ADD1B2
+!define MONGOS(x) database x <<Mongos>> #A9DCDF
+!define SHARD(x) database x <<Shard>> #FFBD9D
+!define REPLICANODE(x) database x <<ReplicaNode>> #FFF59D
+!define CONFIGSVR(x) database x <<ConfigServer>> #C8F2AD
+
+skinparam component {
+    FontColor #000000
+    BackgroundColor<<Microservice>> #ADD1B2
+    BorderColor<<Microservice>> #94ABD9
+}
+
+' Application Layer
+package "Application Layer" {
+    MICROSERVICE(Service1)
+    MICROSERVICE(Service2)
+    note right of Service2
+        Scalable
+        Can add more service instances as needed
+    end note
+}
+
+' Router Layer
+package "Router Layer" {
+    MONGOS(Mongos1)
+    MONGOS(Mongos2)
+    note right of Mongos2
+        Scalable
+        Multiple Mongos instances provide
+        load balancing and high availability
+    end note
+}
+
+' Data Storage Layer
+package "Data Storage Layer" {
+    package "Shard 1 (Replica Set)" {
+        SHARD(Shard1_Primary)
+        REPLICANODE(Shard1_Secondary1)
+        REPLICANODE(Shard1_Secondary2)
+    }
+    package "Shard 2 (Replica Set)" {
+        SHARD(Shard2_Primary)
+        REPLICANODE(Shard2_Secondary1)
+        REPLICANODE(Shard2_Secondary2)
+    }
+    package "Shard 3 (Replica Set)" {
+        SHARD(Shard3_Primary)
+        REPLICANODE(Shard3_Secondary1)
+        REPLICANODE(Shard3_Secondary2)
+    }
+    note right of Shard3_Secondary2
+        Scalable
+        Can add more shards to increase
+        storage capacity and processing power
+    end note
+}
+
+' Config Servers
+package "Config Server Replica Set" {
+    CONFIGSVR(ConfigServer1)
+    CONFIGSVR(ConfigServer2)
+    CONFIGSVR(ConfigServer3)
+    note right of ConfigServer3
+        Fixed number (usually 3)
+        Provides high availability
+        and data consistency
+    end note
+}
+
+' Connections
+Service1 -down-> Mongos1 : Query
+Service2 -down-> Mongos2 : Query
+
+Mongos1 -down-> Shard1_Primary
+Mongos1 -down-> Shard2_Primary
+Mongos1 -down-> Shard3_Primary
+Mongos2 -down-> Shard1_Primary
+Mongos2 -down-> Shard2_Primary
+Mongos2 -down-> Shard3_Primary
+
+Mongos1 -right-> ConfigServer1
+Mongos1 -right-> ConfigServer2
+Mongos1 -right-> ConfigServer3
+Mongos2 -left-> ConfigServer1
+Mongos2 -left-> ConfigServer2
+Mongos2 -left-> ConfigServer3
+
+' Replica Set Internal Replication
+Shard1_Primary <-right-> Shard1_Secondary1 : Replicate
+Shard1_Primary <-down-> Shard1_Secondary2 : Replicate
+Shard2_Primary <-right-> Shard2_Secondary1 : Replicate
+Shard2_Primary <-down-> Shard2_Secondary2 : Replicate
+Shard3_Primary <-right-> Shard3_Secondary1 : Replicate
+Shard3_Primary <-down-> Shard3_Secondary2 : Replicate
+
+@enduml
diff --git a/architecture_diagrams/rate_limiting_system_for_api_and_ddos_protection_with_token_and_leaky_bucket.puml b/architecture_diagrams/rate_limiting_system_for_api_and_ddos_protection_with_token_and_leaky_bucket.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/rate_limiting_system_for_api_and_ddos_protection_with_token_and_leaky_bucket.puml
@@ -0,0 +1,68 @@
+@startuml
+skinparam backgroundColor #ECECEC
+skinparam linetype ortho
+skinparam rectangle {
+  roundCorner 25
+}
+
+rectangle "Client\nApplications" as CLIENT #LightBlue
+rectangle "API Gateway /\nLoad Balancer" as LB #LightGreen
+rectangle "Rate Limiter\nService" as RLS #Orange
+rectangle "Rules Engine" as RE #Yellow
+rectangle "Redis Cache" as RC #Pink
+rectangle "PostgreSQL" as PG #LightCyan
+rectangle "Admin Panel" as ADMIN #LightGray
+
+CLIENT -down-> LB : API Requests
+LB -down-> RLS : Route Requests
+RLS -right-> RE : 1. Check Rules
+RE -down-> RC : 2. Get/Set Counters
+RE <-up-> PG : 3. Sync Rules\n(Periodic)
+ADMIN -down-> PG : Manage Rules
+
+note right of CLIENT
+  Various client applications
+  making API requests
+end note
+
+note right of LB
+  - Distributes requests
+  - Basic rate limiting (optional)
+  - SSL termination
+end note
+
+note right of RLS
+  - Enforces rate limits
+  - Scales horizontally
+  - Stateless design
+end note
+
+note right of RE
+  - Caches active rules
+  - Fast rule evaluation
+  - Periodic sync with PostgreSQL
+end note
+
+note right of RC
+  Key structure examples:
+  - user:{userID}:endpoint:{endpoint}:counter
+  - ip:{ipAddress}:global:counter
+  TTL-based expiration for counters
+end note
+
+note right of PG
+  Tables:
+  - rate_limit_rules
+  - user_data
+  - audit_logs
+  Indexes on frequently queried fields
+end note
+
+note left of ADMIN
+  Web interface for:
+  - Rule management
+  - Analytics dashboard
+  - System configuration
+end note
+
+@enduml
diff --git a/architecture_diagrams/spark_data_partitioning_and_optimization_framework.puml b/architecture_diagrams/spark_data_partitioning_and_optimization_framework.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/spark_data_partitioning_and_optimization_framework.puml
@@ -0,0 +1,62 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+class "Data Partitioning" as Partitioning {
+  + Hash Partitioning
+  + Range Partitioning
+  + Custom Partitioning
+}
+
+class "Partitioning Strategies" as Strategies {
+  + Define number of partitions
+  + Select partitioning method
+  + Consider data skewness
+}
+
+class "Optimization Techniques" as Optimization {
+  + Coalesce for reducing partitions
+  + Repartition for increasing or reshuffling partitions
+  + Persist partitions in memory
+  + Adjust parallelism
+}
+
+class "Data Skewness Handling" as Skewness {
+  + Identify skewed keys
+  + Use salting technique
+  + Split skewed partitions
+}
+
+class "RDD/DataFrame" as Data {
+  + Data is divided into partitions
+}
+
+Partitioning --> Data : Applies to
+Strategies --> Partitioning : Guides
+Optimization --> Strategies : Implements
+Skewness --> Optimization : Part of
+
+note right of Partitioning
+  Partitioning is how Spark splits data
+  into chunks that can be processed in parallel.
+  Different methods are available based on data characteristics.
+end note
+
+note left of Strategies
+  Strategies include how to choose the partitioning method
+  and the number of partitions, considering data characteristics
+  and processing requirements.
+end note
+
+note right of Optimization
+  Optimization techniques involve methods
+  to improve the efficiency of data processing
+  through effective partition management.
+end note
+
+note left of Skewness
+  Handling data skewness involves techniques
+  to evenly distribute data across partitions,
+  especially when certain keys are over-represented.
+end note
+
+@enduml
diff --git a/architecture_diagrams/spark_kubernetes_architecture.puml b/architecture_diagrams/spark_kubernetes_architecture.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/spark_kubernetes_architecture.puml
@@ -0,0 +1,126 @@
+@startuml
+!pragma layout dot
+skinparam backgroundColor #F0F0F0
+allowmixing
+
+rectangle "Spark Application" as SparkApp #FDD835 {
+    component "Driver Program" as Driver
+    component "SparkContext" as SC
+    component "RDD Operations" as RDDOps
+    component "DAG Scheduler" as DAG
+    component "Task Scheduler" as TS
+}
+
+rectangle "Kubernetes Cluster" as K8sCluster #7CB342 {
+    component "Kubernetes Master" as K8sMaster
+    component "Kubernetes API" as K8sAPI
+}
+
+rectangle "Worker Node 1" as Node1 #4A86E8 {
+    component "Pod 1" as Pod1
+    component "Executor 1" as E1
+    component "Cache 1" as C1
+    rectangle "Data Processing 1" as LDP1 #8E24AA {
+        component "Map Task 1.1" as MT11
+        component "Reduce Task 1.2" as RT12
+    }
+}
+
+rectangle "Worker Node 2" as Node2 #4A86E8 {
+    component "Pod 2" as Pod2
+    component "Executor 2" as E2
+    component "Cache 2" as C2
+    rectangle "Data Processing 2" as LDP2 #8E24AA {
+        component "Map Task 2.1" as MT21
+        component "Reduce Task 2.2" as RT22
+    }
+}
+
+component "Final Reduce" as FinalReduce #FB8C00
+
+rectangle "Data Storage" as Storage #E53935 {
+    component "HDFS" as HDFS
+    component "Elasticsearch" as ES
+    component "Other Sources" as Others
+}
+
+Driver -[#78909C,thickness=2]-> SC : <color:#78909C>1. Initialize
+SC -[#78909C,thickness=2]-> RDDOps : <color:#78909C>2. Define
+RDDOps -[#78909C,thickness=2]-> DAG : <color:#78909C>3. Convert
+SC -[#78909C,thickness=2]-> TS : <color:#78909C>4. Submit
+SC -[#78909C,thickness=2]-> K8sAPI : <color:#78909C>5. Deploy
+DAG -[#78909C,thickness=2]-> TS : <color:#78909C>6. Plan
+TS -[#78909C,thickness=2]-> K8sAPI : <color:#78909C>7. Request
+K8sMaster -[#7CB342,thickness=2]-> Pod1 : <color:#7CB342>8. Schedule
+K8sMaster -[#7CB342,thickness=2]-> Pod2 : <color:#7CB342>9. Schedule
+Pod1 -[#4A86E8,thickness=2]-> E1 : <color:#4A86E8>10. Run
+E1 -[#4A86E8,thickness=2]-> C1 : <color:#4A86E8>11. Cache
+E1 -[#8E24AA,thickness=2]-> MT11 : <color:#8E24AA>12. Map
+MT11 -[#8E24AA,thickness=2]-> RT12 : <color:#8E24AA>13. Reduce
+Pod2 -[#4A86E8,thickness=2]-> E2 : <color:#4A86E8>14. Run
+E2 -[#4A86E8,thickness=2]-> C2 : <color:#4A86E8>15. Cache
+E2 -[#8E24AA,thickness=2]-> MT21 : <color:#8E24AA>16. Map
+MT21 -[#8E24AA,thickness=2]-> RT22 : <color:#8E24AA>17. Reduce
+RT12 -[#FB8C00,thickness=2]-> FinalReduce : <color:#FB8C00>18. Aggregate
+RT22 -[#FB8C00,thickness=2]-> FinalReduce : <color:#FB8C00>19. Aggregate
+FinalReduce -[#E53935,thickness=2]-> Storage : <color:#E53935>20. Store
+
+note right of RDDOps
+  RDD Operations:
+  - Define processing logic
+  - Transformations and actions
+  - Lazy evaluation
+  - Distributed processing basis
+end note
+
+note right of DAG
+  DAG (Directed Acyclic Graph):
+  - Represents processing steps
+  - Optimizes execution
+  - Ensures fault-tolerance
+  - No cyclic dependencies
+end note
+
+note right of ES
+  Elasticsearch:
+  - Full-text search
+  - Real-time analysis
+  - Log and event data processing
+  - Data enrichment
+end note
+
+note top of C1
+  Cache:
+  key: value pairs
+  Accelerates data access
+end note
+
+note right of Storage
+  External Storage:
+  Long-term data persistence
+  Fault tolerance
+end note
+
+note right of K8sMaster
+  Kubernetes Master:
+  - Manages cluster
+  - Schedules and orchestrates Pods
+  - Overall cluster management
+end note
+
+note right of E1
+  Executors:
+  - Run in Kubernetes Pods
+  - Managed by Kubernetes
+  - Execute tasks
+  - Local caching capability
+end note
+
+note right of FinalReduce
+  Final Reduce:
+  - Aggregates results
+  - Performs final processing
+  - Prepares for storage
+end note
+
+@enduml
\ No newline at end of file
diff --git a/architecture_diagrams/spark_mapreduce_comparison.puml b/architecture_diagrams/spark_mapreduce_comparison.puml
new file mode 100644
--- /dev/null
+++ ./architecture_diagrams/spark_mapreduce_comparison.puml
@@ -0,0 +1,64 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+skinparam class {
+    BackgroundColor LightYellow
+    ArrowColor Brown
+    BorderColor Brown
+}
+
+skinparam packageStyle rectangle
+
+package "Spark System" {
+class SparkContext {
+  - initialize cluster
+  - distribute data
+  - execute tasks
+}
+
+class RDD {
+  - parallelize data
+  - lineage information
+  - transformations
+  - actions
+}
+
+class Transformation {
+  - map()
+  - filter()
+  - flatMap()
+  - groupBy()
+}
+
+class Action {
+  - reduce()
+  - collect()
+  - count()
+  - take()
+}
+}
+
+package "MapReduce Model" {
+class MapReduceModel {
+  + Map()
+  + Reduce()
+}
+
+class MapOperation {
+}
+
+class ReduceOperation {
+}
+}
+
+SparkContext -down-> RDD : creates >
+RDD -down-> Transformation : performs >
+RDD -down-> Action : performs >
+Transformation -[hidden]down-> MapReduceModel 
+Action -[hidden]down-> MapReduceModel 
+MapReduceModel -left-> MapOperation : corresponds to >
+MapReduceModel -right-> ReduceOperation : corresponds to >
+Transformation ..> MapOperation : includes > #DashedBrown
+Action ..> ReduceOperation : includes > #DashedBrown
+
+@enduml
diff --git a/big_file_upload_system/big_file_upload_system_architecture_with_chunking_and_performance_optimizations.puml b/big_file_upload_system/big_file_upload_system_architecture_with_chunking_and_performance_optimizations.puml
new file mode 100644
--- /dev/null
+++ ./big_file_upload_system/big_file_upload_system_architecture_with_chunking_and_performance_optimizations.puml
@@ -0,0 +1,84 @@
+@startuml
+!pragma layout smetana
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam shadowing false
+skinparam padding 10
+skinparam roundCorner 10
+allowmixing
+
+' Color definitions
+!define CLIENT_COLOR E0E0E0
+!define SERVICE_COLOR D0E0F0
+!define STORAGE_COLOR F0E0D0
+!define QUEUE_COLOR E0F0D0
+!define CACHE_COLOR F0D0E0
+
+rectangle "Client Side" {
+    component "Client" as Client #CLIENT_COLOR
+    component "FileChunker" as FileChunker #CLIENT_COLOR
+    component "ResumableUploadManager" as ResumableUploadManager #CLIENT_COLOR
+    component "ResumableDownloadManager" as ResumableDownloadManager #CLIENT_COLOR
+}
+
+rectangle "Service Layer" {
+    component "LoadBalancer" as LoadBalancer #SERVICE_COLOR
+    component "UploadService" as UploadService #SERVICE_COLOR
+    component "DownloadService" as DownloadService #SERVICE_COLOR
+    component "CDN" as CDN #SERVICE_COLOR
+    component "AuthenticationService" as AuthService #SERVICE_COLOR
+}
+
+rectangle "Storage Layer" {
+    component "ObjectStorage" as ObjectStorage #STORAGE_COLOR {
+        component "FileChunks" as FileChunks
+        component "Metadata" as Metadata
+    }
+    component "Redis" as Redis #CACHE_COLOR
+}
+
+component "MessageQueue" as MessageQueue #QUEUE_COLOR
+
+note right of LoadBalancer
+    Implements rate limiting
+    and request distribution
+end note
+
+note bottom of ObjectStorage
+    Implements data sharding,
+    replication, and database functions
+end note
+
+note right of Redis
+    key: value
+    chunk_id: upload_status
+    file_id: metadata
+end note
+
+Client -[#FF5733,thickness=2]-> FileChunker : <color:#FF5733>1. Split large file</color>
+FileChunker -[#33FF57,thickness=2]-> LoadBalancer : <color:#33FF57>2. Upload/Download file chunks</color>
+LoadBalancer -[#5733FF,thickness=2]-> AuthService : <color:#5733FF>3. Authenticate request</color>
+AuthService -[#FF33F1,thickness=2]-> MessageQueue : <color:#FF33F1>4. Enqueue authenticated requests</color>
+MessageQueue -[#33FFF1,thickness=2]-> UploadService : <color:#33FFF1>5. Dequeue and process upload chunks</color>
+MessageQueue -[#8B4513,thickness=2]-> DownloadService : <color:#8B4513>6. Dequeue and process download requests</color>
+UploadService -[#FF8C33,thickness=2]-> ObjectStorage : <color:#FF8C33>7. Store file chunks and metadata</color>
+DownloadService <-[#3357FF,thickness=2]- ObjectStorage : <color:#3357FF>8. Retrieve file chunks and metadata</color>
+Client -[#FF3357,thickness=2]-> ResumableUploadManager : <color:#FF3357>9. Manage file upload chunks</color>
+Client -[#57FF33,thickness=2]-> ResumableDownloadManager : <color:#57FF33>10. Manage file download chunks</color>
+ResumableUploadManager -[#33FF8C,thickness=2]-> UploadService : <color:#33FF8C>11. Manage upload status</color>
+ResumableDownloadManager -[#8C33FF,thickness=2]-> DownloadService : <color:#8C33FF>12. Manage download status</color>
+UploadService <-[#FF3333,thickness=2]-> Redis : <color:#FF3333>13. Cache/Retrieve upload status</color>
+DownloadService <-[#33FFFF,thickness=2]-> Redis : <color:#33FFFF>14. Cache/Retrieve download status</color>
+DownloadService -[#4B0082,thickness=2]-> CDN : <color:#4B0082>15. Serve popular files</color>
+
+note as PerformanceNote
+    Performance bottlenecks and optimization suggestions:
+    1. Implement parallel chunk uploads for faster large file transfers
+    2. Use adaptive chunk sizes based on network conditions
+    3. Implement progressive file downloads for faster user experience
+    4. Optimize CDN caching strategies for frequently accessed files
+    5. Use database indexing for faster metadata retrieval
+    6. Implement data compression for storage and transfer efficiency
+end note
+
+@enduml
diff --git a/big_file_upload_system/database_schema_sql_nosql_hybrid_design_for_file_metadata_and_chunks.puml b/big_file_upload_system/database_schema_sql_nosql_hybrid_design_for_file_metadata_and_chunks.puml
new file mode 100644
--- /dev/null
+++ ./big_file_upload_system/database_schema_sql_nosql_hybrid_design_for_file_metadata_and_chunks.puml
@@ -0,0 +1,111 @@
+@startuml
+!define table(x) class x << (T,#FFAAAA) >>
+!define nosql(x) class x << (N,#AAFFAA) >>
+!define primary_key(x) <u>x</u>
+!define foreign_key(x) #x
+
+' Global settings
+skinparam classFontSize 14
+skinparam classFontName Helvetica
+skinparam noteFontSize 12
+skinparam noteFontName Helvetica
+skinparam linetype ortho
+skinparam padding 8
+skinparam roundcorner 10
+
+' Layout
+top to bottom direction
+hide circle
+
+' Relational Database Tables
+package "Relational Database" {
+    table(Users) {
+        primary_key(user_id): INT
+        username: VARCHAR(50)
+        email: VARCHAR(100)
+        created_at: TIMESTAMP
+        --
+        <i>index on (email)</i>
+    }
+
+    table(UploadSessions) {
+        primary_key(session_id): VARCHAR(50)
+        foreign_key(file_id): STRING
+        foreign_key(user_id): INT
+        started_at: TIMESTAMP
+        last_activity: TIMESTAMP
+        status: ENUM('active', 'completed', 'expired')
+        --
+        <i>index on (user_id, status)</i>
+    }
+
+    table(DownloadSessions) {
+        primary_key(session_id): VARCHAR(50)
+        foreign_key(file_id): STRING
+        foreign_key(user_id): INT
+        started_at: TIMESTAMP
+        last_activity: TIMESTAMP
+        status: ENUM('active', 'completed', 'expired')
+        bytes_downloaded: BIGINT
+        --
+        <i>index on (user_id, status)</i>
+    }
+}
+
+' NoSQL Stores
+package "NoSQL Database" {
+    nosql(NoSQL_File_Metadata_Store) {
+        file_id: STRING (Partition Key)
+        user_id: INT (Sort Key)
+        filename: STRING
+        total_size: BIGINT
+        created_at: TIMESTAMP
+        updated_at: TIMESTAMP
+        status: STRING
+        md5_hash: STRING
+        --
+        Global Secondary Index:
+        GSI1PK: user_id
+        GSI1SK: created_at
+    }
+
+    nosql(NoSQL_File_Chunks_Store) {
+        file_id: STRING (Partition Key)
+        chunk_index: INT (Sort Key)
+        chunk_size: INT
+        storage_path: STRING
+        status: STRING
+        --
+        Global Secondary Index:
+        GSI1PK: storage_path
+        GSI1SK: status
+    }
+}
+
+' Relationships
+Users ||--o{ UploadSessions
+Users ||--o{ DownloadSessions
+NoSQL_File_Metadata_Store }o--|| Users
+NoSQL_File_Chunks_Store }o--|| NoSQL_File_Metadata_Store
+UploadSessions }o--|| NoSQL_File_Metadata_Store
+DownloadSessions }o--|| NoSQL_File_Metadata_Store
+
+note right of NoSQL_File_Metadata_Store
+    Sharding strategy:
+    - Partition Key: file_id
+    - Sort Key: user_id
+    This allows efficient queries 
+    for both specific files and 
+    all files for a specific user
+end note
+
+note right of NoSQL_File_Chunks_Store
+    Sharding strategy:
+    - Partition Key: file_id
+    - Sort Key: chunk_index
+    This allows efficient retrieval
+    of all chunks for a specific file,
+    as well as specific chunks
+end note
+
+@enduml
diff --git a/blockchain_system_architecture_with_consensus_smart_contracts_p2p_network_and_cryptography.puml b/blockchain_system_architecture_with_consensus_smart_contracts_p2p_network_and_cryptography.puml
new file mode 100644
--- /dev/null
+++ ./blockchain_system_architecture_with_consensus_smart_contracts_p2p_network_and_cryptography.puml
@@ -0,0 +1,114 @@
+@startuml Blockchain System Design
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam roundcorner 20
+skinparam shadowing true
+
+title Blockchain System Design
+
+' User Layer
+rectangle "User Layer" as UserLayer {
+    [Light Node]
+    [Wallet Application]
+    [DApp]
+}
+
+' Application Layer
+rectangle "Application Layer" as ApplicationLayer {
+    [Smart Contract]
+    [Chaincode]
+    [DApp Backend]
+}
+
+' Network Layer
+rectangle "Network Layer" as NetworkLayer {
+    [P2P Network]
+    [Node Discovery]
+    [Data Propagation]
+}
+
+' Consensus Layer
+rectangle "Consensus Layer" as ConsensusLayer {
+    [PoW (Proof of Work)]
+    [PoS (Proof of Stake)]
+    [DPoS (Delegated Proof of Stake)]
+    [PBFT (Practical Byzantine Fault Tolerance)]
+}
+
+' Data Layer
+rectangle "Data Layer" as DataLayer {
+    database "Blockchain" as Blockchain {
+        [Block Header]
+        [Transaction Data]
+        [State Tree]
+    }
+    database "State Database" as StateDB {
+        [Account State]
+        [Contract State]
+    }
+}
+
+' Cryptography Layer
+rectangle "Cryptography Layer" as CryptographyLayer {
+    [Asymmetric Encryption]
+    [Hash Function]
+    [Digital Signature]
+    [Zero-Knowledge Proof]
+}
+
+' Virtual Machine
+rectangle "Virtual Machine" as VirtualMachine {
+    [EVM (Ethereum Virtual Machine)]
+    [WASM (WebAssembly)]
+}
+
+' Storage Layer
+database "Storage Layer" as StorageLayer {
+    [Distributed Storage]
+    [IPFS]
+}
+
+' Monitoring and Management
+rectangle "Monitoring and Management" as MonitoringManagement {
+    [Block Explorer]
+    [Network Monitoring]
+    [Performance Analysis]
+}
+
+' Connections
+UserLayer -down-> ApplicationLayer : Use
+ApplicationLayer -down-> NetworkLayer : Deploy/Call
+NetworkLayer -down-> ConsensusLayer : Reach Consensus
+ConsensusLayer -down-> DataLayer : Write/Read
+DataLayer -left-> CryptographyLayer : Use
+ApplicationLayer -right-> VirtualMachine : Execute
+DataLayer -down-> StorageLayer : Store Data
+MonitoringManagement -up-> DataLayer : Monitor
+
+note right of ConsensusLayer
+  Consensus Mechanism Features:
+  1. PoW: High security, but energy-intensive
+  2. PoS: Energy-efficient, but may lead to centralization
+  3. DPoS: Efficient, but limited representation
+  4. PBFT: High performance, but limited scalability
+end note
+
+note left of DataLayer
+  Data Structure:
+  1. Block: Contains multiple transactions
+  2. Transaction: State transition
+  3. Merkle Tree: Fast verification
+end note
+
+note bottom of CryptographyLayer
+  Cryptographic Guarantees:
+  1. Immutable transactions
+  2. Identity authentication
+  3. Data integrity
+  4. Privacy protection
+end note
+
+@enduml
diff --git a/canary_release_and_blue_green_deployment_strategies.puml b/canary_release_and_blue_green_deployment_strategies.puml
new file mode 100644
--- /dev/null
+++ ./canary_release_and_blue_green_deployment_strategies.puml
@@ -0,0 +1,85 @@
+@startuml Distributed System Deployment Strategies
+
+!pragma layout dot
+
+skinparam backgroundColor #CCE8CF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+allowmixing
+
+title Distributed System Deployment Strategies: Canary Release and Blue-Green Deployment
+
+rectangle "Load Balancer" as LoadBalancer PRIMARY_COLOR {
+    component "Traffic Router" as TrafficRouter
+    component "Health Checker" as HealthChecker
+}
+
+rectangle "Canary Release" as CanaryRelease SECONDARY_COLOR {
+    component "Version A (90%)" as VersionA
+    component "Version B (10%)" as VersionB
+}
+
+rectangle "Blue-Green Deployment" as BlueGreenDeployment TERTIARY_COLOR {
+    component "Blue Environment" as BlueEnv
+    component "Green Environment" as GreenEnv
+}
+
+rectangle "Monitoring & Analytics" as Monitoring QUATERNARY_COLOR {
+    component "Performance Metrics" as PerformanceMetrics
+    component "Error Rates" as ErrorRates
+    component "User Feedback" as UserFeedback
+}
+
+rectangle "Deployment Pipeline" as DeploymentPipeline QUINARY_COLOR {
+    component "CI/CD System" as CICD
+    component "Automated Tests" as AutomatedTests
+    component "Rollback Mechanism" as RollbackMechanism
+}
+
+LoadBalancer -[PRIMARY_COLOR,thickness=2]down-> CanaryRelease : <back:#FFFFFF><color:PRIMARY_COLOR>1. Route Traffic</color></back>
+LoadBalancer -[PRIMARY_COLOR,thickness=2]right-> BlueGreenDeployment : <back:#FFFFFF><color:PRIMARY_COLOR>2. Switch Environment</color></back>
+CanaryRelease -[SECONDARY_COLOR,thickness=2]right-> Monitoring : <back:#FFFFFF><color:SECONDARY_COLOR>3. Monitor Performance</color></back>
+BlueGreenDeployment -[TERTIARY_COLOR,thickness=2]down-> Monitoring : <back:#FFFFFF><color:TERTIARY_COLOR>4. Compare Environments</color></back>
+DeploymentPipeline -[QUINARY_COLOR,thickness=2]up-> CanaryRelease : <back:#FFFFFF><color:QUINARY_COLOR>5. Deploy New Version</color></back>
+DeploymentPipeline -[QUINARY_COLOR,thickness=2]right-> BlueGreenDeployment : <back:#FFFFFF><color:QUINARY_COLOR>6. Prepare New Environment</color></back>
+Monitoring -[QUATERNARY_COLOR,thickness=2]left-> DeploymentPipeline : <back:#FFFFFF><color:QUATERNARY_COLOR>7. Trigger Rollback if Needed</color></back>
+
+note right of CanaryRelease
+  Canary Release:
+  - Gradually roll out new version
+  - Limit potential impact of issues
+  - Easy to rollback if problems occur
+end note
+
+note right of BlueGreenDeployment
+  Blue-Green Deployment:
+  - Two identical production environments
+  - Instant cutover between versions
+  - Zero downtime deployments
+end note
+
+note bottom of Monitoring
+  Monitoring & Analytics:
+  - Real-time performance tracking
+  - Automated alerts for anomalies
+  - User experience feedback loop
+end note
+
+note bottom of DeploymentPipeline
+  Deployment Pipeline:
+  - Automated build, test, and deploy
+  - Consistent deployment process
+  - Quick rollback capabilities
+end note
+
+@enduml
diff --git a/cart_and_inventory_service_architecture.puml b/cart_and_inventory_service_architecture.puml
new file mode 100644
--- /dev/null
+++ ./cart_and_inventory_service_architecture.puml
@@ -0,0 +1,121 @@
+@startuml Cart_Inventory_Architecture
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #F0F8FF
+skinparam shadowing false
+skinparam RoundCorner 10
+skinparam ArrowColor 454645
+skinparam DefaultFontName Arial
+skinparam DefaultFontSize 12
+
+rectangle "Client Layer" as ClientLayer #E6E6FA {
+    component "Web App" as WebApp #FFF0F5
+    component "Mobile App" as MobileApp #FFF0F5
+}
+
+rectangle "API Gateway" as APIGateway #D8BFD8
+
+rectangle "Service Layer" as ServiceLayer #E6E6FA {
+    component "Cart Service" as CartService #FFF0F5
+    component "Inventory Service" as InventoryService #FFF0F5
+    component "Product Service" as ProductService #FFF0F5
+    component "User Service" as UserService #FFF0F5
+}
+
+rectangle "Cache Layer" as CacheLayer #FFE4E1 {
+    component "Redis Cache" as RedisCache #FFA07A
+}
+
+rectangle "Database Layer" as DatabaseLayer #B0E0E6 {
+    component "Cart DB" as CartDB #87CEEB
+    component "Inventory DB" as InventoryDB #87CEEB
+    component "Product DB" as ProductDB #87CEEB
+    component "User DB" as UserDB #87CEEB
+}
+
+rectangle "Message Queue" as MessageQueue #98FB98
+
+rectangle "External Services" as ExternalServices #F0E68C {
+    component "Payment Gateway" as PaymentGateway #FFD700
+    component "Shipping Service" as ShippingService #FFD700
+}
+
+ClientLayer -[#FF6347,thickness=2]down-> APIGateway : "<color:#FF6347>1. User requests"
+APIGateway -[#4169E1,thickness=2]down-> ServiceLayer : "<color:#4169E1>2. Route requests"
+
+CartService -[#32CD32,thickness=2]right-> InventoryService : "<color:#32CD32>3. Check stock"
+CartService -[#9932CC,thickness=2]right-> ProductService : "<color:#9932CC>4. Get product details"
+CartService -[#FF8C00,thickness=2]right-> UserService : "<color:#FF8C00>5. Validate user"
+
+CartService -[#1E90FF,thickness=2]down-> RedisCache : "<color:#1E90FF>6. Cache cart data"
+InventoryService -[#DC143C,thickness=2]down-> RedisCache : "<color:#DC143C>7. Cache inventory data"
+
+CartService -[#008080,thickness=2]down-> CartDB : "<color:#008080>8. Persist cart"
+InventoryService -[#8B4513,thickness=2]down-> InventoryDB : "<color:#8B4513>9. Update inventory"
+ProductService -[#4682B4,thickness=2]down-> ProductDB : "<color:#4682B4>10. Fetch product info"
+UserService -[#2E8B57,thickness=2]down-> UserDB : "<color:#2E8B57>11. Get user data"
+
+CartService -[#CD5C5C,thickness=2]right-> MessageQueue : "<color:#CD5C5C>12. Publish events"
+InventoryService -[#6A5ACD,thickness=2]left-> MessageQueue : "<color:#6A5ACD>13. Subscribe to events"
+
+CartService -[#DAA520,thickness=2]-> PaymentGateway : "<color:#DAA520>14. Process payment"
+CartService -[#20B2AA,thickness=2]-> ShippingService : "<color:#20B2AA>15. Initiate shipping"
+
+note right of ClientLayer
+  Client Layer:
+  - Responsive UI
+  - Offline capabilities
+  - Data caching
+end note
+
+note right of APIGateway
+  API Gateway:
+  - Rate limiting
+  - Authentication
+  - Request routing
+end note
+
+note right of CartService
+  Cart Service:
+  - High concurrency
+  - Data consistency
+  - Cart merging
+  - Product validation
+end note
+
+note right of InventoryService
+  Inventory Service:
+  - Real-time stock updates
+  - Reservation system
+  - Restock alerts
+end note
+
+note right of RedisCache
+  Redis Cache:
+  - Key: user_id
+  - Value: cart_data
+  - TTL: 1 hour
+end note
+
+note right of CartDB
+  Cart DB:
+  - Scalable
+  - Redundant
+  - Backup & Recovery
+end note
+
+note right of MessageQueue
+  Message Queue:
+  - Asynchronous processing
+  - Event-driven architecture
+  - Fault tolerance
+end note
+
+note bottom of ExternalServices
+  External Services:
+  - Fault tolerance
+  - Circuit breaker
+  - Retry mechanisms
+end note
+
+@enduml
\ No newline at end of file
diff --git a/cdn/cdn_detailed_architecture.puml b/cdn/cdn_detailed_architecture.puml
new file mode 100644
--- /dev/null
+++ ./cdn/cdn_detailed_architecture.puml
@@ -0,0 +1,50 @@
+@startuml CDN Detailed Architecture
+!define RECTANGLE class
+
+skinparam backgroundColor #F0F8FF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam ArrowColor #2C3E50
+skinparam NoteBackgroundColor #D5E8D4
+skinparam NoteBorderColor #82B366
+
+title CDN Detailed Architecture
+
+rectangle "User" as user #FFD700
+rectangle "DNS" as dns #87CEFA
+
+package "CDN Network" {
+    rectangle "Edge Node" as edge #98FB98
+    rectangle "Regional Node" as regional #FFA07A
+    rectangle "Central Node" as central #DDA0DD
+}
+
+rectangle "Origin Server" as origin #F0E68C
+
+user -[#FF6347]-> dns : 1. DNS query
+dns -[#FF6347]-> user : 2. Return CDN edge node IP
+user -[#4169E1]-> edge : 3. Content request
+edge -[#32CD32]-> regional : 4. If not in edge cache
+regional -[#FF8C00]-> central : 5. If not in regional cache
+central -[#8A2BE2]-> origin : 6. If not in central cache
+origin -[#8A2BE2]-> central : 7. Return content
+central -[#FF8C00]-> regional : 8. Cache and forward
+regional -[#32CD32]-> edge : 9. Cache and forward
+edge -[#4169E1]-> user : 10. Serve content
+
+note right of edge
+  Closest to end users
+  Handles most requests
+end note
+
+note right of regional
+  Aggregates multiple edge nodes
+  Larger cache capacity
+end note
+
+note right of central
+  Connects to origin
+  Global load balancing
+end note
+
+@enduml
diff --git a/cdn/cdn_global_deployment_map.puml b/cdn/cdn_global_deployment_map.puml
new file mode 100644
--- /dev/null
+++ ./cdn/cdn_global_deployment_map.puml
@@ -0,0 +1,25 @@
+@startuml CDN Global Deployment Map
+!define RECTANGLE class
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+
+title CDN Global Deployment Map
+
+map World_Map {
+    North_America => NA1, NA2, NA3
+    South_America => SA1, SA2
+    Europe => EU1, EU2, EU3, EU4
+    Asia => AS1, AS2, AS3, AS4, AS5
+    Africa => AF1, AF2
+    Oceania => OC1, OC2
+}
+
+legend right
+    NA1, NA2, etc. : CDN Points of Presence (PoPs)
+end legend
+
+note "CDN nodes are strategically placed\nto minimize latency and optimize\ncontent delivery for global users" as N1
+
+@enduml
diff --git a/cdn/cdn_origin_interaction_sequence.puml b/cdn/cdn_origin_interaction_sequence.puml
new file mode 100644
--- /dev/null
+++ ./cdn/cdn_origin_interaction_sequence.puml
@@ -0,0 +1,51 @@
+@startuml CDN-Origin Interaction Sequence
+!define RECTANGLE class
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+
+title CDN-Origin Interaction Sequence
+
+actor User
+participant "Edge Node" as Edge
+participant "Regional Node" as Regional
+participant "Central Node" as Central
+participant "Origin Server" as Origin
+
+User -> Edge : Request content
+activate Edge
+
+alt Content in Edge Cache
+    Edge -> User : Serve content
+else Content not in Edge Cache
+    Edge -> Regional : Request content
+    activate Regional
+    
+    alt Content in Regional Cache
+        Regional -> Edge : Return content
+        Edge -> User : Serve content
+    else Content not in Regional Cache
+        Regional -> Central : Request content
+        activate Central
+        
+        alt Content in Central Cache
+            Central -> Regional : Return content
+            Regional -> Edge : Forward content
+            Edge -> User : Serve content
+        else Content not in Central Cache
+            Central -> Origin : Request content
+            activate Origin
+            Origin -> Central : Return content
+            Central -> Regional : Forward and cache content
+            Regional -> Edge : Forward and cache content
+            Edge -> User : Serve content
+            deactivate Origin
+        end
+        deactivate Central
+    end
+    deactivate Regional
+end
+deactivate Edge
+
+@enduml
diff --git a/cdn/cdn_performance_optimization_strategies_content_delivery_caching_and_security.puml b/cdn/cdn_performance_optimization_strategies_content_delivery_caching_and_security.puml
new file mode 100644
--- /dev/null
+++ ./cdn/cdn_performance_optimization_strategies_content_delivery_caching_and_security.puml
@@ -0,0 +1,69 @@
+@startuml CDN Performance Optimization Strategies
+
+skinparam {
+    backgroundColor #FAFAFA
+    handwritten false
+    defaultFontName Arial
+    defaultFontSize 14
+    roundcorner 20
+    shadowing false
+}
+
+rectangle "CDN Performance Optimization Strategies" as CDN {
+    rectangle "Content Optimization" as ContentOpt #E1F5FE
+    rectangle "Delivery Optimization" as DeliveryOpt #FFF3E0
+    rectangle "Caching Strategies" as CacheStrat #E8F5E9
+    rectangle "Advanced Techniques" as AdvTech #FFEBEE
+    rectangle "Security Measures" as Security #E0F7FA
+    rectangle "Analytics & Monitoring" as Analytics #F3E5F5
+}
+
+ContentOpt -[#4CAF50,thickness=2]down-> DeliveryOpt : 1. Optimize Content
+DeliveryOpt -[#2196F3,thickness=2]right-> CacheStrat : 2. Efficient Delivery
+CacheStrat -[#FF5722,thickness=2]right-> AdvTech : 3. Enhance Caching
+DeliveryOpt -[#9C27B0,thickness=2]down-> Security : 4. Secure Delivery
+AdvTech -[#795548,thickness=2]down-> Analytics : 5. Performance Insights
+
+note bottom of ContentOpt
+  - Compress files
+  - Minify code
+  - Optimize images
+  - Use HTTP/2 and HTTP/3
+end note
+
+note bottom of DeliveryOpt
+  - Edge caching
+  - Dynamic content acceleration
+  - Intelligent routing
+  - Anycast DNS
+end note
+
+note bottom of CacheStrat
+  - Browser caching
+  - Adaptive TTL
+  - Stale-while-revalidate
+  - Cache key optimization
+end note
+
+note bottom of AdvTech
+  - Content prefetching
+  - Load balancing
+  - Multi-CDN strategy
+  - Edge computing
+end note
+
+note bottom of Security
+  - DDoS protection
+  - Web Application Firewall
+  - SSL/TLS optimization
+  - Bot traffic management
+end note
+
+note bottom of Analytics
+  - Real-time analytics
+  - Performance monitoring
+  - Error tracking
+  - User experience metrics
+end note
+
+@enduml
diff --git a/cdn/cdn_security_protection_architecture.puml b/cdn/cdn_security_protection_architecture.puml
new file mode 100644
--- /dev/null
+++ ./cdn/cdn_security_protection_architecture.puml
@@ -0,0 +1,40 @@
+@startuml CDN Security Protection Architecture
+!define RECTANGLE class
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+
+title CDN Security Protection Architecture
+
+actor "User" as user
+actor "Attacker" as attacker
+
+rectangle "CDN Security Layer" {
+    rectangle "DDoS Protection" as ddos
+    rectangle "Web Application Firewall" as waf
+    rectangle "Bot Management" as botmgmt
+    rectangle "SSL/TLS Encryption" as ssl
+    rectangle "Access Control" as access
+    rectangle "Rate Limiting" as ratelimit
+}
+
+rectangle "Origin Server" as origin
+
+user --> ssl
+attacker --> ddos
+
+ddos --> waf : Filtered traffic
+waf --> botmgmt : Legitimate requests
+botmgmt --> access : Human traffic
+access --> ratelimit : Authorized requests
+ratelimit --> origin : Controlled traffic flow
+
+note right of ddos : Mitigates volumetric attacks
+note right of waf : Blocks malicious requests
+note right of botmgmt : Identifies and manages bot traffic
+note right of ssl : Ensures data in transit security
+note right of access : Implements authentication and authorization
+note right of ratelimit : Prevents abuse and ensures fair usage
+
+@enduml
diff --git a/cdn/comprehensive_cdn_architecture_with_edge_servers_caching_and_request_routing.puml b/cdn/comprehensive_cdn_architecture_with_edge_servers_caching_and_request_routing.puml
new file mode 100644
--- /dev/null
+++ ./cdn/comprehensive_cdn_architecture_with_edge_servers_caching_and_request_routing.puml
@@ -0,0 +1,79 @@
+@startuml Comprehensive CDN Architecture
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontSize 14
+skinparam noteFontSize 12
+skinparam arrowFontSize 12
+
+rectangle "CDN System Architecture" as CDN #E6E6FA {
+    actor "User" as User
+    rectangle "DNS Server" as DNSServer #D0E0F0
+    rectangle "Global Load Balancer" as GLB #D0E0F0
+    rectangle "Origin Server" as Origin #D0E0F0
+    
+    rectangle "Region 1" as Region1 #E0F0E0 {
+        rectangle "Regional Load Balancer 1" as RLB1 #D0E0F0
+        rectangle "Edge Server 1" as ES1 #D0E0F0
+        component "Cache 1" as Cache1 #F0E0D0
+    }
+    
+    rectangle "Region 2" as Region2 #E0F0E0 {
+        rectangle "Regional Load Balancer 2" as RLB2 #D0E0F0
+        rectangle "Edge Server 2" as ES2 #D0E0F0
+        component "Cache 2" as Cache2 #F0E0D0
+    }
+    
+    rectangle "Cache Decision System" as CDS #D0E0F0
+    rectangle "Analytics & Monitoring" as Analytics #D0E0F0
+    
+    rectangle "CDN Management" as CDNManagement #E0F0E0 {
+        component "Pre-Caching Strategy" as PreCache
+        component "Cache Synchronization" as Sync
+        component "Intelligent Routing" as Routing
+        component "Locality Principle" as Locality
+    }
+}
+
+User -[#green,thickness=2]-> DNSServer : <color:#green>1. User Request
+DNSServer -[#blue,thickness=2]-> GLB : <color:#blue>2. DNS Resolution
+GLB -[#purple,thickness=2]-> RLB1 : <color:#purple>3. Route to Region
+GLB -[#purple,thickness=2]-> RLB2 : <color:#purple>3. Route to Region
+RLB1 -[#orange,thickness=2]-> ES1 : <color:#orange>4. Load Balance
+RLB2 -[#orange,thickness=2]-> ES2 : <color:#orange>4. Load Balance
+ES1 -[#red,thickness=2]-> Cache1 : <color:#red>5. Check Cache
+ES2 -[#red,thickness=2]-> Cache2 : <color:#red>5. Check Cache
+Cache1 -[#brown,thickness=2]-> CDS : <color:#brown>6. Cache Query
+Cache2 -[#brown,thickness=2]-> CDS : <color:#brown>6. Cache Query
+CDS -[#gray,thickness=2]-> Origin : <color:#gray>7. Request Origin\n(if not cached)
+Origin -[#gray,thickness=2]-> Cache1 : <color:#gray>8. Update Cache
+Origin -[#gray,thickness=2]-> Cache2 : <color:#gray>8. Update Cache
+ES1 -[#green,thickness=2]-> User : <color:#green>9. Response
+ES2 -[#green,thickness=2]-> User : <color:#green>9. Response
+
+CDS -[#darkblue,thickness=2]-> Analytics : <color:#darkblue>10. Data Analysis
+Analytics -[#darkblue,thickness=2]-> GLB : <color:#darkblue>11. Optimize Decisions
+
+CDNManagement -[#purple,thickness=2]-> Cache1 : <color:#purple>Manage
+
+note right of CDNManagement
+  System Optimization:
+  1. Real-time CDN performance analysis
+  2. Optimize routing decisions
+  3. Intelligent pre-caching
+  4. Improve cache hit rates
+  5. Edge computing for simple requests
+  6. Multi-level caching strategy
+  7. Security: DDoS protection, encryption
+end note
+
+note bottom of ES2
+  Composite Cache Key:
+  - Hostname
+  - Path
+  - Query String
+  - Request Headers
+end note
+
+@enduml
diff --git a/check_puml.sh b/check_puml.sh
new file mode 100755
--- /dev/null
+++ ./check_puml.sh
@@ -0,0 +1,112 @@
+#!/bin/bash
+
+# 设置颜色
+GREEN='\033[0;32m'
+RED='\033[0;31m'
+YELLOW='\033[1;33m'
+NC='\033[0m' # No Color
+
+# 查找最新版本的PlantUML jar
+find_plantuml_jar() {
+    local jar_path=$(find /usr/local/Cellar/plantuml -name "plantuml.jar" -type f | sort -r | head -n 1)
+    if [ -z "$jar_path" ]; then
+        echo -e "${RED}Error: PlantUML jar not found. Please ensure PlantUML is installed.${NC}"
+        exit 1
+    fi
+    echo "$jar_path"
+}
+
+# 检查单个文件
+check_file() {
+    local file="$1"
+    if [ ! -f "$file" ]; then
+        echo -e "${RED}Error: File not found: $file${NC}"
+        return 1
+    fi
+
+    echo -e "\n${GREEN}Checking $file...${NC}"
+    echo "----------------------------------------"
+    
+    # 检查语法
+    java -jar "$PLANTUML_JAR" -checkonly "$file" > /dev/null 2>&1
+    syntax_status=$?
+    
+    if [ $syntax_status -eq 0 ]; then
+        echo -e "${GREEN}✅ Syntax OK${NC}"
+        
+        # 尝试渲染
+        java -jar "$PLANTUML_JAR" "$file" > /dev/null 2>&1
+        render_status=$?
+        
+        if [ $render_status -eq 0 ]; then
+            echo -e "${GREEN}✅ Render OK${NC}"
+            # 检查是否生成了png文件
+            png_file="${file%.*}.png"
+            if [ -f "$png_file" ]; then
+                echo -e "${GREEN}✅ Generated: $png_file${NC}"
+            fi
+            return 0
+        else
+            echo -e "${RED}❌ Render Failed${NC}"
+            return 1
+        fi
+    else
+        echo -e "${RED}❌ Syntax Error${NC}"
+        echo -e "\nFile contents with line numbers:"
+        nl -ba "$file"
+        return 1
+    fi
+}
+
+# 显示使用方法
+show_usage() {
+    echo "Usage: $0 [file.puml]"
+    echo "If no file is specified, checks all .puml files in the current directory and subdirectories"
+    echo "Options:"
+    echo "  -h, --help    Show this help message"
+}
+
+# 主函数
+main() {
+    # 查找PlantUML jar
+    PLANTUML_JAR=$(find_plantuml_jar)
+    echo -e "${YELLOW}Using PlantUML jar: $PLANTUML_JAR${NC}"
+
+    # 处理命令行参数
+    if [ "$1" = "-h" ] || [ "$1" = "--help" ]; then
+        show_usage
+        exit 0
+    fi
+
+    # 如果指定了文件，只检查该文件
+    if [ $# -eq 1 ]; then
+        if [[ "$1" != *.puml ]]; then
+            echo -e "${RED}Error: File must have .puml extension${NC}"
+            exit 1
+        fi
+        check_file "$1"
+        exit $?
+    fi
+
+    # 否则检查所有puml文件
+    local error_found=0
+    while IFS= read -r file; do
+        check_file "$file"
+        if [ $? -ne 0 ]; then
+            echo -e "${RED}Error found in $file${NC}"
+            error_found=1
+            break
+        fi
+    done < <(find . -name "*.puml" -type f)
+
+    if [ $error_found -eq 1 ]; then
+        echo -e "${RED}Errors found. Stopping check.${NC}"
+        exit 1
+    else
+        echo -e "${GREEN}All files checked successfully.${NC}"
+        exit 0
+    fi
+}
+
+# 运行主函数
+main "$@" 
\ No newline at end of file
diff --git a/cicd_devops_pipeline_architecture_with_zero_downtime_deployment_and_automated_rollback.puml b/cicd_devops_pipeline_architecture_with_zero_downtime_deployment_and_automated_rollback.puml
new file mode 100644
--- /dev/null
+++ ./cicd_devops_pipeline_architecture_with_zero_downtime_deployment_and_automated_rollback.puml
@@ -0,0 +1,107 @@
+@startuml CI CD and DevOps Pipeline Architecture
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+skinparam backgroundColor #CCE8CF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title CI/CD and DevOps Pipeline Architecture with Zero Downtime Deployment
+
+allowmixing
+!pragma layout smetana
+
+rectangle "Version Control" as VC PRIMARY_COLOR {
+    component "Git Repository" as GitRepo
+    component "Branch Management" as BranchMgmt
+}
+
+rectangle "Continuous Integration" as CI SECONDARY_COLOR {
+    component "Build Server" as BuildServer
+    component "Automated Tests" as AutoTests
+    component "Code Quality Analysis" as CodeQuality
+}
+
+rectangle "Artifact Management" as AM TERTIARY_COLOR {
+    component "Artifact Repository" as ArtifactRepo
+    component "Version Tagging" as VersionTag
+}
+
+rectangle "Continuous Delivery" as CD QUATERNARY_COLOR {
+    component "Deployment Automation" as DeployAuto
+    component "Environment Management" as EnvMgmt
+    component "Configuration Management" as ConfigMgmt
+}
+
+rectangle "Production Environment" as PE QUINARY_COLOR {
+    component "Blue Environment" as BlueEnv
+    component "Green Environment" as GreenEnv
+    component "Load Balancer" as LoadBalancer
+}
+
+rectangle "Monitoring & Feedback" as MF #FF6B6B {
+    component "Performance Monitoring" as PerfMon
+    component "Log Analysis" as LogAnalysis
+    component "Alerting System" as AlertSystem
+}
+
+VC -[PRIMARY_COLOR,thickness=2]down-> CI : <back:#FFFFFF><color:PRIMARY_COLOR>1. Trigger Build</color></back>
+CI -[SECONDARY_COLOR,thickness=2]right-> AM : <back:#FFFFFF><color:SECONDARY_COLOR>2. Store Artifact</color></back>
+AM -[TERTIARY_COLOR,thickness=2]down-> CD : <back:#FFFFFF><color:TERTIARY_COLOR>3. Deploy Artifact</color></back>
+CD -[QUATERNARY_COLOR,thickness=2]right-> PE : <back:#FFFFFF><color:QUATERNARY_COLOR>4. Update Environment</color></back>
+PE -[QUINARY_COLOR,thickness=2]up-> MF : <back:#FFFFFF><color:QUINARY_COLOR>5. Collect Metrics</color></back>
+MF -[#FF6B6B,thickness=2]left-> VC : <back:#FFFFFF><color:#FF6B6B>6. Feedback Loop</color></back>
+
+note right of VC
+  Version Control:
+  - Feature branching
+  - Pull request reviews
+  - Automated merge checks
+end note
+
+note right of CI
+  Continuous Integration:
+  - Parallel test execution
+  - Security scans
+  - Docker image building
+end note
+
+note right of AM
+  Artifact Management:
+  - Immutable artifacts
+  - Dependency management
+  - Artifact promotion
+end note
+
+note right of CD
+  Continuous Delivery:
+  - Infrastructure as Code
+  - Canary releases
+  - Feature flags
+end note
+
+note bottom of PE
+  Zero Downtime Deployment:
+  1. Deploy to inactive environment
+  2. Run smoke tests
+  3. Switch traffic gradually
+  4. Monitor for issues
+  5. Rollback if necessary
+end note
+
+note bottom of MF
+  Monitoring & Feedback:
+  - Real-time dashboards
+  - Automated rollback triggers
+  - A/B testing analysis
+end note
+
+@enduml
+
diff --git a/cohesity_system_architecture/cohesity_azure_integration_diagram.puml b/cohesity_system_architecture/cohesity_azure_integration_diagram.puml
new file mode 100644
--- /dev/null
+++ ./cohesity_system_architecture/cohesity_azure_integration_diagram.puml
@@ -0,0 +1,104 @@
+@startuml
+skinparam backgroundColor #FAFAFA
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam linetype ortho
+skinparam Padding 20
+skinparam ParticipantPadding 30
+skinparam ComponentPadding 25
+skinparam rectanglePadding 30
+
+skinparam rectangle {
+    roundCorner 10
+    BackgroundColor<<Cohesity>> #E3F2FD
+    BorderColor<<Cohesity>> #1E88E5
+    BackgroundColor<<Azure>> #E8EAF6
+    BorderColor<<Azure>> #3949AB
+    BackgroundColor<<ThirdParty>> #FFF3E0
+    BorderColor<<ThirdParty>> #FB8C00
+}
+
+skinparam arrow {
+    Color #616161
+    Thickness 1.2
+}
+
+skinparam note {
+    BackgroundColor #FFFDE7
+    BorderColor #FBC02D
+}
+
+rectangle "Cohesity" <<Cohesity>> {
+    [REST Endpoint]
+}
+
+rectangle "Microsoft Azure" <<Azure>> {
+    [Producer Function] as Producer
+    [Consumer Function] as Consumer
+    [Service Bus Queue] as Queue
+    [Azure Sentinel] as Sentinel
+    [Azure Cache for Redis] as Redis
+}
+
+rectangle "Third-party" <<ThirdParty>> {
+    [ServiceNow]
+}
+
+[REST Endpoint] -right-> Producer : Fetch Data
+Producer -down-> Queue : Queue Alerts
+Producer <-right-> Redis : Cache Query Times
+Queue -right-> Consumer : Process Alerts
+Consumer -up-> Sentinel : Create Incidents
+Sentinel -down-> ServiceNow : Create Tickets
+
+note top of [REST Endpoint]
+  Provides alerts and
+  security events data
+  PK: AlertID
+  Index: Timestamp
+end note
+
+note bottom of Producer
+  - Timer Triggered
+  - Transforms raw data
+  - Handles rate limiting
+  - Auto-scaling enabled
+end note
+
+note top of Redis
+  Stores:
+  - Last query timestamp
+  - Query performance metrics
+  Key: LastQueryTime
+end note
+
+note bottom of Consumer
+  - Event-driven
+  - Processes alerts in real-time
+  - Creates standardized incidents
+  - Auto-scaling based on queue length
+end note
+
+note bottom of Queue
+  - FIFO order
+  - Deduplication
+  - Transactional support
+  - Long-polling for efficiency
+  - Partitioned for high throughput
+end note
+
+note left of Sentinel
+  - Central security management
+  - Threat detection & response
+  - Utilizes Log Analytics
+  - Supports custom analytics rules
+end note
+
+note right of ServiceNow
+  - Automated ticket creation
+  - Integrates with Azure Sentinel
+  - Enables workflow automation
+  - Supports custom fields and rules
+end note
+
+@enduml
diff --git a/common_style.puml b/common_style.puml
new file mode 100644
--- /dev/null
+++ ./common_style.puml
@@ -0,0 +1,35 @@
+@startuml
+
+' Colors
+!define BLUE #1E90FF
+!define GREEN #32CD32
+!define ORANGE #FFA500
+!define RED #FF4500
+!define PURPLE #9370DB
+!define GRAY #A9A9A9
+
+' Common styles
+skinparam componentStyle rectangle
+skinparam backgroundColor #FAFAFA
+skinparam defaultFontSize 14
+skinparam defaultFontName Arial
+skinparam ArrowColor #2C3E50
+skinparam PackageBackgroundColor #FAFAFA
+skinparam PackageBorderColor #CCCCCC
+    
+skinparam component {
+    BorderColor #666666
+    BackgroundColor #FFFFFF
+}
+    
+skinparam note {
+    BorderColor #666666
+    BackgroundColor #FFFFCC
+}
+    
+skinparam package {
+    BorderColor #666666
+    BackgroundColor #FFFFFF
+}
+
+@enduml 
\ No newline at end of file
diff --git a/comprehensive_distributed_system_monitoring_alerting_log_analysis_with_data_processing_and_visualization.puml b/comprehensive_distributed_system_monitoring_alerting_log_analysis_with_data_processing_and_visualization.puml
new file mode 100644
--- /dev/null
+++ ./comprehensive_distributed_system_monitoring_alerting_log_analysis_with_data_processing_and_visualization.puml
@@ -0,0 +1,107 @@
+@startuml Distributed System Monitoring Architecture
+
+skinparam backgroundColor #CCE8CF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam roundcorner 20
+skinparam shadowing true
+allowmixing
+!pragma layout dot
+
+title Distributed System Monitoring Architecture
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+' 监控代理
+rectangle "Monitoring Agent" as Agent PRIMARY_COLOR {
+    component "Metric Collector" 
+    component "Log Collector" 
+    component "Health Checker" 
+}
+
+' 数据收集和传输
+queue "Message Queue" as MQ SECONDARY_COLOR
+component "Data Ingestion Service" as Ingestion TERTIARY_COLOR
+
+' 数据存储
+database "Time Series DB\n(Metrics)" as TSDB QUATERNARY_COLOR
+database "Log Storage\n(Event Logs)" as LogDB QUINARY_COLOR
+
+' 数据处理和分析
+rectangle "Data Processing" as Processing PRIMARY_COLOR {
+    component "Stream Processing" 
+    component "Batch Processing" 
+    component "Log Analyzer" 
+    component "Metric Analyzer" 
+}
+
+' 告警系统
+rectangle "Alert Manager" as AlertManager SECONDARY_COLOR {
+    component "Rule Engine" 
+    component "Notification Service" 
+}
+
+' 可视化
+rectangle "Visualization" as Viz TERTIARY_COLOR {
+    component "Dashboard" 
+    component "Report Generator" 
+}
+
+' API网关
+rectangle "API Gateway" as Gateway QUATERNARY_COLOR
+
+' 用户界面
+actor "Admin/User" as User
+
+' 关系
+Agent -[PRIMARY_COLOR,thickness=2]down-> MQ : <back:#FFFFFF><color:PRIMARY_COLOR>1. Send data</color></back>
+MQ -[SECONDARY_COLOR,thickness=2]down-> Ingestion : <back:#FFFFFF><color:SECONDARY_COLOR>2. Consume data</color></back>
+Ingestion -[TERTIARY_COLOR,thickness=2]down-> TSDB : <back:#FFFFFF><color:TERTIARY_COLOR>3. Store metrics</color></back>
+Ingestion -[QUATERNARY_COLOR,thickness=2]down-> LogDB : <back:#FFFFFF><color:QUATERNARY_COLOR>4. Store logs</color></back>
+TSDB -[QUINARY_COLOR,thickness=2]right-> Processing : <back:#FFFFFF><color:QUINARY_COLOR>5. Query metrics</color></back>
+LogDB -[PRIMARY_COLOR,thickness=2]right-> Processing : <back:#FFFFFF><color:PRIMARY_COLOR>6. Query logs</color></back>
+Processing -[SECONDARY_COLOR,thickness=2]up-> AlertManager : <back:#FFFFFF><color:SECONDARY_COLOR>7. Trigger alerts</color></back>
+Processing -[TERTIARY_COLOR,thickness=2]down-> Viz : <back:#FFFFFF><color:TERTIARY_COLOR>8. Provide processed data</color></back>
+
+' 数据存储之间的关系
+LogDB ..[QUATERNARY_COLOR,thickness=2]> TSDB : <back:#FFFFFF><color:QUATERNARY_COLOR>9. Extract metrics</color></back>
+
+note right of TSDB
+  Stores time-based numerical data
+  E.g., CPU usage, memory usage, request count
+end note
+
+note right of LogDB
+  Stores detailed event logs
+  E.g., error messages, access logs, system events
+end note
+
+note bottom of LogDB
+  Some log data can be 
+  transformed into metrics
+  and stored in Time Series DB
+end note
+
+note right of Agent
+  Deployed on each node/service
+  Collects metrics, logs, and health status
+end note
+
+note right of AlertManager
+  Configurable alert rules
+  Multiple notification channels
+  (Email, SMS, Slack, etc.)
+end note
+
+note right of Viz
+  Real-time and historical data visualization
+  Customizable dashboards
+  Automated reporting
+end note
+
+@enduml
diff --git a/comprehensive_performance_monitoring_system_with_data_processing_alerting_and_visualization.puml b/comprehensive_performance_monitoring_system_with_data_processing_alerting_and_visualization.puml
new file mode 100644
--- /dev/null
+++ ./comprehensive_performance_monitoring_system_with_data_processing_alerting_and_visualization.puml
@@ -0,0 +1,91 @@
+@startuml Performance Monitoring System
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #CCE8CF
+
+rectangle "Data Pipeline" as DP PRIMARY_COLOR {
+    component "Data Ingestion" as DI
+    component "Stream Processing" as SP
+    component "Analytics Engine" as AE
+}
+
+rectangle "Instrumentation" as IN TERTIARY_COLOR {
+    component "Metrics Collectors" as MC
+    component "Distributed Tracing" as DT
+    component "Log Aggregators" as LA
+}
+
+rectangle "Monitoring System" as MS SECONDARY_COLOR {
+    component "Prometheus" as PR
+    component "Grafana" as GR
+    component "Jaeger" as JA
+    component "ELK Stack" as ELK
+}
+
+rectangle "Performance Indicators" as PI QUATERNARY_COLOR {
+    component "Latency (ms)" as LM
+    component "Throughput (req/s)" as TC
+    component "Error Rate (%)" as ERT
+    component "CPU/Memory Usage (%)" as RU
+}
+
+rectangle "Alerting System" as AS QUINARY_COLOR {
+    component "Alert Manager" as AM
+    component "Notification Service" as NS
+}
+
+rectangle "User Interface" as UI PRIMARY_COLOR {
+    component "Dashboard" as DB
+    component "Alert Configuration" as AC
+}
+
+DP -[PRIMARY_COLOR,thickness=2]-> IN : <back:#FFFFFF><color:PRIMARY_COLOR>1. Instrument</color></back>
+IN -[SECONDARY_COLOR,thickness=2]-> MS : <back:#FFFFFF><color:SECONDARY_COLOR>2. Collect Data</color></back>
+MS -[TERTIARY_COLOR,thickness=2]-> PI : <back:#FFFFFF><color:TERTIARY_COLOR>3. Calculate KPIs</color></back>
+PI -[QUATERNARY_COLOR,thickness=2]-> AS : <back:#FFFFFF><color:QUATERNARY_COLOR>4. Trigger Alerts</color></back>
+AS -[QUINARY_COLOR,thickness=2]-> NS : <back:#FFFFFF><color:QUINARY_COLOR>5. Notify</color></back>
+MS -[PRIMARY_COLOR,thickness=2]-> UI : <back:#FFFFFF><color:PRIMARY_COLOR>6. Visualize</color></back>
+UI -[SECONDARY_COLOR,thickness=2]-> AS : <back:#FFFFFF><color:SECONDARY_COLOR>7. Configure</color></back>
+
+note right of DP
+  Process and analyze
+  incoming data
+end note
+
+note right of IN
+  Add performance monitoring
+  metrics, distributed tracing,
+  and log collection to code
+end note
+
+note right of MS
+  Collect, store, and
+  process performance data
+end note
+
+note right of PI
+  Calculate key performance
+  indicators such as response
+  time, throughput, etc.
+end note
+
+note right of AS
+  Trigger alerts based on
+  preset thresholds, notify
+  relevant personnel
+end note
+
+note right of UI
+  Provide user interface to
+  view monitoring data and
+  configure alert rules
+end note
+
+@enduml
diff --git a/cross_datacenter_distributed_file_system_architecture_with_consistency_fault_tolerance_and_global_load_balancing.puml b/cross_datacenter_distributed_file_system_architecture_with_consistency_fault_tolerance_and_global_load_balancing.puml
new file mode 100644
--- /dev/null
+++ ./cross_datacenter_distributed_file_system_architecture_with_consistency_fault_tolerance_and_global_load_balancing.puml
@@ -0,0 +1,166 @@
+@startuml Optimized Cross-Datacenter Distributed File System Architecture
+!define RECTANGLE rectangle
+!define DATABASE database
+!define CLOUD cloud
+
+skinparam backgroundColor #F0F0F0
+skinparam handwritten false
+skinparam monochrome false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 20
+skinparam roundCorner 10
+skinparam ArrowColor #2C3E50
+skinparam ArrowThickness 1.5
+skinparam linetype ortho
+
+allowmixing
+
+' Global Components
+CLOUD "Global Load Balancer" as GLB #D6EAF8 {
+    component "Geo-aware Routing" as GeoRouting
+    component "Request Distribution" as ReqDist
+}
+
+RECTANGLE "Global Metadata Management" as GMM #AED6F1 {
+    component "Cross-DC Metadata Sync" as MetaSync
+    component "Global Namespace" as GlobalNS
+}
+
+RECTANGLE "Global Admin & Monitoring" as GAM #D5F5E3 {
+    component "Cross-DC Monitoring" as CrossDCMon
+    component "Disaster Recovery" as DR
+    component "Configuration Management" as ConfigMgmt
+}
+
+' Data Center 1
+RECTANGLE "Data Center 1" as DC1 #FDEBD0 {
+    ' Client
+    rectangle "Client" as Client1 #A9CCE3
+
+    ' Load Balancer Cluster
+    RECTANGLE "Load Balancer Cluster" as LB1 #F5B041 {
+        component "Health Check" as HealthCheck1
+        component "Request Router" as RequestRouter1
+    }
+
+    ' Caching Layer
+    RECTANGLE "Caching Layer" as CacheLayer1 #85C1E9 {
+        DATABASE "Redis Cluster" as RedisCache1 {
+            note right
+                Cache Structure:
+                1. File chunk location:
+                   Key: "chunk:{file_path}:{chunk_id}"
+                   Value: Node ID storing the chunk
+                2. File metadata:
+                   Key: "metadata:{file_path}"
+                   Value: Serialized file metadata
+            end note
+        }
+    }
+
+    ' Name Node Cluster
+    RECTANGLE "Name Node Cluster" as NameNodeCluster1 #82E0AA {
+        DATABASE "Metadata Storage\n(ZooKeeper)" as MetadataStorage1 {
+            note right
+                Stored Metadata:
+                1. File system tree:
+                   - Directory structure
+                   - File names and attributes
+                2. Block mappings:
+                   - File to data block mapping
+                   - Data block to storage node mapping
+                3. Node status:
+                   - Health status of data nodes
+                   - Storage capacity and usage
+                   - Current load information
+            end note
+        }
+        component "File System Manager" as FSManager1
+        component "Block Manager" as BlockManager1
+        component "Replication Manager" as ReplicationManager1
+        component "Data Node Manager" as DataNodeManager1
+    }
+
+    ' Data Node Clusters
+    RECTANGLE "Data Node Cluster" as DataNodeCluster1 #F1948A {
+        DATABASE "Primary Storage" as PrimaryStorage1 {
+            note right: "Sharding Key: file_id % 2 == 0"
+        }
+        DATABASE "Replica Storage" as ReplicaStorage1 {
+            note right: "Replica of DC2"
+        }
+    }
+
+    ' Read Coordinator
+    component "Read Coordinator" as ReadCoord1 #FFA07A
+}
+
+' Data Center 2 (more detailed now)
+RECTANGLE "Data Center 2" as DC2 #FDEBD0 {
+    rectangle "Client" as Client2 #A9CCE3
+    RECTANGLE "Load Balancer Cluster" as LB2 #F5B041
+    RECTANGLE "Caching Layer" as CacheLayer2 #85C1E9
+    RECTANGLE "Name Node Cluster" as NameNodeCluster2 #82E0AA {
+        DATABASE "Metadata Storage\n(ZooKeeper)" as MetadataStorage2
+    }
+    RECTANGLE "Data Node Cluster" as DataNodeCluster2 #F1948A {
+        DATABASE "Primary Storage" as PrimaryStorage2 {
+            note left: "Sharding Key: file_id % 2 == 1"
+        }
+        DATABASE "Replica Storage" as ReplicaStorage2 {
+            note left: "Replica of DC1"
+        }
+    }
+    component "Read Coordinator" as ReadCoord2 #FFA07A
+}
+
+' Connections
+Client1 --> GLB : "1. Client Request"
+Client2 --> GLB : "1. Client Request"
+GLB --> LB1 : "2. Route to nearest DC"
+GLB --> LB2 : "2. Route to nearest DC"
+LB1 --> ReadCoord1 : "3a. Read Request"
+LB1 --> NameNodeCluster1 : "3b. Write Request"
+ReadCoord1 --> CacheLayer1 : "4a. Check Cache"
+ReadCoord1 --> NameNodeCluster1 : "4b. Get Metadata"
+NameNodeCluster1 --> PrimaryStorage1 : "5a. Write Data"
+ReadCoord1 --> ReplicaStorage1 : "5b. Read Data"
+CacheLayer1 --> NameNodeCluster1 : "6. Cache Miss"
+PrimaryStorage1 --> ReplicaStorage2 : "7. Replicate Data"
+NameNodeCluster1 <--> GMM : "8a. Sync Metadata"
+NameNodeCluster2 <--> GMM : "8b. Sync Metadata"
+NameNodeCluster1 <--> NameNodeCluster2 : "9. Cross-DC Metadata Sync"
+DataNodeCluster1 <--> DataNodeCluster2 : "10. Cross-DC Data Replication"
+GAM --> DC1 : "11. Monitor & Manage"
+GAM --> DC2 : "11. Monitor & Manage"
+
+' Additional notes for explanation
+note top of GLB
+  Global Load Balancer provides 
+  geo-aware routing and ensures 
+  even distribution of requests 
+  across data centers
+end note
+
+note bottom of GMM
+  Global Metadata Management 
+  ensures consistency of metadata 
+  across data centers and manages 
+  the global namespace
+end note
+
+note bottom of GAM
+  Global Admin & Monitoring oversees 
+  system health across all data centers, 
+  manages configuration, and handles 
+  disaster recovery
+end note
+
+note right of DataNodeCluster1
+  Read-Write Separation:
+  - Write requests go to Primary Storage
+  - Read requests are served by Replica Storage
+  - Cross-DC replication ensures data consistency
+end note
+
+@enduml
diff --git a/database_optimization_and_sharding_strategies.puml b/database_optimization_and_sharding_strategies.puml
new file mode 100644
--- /dev/null
+++ ./database_optimization_and_sharding_strategies.puml
@@ -0,0 +1,113 @@
+@startuml Database Optimization and Sharding Strategies
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Database Optimization and Sharding Strategies
+
+rectangle "Database Optimization" as DBOpt #E1F5FE {
+    component "Query Optimization" as QueryOpt
+    component "Indexing Strategy" as IndexStrat
+    component "Caching Layer" as CacheLayer
+    component "Read/Write Separation" as RWSepar
+}
+
+rectangle "Sharding Strategies" as ShardStrat #B3E5FC {
+    component "Range-Based Sharding" as RangeSharding
+    component "Hash-Based Sharding" as HashSharding
+    component "Directory-Based Sharding" as DirSharding
+}
+
+rectangle "Data Consistency" as DataCons #81D4FA {
+    component "Eventual Consistency" as EventCons
+    component "Strong Consistency" as StrongCons
+    component "Quorum-Based Consistency" as QuorumCons
+}
+
+rectangle "High Availability" as HighAvail #4FC3F7 {
+    component "Replication" as Replication
+    component "Failover Mechanism" as Failover
+    component "Load Balancing" as LoadBal
+}
+
+database "Primary Database" as PrimaryDB #03A9F4
+database "Shard 1" as Shard1 #29B6F6
+database "Shard 2" as Shard2 #29B6F6
+database "Shard N" as ShardN #29B6F6
+
+PrimaryDB -[#FF5722,thickness=2]down-> Shard1 : <back:#FFFFFF><color:#FF5722>1. Distribute Data</color></back>
+PrimaryDB -[#FF5722,thickness=2]down-> Shard2 : <back:#FFFFFF><color:#FF5722>1. Distribute Data</color></back>
+PrimaryDB -[#FF5722,thickness=2]down-> ShardN : <back:#FFFFFF><color:#FF5722>1. Distribute Data</color></back>
+
+DBOpt -[#FF9800,thickness=2]right-> ShardStrat : <back:#FFFFFF><color:#FF9800>2. Apply Sharding</color></back>
+ShardStrat -[#FFC107,thickness=2]right-> DataCons : <back:#FFFFFF><color:#FFC107>3. Ensure Consistency</color></back>
+DataCons -[#4CAF50,thickness=2]right-> HighAvail : <back:#FFFFFF><color:#4CAF50>4. Maintain Availability</color></back>
+
+note bottom of DBOpt
+  Query Optimization:
+  - Use EXPLAIN to analyze query execution plans
+  - Optimize complex queries and joins
+  - Use appropriate data types
+
+  Indexing Strategy:
+  - Create indexes on frequently queried columns
+  - Use composite indexes for multi-column queries
+  - Regularly analyze and update indexes
+
+  Caching Layer:
+  - Implement application-level caching (e.g., Redis)
+  - Use database query cache wisely
+
+  Read/Write Separation:
+  - Direct reads to replicas
+  - Use write-through or write-behind caching
+end note
+
+note bottom of ShardStrat
+  Range-Based Sharding:
+  - Shard by date ranges or alphabetical ranges
+  - Good for time-series data or geographically distributed data
+
+  Hash-Based Sharding:
+  - Use a hash function to determine shard
+  - Ensures even distribution but can complicate range queries
+
+  Directory-Based Sharding:
+  - Use a lookup service to map data to shards
+  - Flexible but adds complexity and potential bottleneck
+end note
+
+note bottom of DataCons
+  Eventual Consistency:
+  - Faster writes, but reads may be stale
+  - Suitable for systems that can tolerate temporary inconsistencies
+
+  Strong Consistency:
+  - Ensures all reads reflect the latest write
+  - Can impact performance and availability
+
+  Quorum-Based Consistency:
+  - Balance between consistency and availability
+  - Configurable read/write quorums
+end note
+
+note bottom of HighAvail
+  Replication:
+  - Implement master-slave or multi-master replication
+  - Consider read replicas for scaling read operations
+
+  Failover Mechanism:
+  - Implement automatic failover to standby nodes
+  - Use health checks to detect node failures
+
+  Load Balancing:
+  - Distribute queries across multiple database instances
+  - Consider connection pooling for efficient resource use
+end note
+
+@enduml
+
diff --git a/database_performance_optimization_strategies_indexing_query_caching_partitioning_and_configuration.puml b/database_performance_optimization_strategies_indexing_query_caching_partitioning_and_configuration.puml
new file mode 100644
--- /dev/null
+++ ./database_performance_optimization_strategies_indexing_query_caching_partitioning_and_configuration.puml
@@ -0,0 +1,125 @@
+@startuml Database Software Optimization Strategies
+
+skinparam backgroundColor #F5F5F5
+skinparam defaultFontName Arial
+skinparam titleFontSize 20
+skinparam titleFontColor #333333
+skinparam classFontSize 14
+skinparam classAttributeFontSize 12
+skinparam noteFontSize 11
+skinparam noteFontColor #333333
+
+title Database Software Optimization Strategies
+
+class "Database Optimization" as Optimization #FFFFFF {
+    + Index Optimization
+    + Query Optimization
+    + Table Structure Adjustment
+    + Data Partitioning
+    + Caching Strategy
+    + Database Configuration
+    + Monitoring and Tuning
+}
+
+class "Index Optimization" as IndexOptimization #F8D7DA {
+    + Add Missing Indexes
+    + Remove Redundant Indexes
+    + Use Composite Indexes
+    + Implement Covering Indexes
+    + Analyze Index Usage
+}
+note bottom of IndexOptimization
+  Key to improving query performance
+  Reduces full table scans
+  Accelerates data retrieval
+end note
+
+class "Query Optimization" as QueryOptimization #D4EDDA {
+    + Avoid Full Table Scans
+    + Optimize Subqueries
+    + Use JOINs Instead of Subqueries
+    + Utilize Explain Plan
+    + Optimize WHERE Clauses
+    + Use Stored Procedures
+}
+note right of QueryOptimization
+  Improves SQL execution efficiency
+  Reduces resource consumption
+  Faster response times
+end note
+
+class "Table Structure Adjustment" as TableOptimization #E2E3E5 {
+    + Normalize Data
+    + Denormalize When Necessary
+    + Optimize Data Types
+    + Implement Vertical Partitioning
+    + Use Appropriate Primary Keys
+}
+note bottom of TableOptimization
+  Optimizes data storage structure
+  Improves data integrity
+  Enhances query efficiency
+end note
+
+class "Data Partitioning" as Partitioning #D1ECF1 {
+    + Time-Based Partitioning
+    + Key-Based Partitioning
+    + List Partitioning
+    + Hash Partitioning
+    + Range Partitioning
+}
+note left of Partitioning
+  Distributes data storage
+  Improves parallel processing
+  Enhances performance for large tables
+end note
+
+class "Caching Strategy" as Caching #FFF3CD {
+    + Query Result Caching
+    + Object Caching
+    + Page Caching
+    + Implement Redis/Memcached
+    + Database Buffer Cache Tuning
+}
+note right of Caching
+  Reduces database load
+  Improves response speed
+  Decreases I/O operations
+end note
+
+class "Database Configuration" as Configuration #FFE4B5 {
+    + Optimize Buffer Pool Size
+    + Tune Query Cache
+    + Adjust Thread Pool
+    + Configure I/O Parameters
+    + Set Appropriate Isolation Levels
+}
+note left of Configuration
+  Adjusts DB parameters for hardware
+  and business requirements
+  Optimizes resource utilization
+end note
+
+class "Monitoring and Tuning" as Monitoring #98FB98 {
+    + Use Performance Schema
+    + Implement Slow Query Log
+    + Regular Performance Reviews
+    + Automated Monitoring Tools
+    + Query Plan Analysis
+}
+note top of Monitoring
+  Continuous performance monitoring
+  Timely detection and resolution
+  of potential issues
+end note
+
+Optimization -down-> IndexOptimization
+Optimization -down-> QueryOptimization
+Optimization -down-> TableOptimization
+Optimization -down-> Partitioning
+Optimization -[hidden]right-> Caching
+Caching -up-> Optimization
+Configuration -up-> Optimization
+Monitoring -up-> Optimization
+
+@enduml
diff --git a/distributed_cache_system_with_consistency_eviction_cache_warming_and_bloom_filter.puml b/distributed_cache_system_with_consistency_eviction_cache_warming_and_bloom_filter.puml
new file mode 100644
--- /dev/null
+++ ./distributed_cache_system_with_consistency_eviction_cache_warming_and_bloom_filter.puml
@@ -0,0 +1,104 @@
+@startuml Distributed Cache System with Consistency Strategies
+
+!pragma layout dot
+
+allowmixing
+
+' 定义颜色变量
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+!define BG_COLOR #CCE8CF
+
+skinparam backgroundColor BG_COLOR
+
+' 定义组件
+rectangle "Client" as Client PRIMARY_COLOR {
+    component "Client App" as ClientApp
+}
+
+rectangle "Load Balancer" as LB SECONDARY_COLOR {
+    component "Request Router" as RequestRouter
+}
+
+rectangle "Cache Cluster" as CC TERTIARY_COLOR {
+    component "Cache Nodes" as CacheNodes
+    component "Eviction Policy" as EvictionPolicy
+}
+
+rectangle "Consistency Manager" as CM QUATERNARY_COLOR {
+    component "Consistency Checker" as ConsistencyChecker
+}
+
+rectangle "Persistence Store" as PS QUINARY_COLOR {
+    component "Database" as Database
+}
+
+rectangle "Cache Warmer" as CW PRIMARY_COLOR {
+    component "Data Preloader" as DataPreloader
+}
+
+rectangle "Bloom Filter" as BF SECONDARY_COLOR {
+    component "Key Filter" as KeyFilter
+}
+
+rectangle "Monitoring & Alerts" as MA TERTIARY_COLOR {
+    component "Performance Monitor" as PerformanceMonitor
+}
+
+' 连接和流程
+Client -[PRIMARY_COLOR,thickness=2]-> LB : <back:#FFFFFF><color:PRIMARY_COLOR>1. Request</color></back>
+LB -[SECONDARY_COLOR,thickness=2]-> CC : <back:#FFFFFF><color:SECONDARY_COLOR>2. Route request</color></back>
+CC -[TERTIARY_COLOR,thickness=2]-> BF : <back:#FFFFFF><color:TERTIARY_COLOR>3. Filter non-existent keys</color></back>
+BF -[QUATERNARY_COLOR,thickness=2]-> CM : <back:#FFFFFF><color:QUATERNARY_COLOR>4. Ensure consistency</color></back>
+CM -[QUINARY_COLOR,thickness=2]-> PS : <back:#FFFFFF><color:QUINARY_COLOR>5. Sync if needed</color></back>
+CC -[PRIMARY_COLOR,thickness=2]-> PS : <back:#FFFFFF><color:PRIMARY_COLOR>6. Read/Write through</color></back>
+
+PS -[SECONDARY_COLOR,thickness=2,dashed]-> CC : <back:#FFFFFF><color:SECONDARY_COLOR>7. Return data (if not in cache)</color></back>
+CC -[TERTIARY_COLOR,thickness=2,dashed]-> LB : <back:#FFFFFF><color:TERTIARY_COLOR>8. Return cached/fetched data</color></back>
+LB -[QUATERNARY_COLOR,thickness=2,dashed]-> Client : <back:#FFFFFF><color:QUATERNARY_COLOR>9. Return response to client</color></back>
+
+CW -[QUINARY_COLOR,thickness=2]-> CC : <back:#FFFFFF><color:QUINARY_COLOR>10. Preload hot data</color></back>
+MA -[PRIMARY_COLOR,thickness=2]-> CC : <back:#FFFFFF><color:PRIMARY_COLOR>11. Monitor performance</color></back>
+
+' 注释
+note bottom of CM
+  Consistency strategies:
+  - Strong consistency
+  - Eventual consistency
+  - Read-your-writes consistency
+  - Causal consistency
+  - Session consistency
+end note
+
+note right of CC
+  Cache eviction policies:
+  - LRU (Least Recently Used)
+  - LFU (Least Frequently Used)
+  - FIFO (First In First Out)
+  - Time-based expiration
+end note
+
+note bottom of CW
+  Cache warming strategies:
+  - Periodic data preloading
+  - On-demand preloading
+  - Gradual cache population
+end note
+
+note right of BF
+  Cache penetration prevention:
+  - Filters out requests for non-existent keys
+  - Reduces unnecessary database queries
+end note
+
+note bottom of PS
+  Write strategies:
+  - Write-through
+  - Write-behind (Write-back)
+  - Write-around
+end note
+
+@enduml
diff --git a/distributed_configuration_center_with_caching_change_notification_and_access_control.puml b/distributed_configuration_center_with_caching_change_notification_and_access_control.puml
new file mode 100644
--- /dev/null
+++ ./distributed_configuration_center_with_caching_change_notification_and_access_control.puml
@@ -0,0 +1,114 @@
+@startuml Advanced Distributed Configuration Center
+
+!define PRIMARY_COLOR #4CAF50
+!define SECONDARY_COLOR #FFA000
+!define ACCENT_COLOR #00BCD4
+!define DARK_COLOR #607D8B
+!define LIGHT_COLOR #ECEFF1
+!define ERROR_COLOR #FF5252
+!define CACHE_COLOR #FF9800
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontSize 14
+skinparam componentFontSize 16
+skinparam noteFontSize 14
+allowmixing
+!pragma layout dot
+
+rectangle "Client Applications" as ClientApps PRIMARY_COLOR {
+    component "Web App" as WebApp
+    component "Mobile App" as MobileApp
+    component "Microservices" as Microservices
+    component "Client SDK" as ClientSDK
+    component "Local Cache" as LocalCache CACHE_COLOR
+}
+
+rectangle "Configuration Center" as ConfigCenter SECONDARY_COLOR {
+    component "API Gateway" as APIGateway
+    component "Load Balancer" as LoadBalancer
+    component "Config Service" as ConfigService
+    component "Version Control" as VersionControl
+    component "Change Notifier" as ChangeNotifier
+    component "Environment Manager" as EnvManager
+    component "Access Control" as AccessControl
+    component "Audit Logger" as AuditLogger
+    component "Encryption Service" as EncryptionService
+}
+
+database "Configuration Storage" as ConfigStorage ACCENT_COLOR {
+    component "Config Data" as ConfigData
+    component "Version History" as VersionHistory
+    component "Audit Logs" as AuditLogs
+}
+
+rectangle "Admin Interface" as AdminInterface DARK_COLOR {
+    component "Web Console" as WebConsole
+    component "CLI Tool" as CLITool
+    component "API Documentation" as APIDocs
+}
+
+cloud "External Services" as ExternalServices LIGHT_COLOR {
+    component "Monitoring" as Monitoring
+    component "Alerting" as Alerting
+    component "Analytics" as Analytics
+}
+
+database "Caching Layer" as CachingLayer CACHE_COLOR {
+    component "Redis Cluster" as RedisCluster
+}
+
+ClientApps -[PRIMARY_COLOR,thickness=2]down-> LoadBalancer : <back:#FFFFFF><color:PRIMARY_COLOR>1. Request config</color></back>
+LoadBalancer -[SECONDARY_COLOR,thickness=2]right-> APIGateway : <back:#FFFFFF><color:SECONDARY_COLOR>2. Route request</color></back>
+APIGateway -[SECONDARY_COLOR,thickness=2]right-> ConfigService : <back:#FFFFFF><color:SECONDARY_COLOR>3. Fetch config</color></back>
+ConfigService -[ACCENT_COLOR,thickness=2]down-> CachingLayer : <back:#FFFFFF><color:ACCENT_COLOR>4. Check cache</color></back>
+ConfigService -[ACCENT_COLOR,thickness=2]right-> ConfigStorage : <back:#FFFFFF><color:ACCENT_COLOR>5. Read/Write if not in cache</color></back>
+AdminInterface -[DARK_COLOR,thickness=2]up-> APIGateway : <back:#FFFFFF><color:DARK_COLOR>6. Manage configs</color></back>
+ChangeNotifier -[SECONDARY_COLOR,thickness=2]up-> ClientApps : <back:#FFFFFF><color:SECONDARY_COLOR>7. Push updates</color></back>
+ConfigCenter -[LIGHT_COLOR,thickness=2]right-> ExternalServices : <back:#FFFFFF><color:LIGHT_COLOR>8. Send metrics and logs</color></back>
+
+note top of VersionControl
+  Git-like version control
+  Supports branching and merging
+  Enables easy rollbacks
+end note
+
+note bottom of EnvManager
+  Manages configs for multiple
+  environments and regions
+  Supports config inheritance
+end note
+
+note top of AccessControl
+  Role-based access control (RBAC)
+  Multi-factor authentication
+  Integration with SSO systems
+end note
+
+note bottom of ChangeNotifier
+  Supports various protocols:
+  WebSocket, Long Polling, Kafka
+end note
+
+note top of EncryptionService
+  Handles encryption/decryption
+  of sensitive config values
+  Supports key rotation
+end note
+
+note left of ClientSDK
+  Provides easy integration
+  Handles caching and failover
+end note
+
+note right of RedisCluster
+  Distributed caching
+  Improves read performance
+  Reduces database load
+end note
+
+note bottom of AuditLogger
+  Logs all config changes
+  Supports compliance requirements
+end note
+
+@enduml
diff --git a/distributed_configuration_center_with_version_control_real_time_updates_and_multi_environment_management.puml b/distributed_configuration_center_with_version_control_real_time_updates_and_multi_environment_management.puml
new file mode 100644
--- /dev/null
+++ ./distributed_configuration_center_with_version_control_real_time_updates_and_multi_environment_management.puml
@@ -0,0 +1,114 @@
+@startuml Advanced Distributed Configuration Center
+
+!define PRIMARY_COLOR #4CAF50
+!define SECONDARY_COLOR #FFA000
+!define ACCENT_COLOR #00BCD4
+!define DARK_COLOR #607D8B
+!define LIGHT_COLOR #ECEFF1
+!define ERROR_COLOR #FF5252
+!define CACHE_COLOR #FF9800
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontSize 18
+skinparam componentFontSize 18
+skinparam noteFontSize 18
+allowmixing
+!pragma layout dot
+
+rectangle "Client Applications" as ClientApps PRIMARY_COLOR {
+    component "Web App" as WebApp
+    component "Mobile App" as MobileApp
+    component "Microservices" as Microservices
+    component "Client SDK" as ClientSDK
+    component "Local Cache" as LocalCache CACHE_COLOR
+}
+
+rectangle "Configuration Center" as ConfigCenter SECONDARY_COLOR {
+    component "API Gateway" as APIGateway
+    component "Load Balancer" as LoadBalancer
+    component "Config Service" as ConfigService
+    component "Version Control" as VersionControl
+    component "Change Notifier" as ChangeNotifier
+    component "Environment Manager" as EnvManager
+    component "Access Control" as AccessControl
+    component "Audit Logger" as AuditLogger
+    component "Encryption Service" as EncryptionService
+}
+
+database "Configuration Storage" as ConfigStorage ACCENT_COLOR {
+    component "Config Data" as ConfigData
+    component "Version History" as VersionHistory
+    component "Audit Logs" as AuditLogs
+}
+
+rectangle "Admin Interface" as AdminInterface DARK_COLOR {
+    component "Web Console" as WebConsole
+    component "CLI Tool" as CLITool
+    component "API Documentation" as APIDocs
+}
+
+cloud "External Services" as ExternalServices LIGHT_COLOR {
+    component "Monitoring" as Monitoring
+    component "Alerting" as Alerting
+    component "Analytics" as Analytics
+}
+
+database "Caching Layer" as CachingLayer CACHE_COLOR {
+    component "Redis Cluster" as RedisCluster
+}
+
+ClientApps -[PRIMARY_COLOR,thickness=2]down-> LoadBalancer : <back:#FFFFFF><color:PRIMARY_COLOR>1. Request config</color></back>
+LoadBalancer -[SECONDARY_COLOR,thickness=2]right-> APIGateway : <back:#FFFFFF><color:SECONDARY_COLOR>2. Route request</color></back>
+APIGateway -[SECONDARY_COLOR,thickness=2]right-> ConfigService : <back:#FFFFFF><color:SECONDARY_COLOR>3. Fetch config</color></back>
+ConfigService -[ACCENT_COLOR,thickness=2]down-> CachingLayer : <back:#FFFFFF><color:ACCENT_COLOR>4. Check cache</color></back>
+ConfigService -[ACCENT_COLOR,thickness=2]right-> ConfigStorage : <back:#FFFFFF><color:ACCENT_COLOR>5. Read/Write if not in cache</color></back>
+AdminInterface -[DARK_COLOR,thickness=2]up-> APIGateway : <back:#FFFFFF><color:DARK_COLOR>6. Manage configs</color></back>
+ChangeNotifier -[SECONDARY_COLOR,thickness=2]up-> ClientApps : <back:#FFFFFF><color:SECONDARY_COLOR>7. Push updates</color></back>
+ConfigCenter -[LIGHT_COLOR,thickness=2]right-> ExternalServices : <back:#FFFFFF><color:LIGHT_COLOR>8. Send metrics and logs</color></back>
+
+note right of VersionControl
+  Git-like version control
+  Supports branching and merging
+  Enables easy rollbacks
+end note
+
+note bottom of EnvManager
+  Manages configs for multiple
+  environments and regions
+  Supports config inheritance
+end note
+
+note left of AccessControl
+  Role-based access control (RBAC)
+  Multi-factor authentication
+  Integration with SSO systems
+end note
+
+note bottom of ChangeNotifier
+  Supports various protocols:
+  WebSocket, Long Polling, Kafka
+end note
+
+note top of EncryptionService
+  Handles encryption/decryption
+  of sensitive config values
+  Supports key rotation
+end note
+
+note left of ClientSDK
+  Provides easy integration
+  Handles caching and failover
+end note
+
+note right of RedisCluster
+  Distributed caching
+  Improves read performance
+  Reduces database load
+end note
+
+note bottom of AuditLogger
+  Logs all config changes
+  Supports compliance requirements
+end note
+
+@enduml
diff --git a/distributed_file_storage_sync_system_with_metadata_management_analytics_and_version_control.puml b/distributed_file_storage_sync_system_with_metadata_management_analytics_and_version_control.puml
new file mode 100644
--- /dev/null
+++ ./distributed_file_storage_sync_system_with_metadata_management_analytics_and_version_control.puml
@@ -0,0 +1,122 @@
+@startuml Dropbox-like Distributed Storage System Architecture
+
+!pragma layout dot
+skinparam backgroundColor #FAFAFA
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundCorner 10
+skinparam ArrowColor #2C3E50
+skinparam ArrowThickness 1.5
+
+allowmixing
+
+!define PRIMARY_COLOR #3498DB
+!define SECONDARY_COLOR #2ECC71
+!define TERTIARY_COLOR #E74C3C
+!define QUATERNARY_COLOR #F39C12
+!define QUINARY_COLOR #9B59B6
+
+rectangle "Client Applications" as ClientApps PRIMARY_COLOR {
+    component "Desktop Client" as DesktopClient
+    component "Mobile Client" as MobileClient
+    component "Web Interface" as WebInterface
+}
+
+cloud "Internet" as Internet SECONDARY_COLOR
+
+rectangle "Global Load Balancer" as GLB QUATERNARY_COLOR {
+    component "Geo-DNS" as GeoDNS
+    component "Traffic Distribution" as TrafficDist
+}
+
+rectangle "API Gateway" as APIGateway TERTIARY_COLOR {
+    component "Authentication" as Auth
+    component "Rate Limiting" as RateLimit
+    component "Request Routing" as ReqRouting
+}
+
+rectangle "Application Servers" as AppServers PRIMARY_COLOR {
+    component "File Sync Service" as FileSyncService
+    component "Sharing Service" as SharingService
+    component "Version Control" as VersionControl
+}
+
+rectangle "Metadata Management" as MetadataManagement SECONDARY_COLOR {
+    component "File Metadata DB" as FileMetadataDB
+    component "User Metadata DB" as UserMetadataDB
+}
+
+rectangle "Storage Layer" as StorageLayer QUINARY_COLOR {
+    component "Object Storage" as ObjectStorage
+    component "Block Storage" as BlockStorage
+}
+
+rectangle "Caching Layer" as CachingLayer QUATERNARY_COLOR {
+    component "Redis Cluster" as RedisCluster
+}
+
+rectangle "Notification Service" as NotificationService TERTIARY_COLOR {
+    component "Real-time Updates" as RealtimeUpdates
+    component "Push Notifications" as PushNotifications
+}
+
+rectangle "Analytics & Monitoring" as AnalyticsMonitoring PRIMARY_COLOR {
+    component "Usage Analytics" as UsageAnalytics
+    component "System Monitoring" as SystemMonitoring
+}
+
+ClientApps -[#000000,thickness=2]-> Internet : <back:#FFFFFF><color:#000000>1. User Requests</color></back>
+Internet -[#1E8449,thickness=2]-> GLB : <back:#FFFFFF><color:#1E8449>2. Route Requests</color></back>
+GLB -[#922B21,thickness=2]-> APIGateway : <back:#FFFFFF><color:#922B21>3. Authenticate & Route</color></back>
+APIGateway -[#7D3C98,thickness=2]-> AppServers : <back:#FFFFFF><color:#7D3C98>4. Process Requests</color></back>
+AppServers -[#2E86C1,thickness=2]-> MetadataManagement : <back:#FFFFFF><color:#2E86C1>5. Query/Update Metadata</color></back>
+AppServers -[#D35400,thickness=2]-> StorageLayer : <back:#FFFFFF><color:#D35400>6. Read/Write Files</color></back>
+AppServers -[#1ABC9C,thickness=2]-> CachingLayer : <back:#FFFFFF><color:#1ABC9C>7. Cache Data</color></back>
+AppServers -[#8E44AD,thickness=2]-> NotificationService : <back:#FFFFFF><color:#8E44AD>8. Send Updates</color></back>
+NotificationService -[#2980B9,thickness=2]-> ClientApps : <back:#FFFFFF><color:#2980B9>9. Push Notifications</color></back>
+AnalyticsMonitoring -[#27AE60,thickness=2]-> AppServers : <back:#FFFFFF><color:#27AE60>10. Monitor & Analyze</color></back>
+
+note right of ClientApps
+  Clients sync files and
+  receive real-time updates
+end note
+
+note right of GLB
+  Distributes traffic based on
+  geographic location and load
+end note
+
+note right of APIGateway
+  Handles authentication, rate limiting,
+  and routes requests to appropriate services
+end note
+
+note right of AppServers
+  Core services for file synchronization,
+  sharing, and version control
+end note
+
+note right of MetadataManagement
+  Stores and manages file and user metadata
+end note
+
+note right of StorageLayer
+  Scalable storage for file contents
+  using object and block storage
+end note
+
+note right of CachingLayer
+  Improves performance by caching
+  frequently accessed data
+end note
+
+note right of NotificationService
+  Provides real-time updates and
+  push notifications to clients
+end note
+
+note right of AnalyticsMonitoring
+  Tracks system health and user behavior
+end note
+
+@enduml
\ No newline at end of file
diff --git a/distributed_file_system/dfs-load-balancer.puml b/distributed_file_system/dfs-load-balancer.puml
new file mode 100644
--- /dev/null
+++ ./distributed_file_system/dfs-load-balancer.puml
@@ -0,0 +1,47 @@
+@startuml dfs-load-balancer
+!define RECTANGLE rectangle
+skinparam backgroundColor #E0E0E0
+skinparam handwritten false
+skinparam monochrome false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundCorner 25
+skinparam Padding 40
+skinparam ParticipantPadding 60
+skinparam BoxPadding 60
+skinparam shadowing false
+skinparam ArrowThickness 1.5
+skinparam NoteBackgroundColor #FFFAB1
+skinparam NoteBorderColor #7F7F00
+skinparam linetype polyline
+skinparam ArrowColor #2C3E50
+
+!define BLUE #0066CC
+!define GREEN #00A86B
+!define YELLOW #FFD700
+
+rectangle "Client" as Client #A9CCE3
+
+RECTANGLE "Load Balancer Cluster" as LB #F5B041 {
+    component "Health Check" as HealthCheck
+    component "Request Router" as RequestRouter
+    component "Session Persistence" as SessionPersistence
+    component "SSL Termination" as SSLTermination
+    component "Traffic Shaping" as TrafficShaping
+}
+
+Client -[#0066CC]-> SSLTermination : <color:#0066CC>1. HTTPS Request</color>
+SSLTermination -[#00A86B]-> RequestRouter : <color:#00A86B>2. Decrypted Request</color>
+RequestRouter -[#8B4513]-> HealthCheck : <color:#8B4513>3. Check Server Health</color>
+HealthCheck -[#8B4513]-> RequestRouter : <color:#8B4513>4. Health Status</color>
+RequestRouter -[#4B0082]-> SessionPersistence : <color:#4B0082>5. Route Request</color>
+SessionPersistence -[#00A86B]-> TrafficShaping : <color>#00A86B>6. Apply Session Rules</color>
+TrafficShaping -[#0066CC]-> [Backend] : <color:#0066CC>7. Forward to Backend</color>
+
+note bottom of HealthCheck : Continuously monitors backend server health
+note right of RequestRouter : Distributes requests based on various algorithms
+note bottom of SessionPersistence : Ensures related requests go to the same server
+note left of SSLTermination : Handles encryption/decryption to offload backend servers
+note bottom of TrafficShaping : Applies rate limiting and prioritization rules
+
+@enduml
diff --git a/distributed_file_system/distributed_file_system_database_schema_with_namenode_and_datanode_components.puml b/distributed_file_system/distributed_file_system_database_schema_with_namenode_and_datanode_components.puml
new file mode 100644
--- /dev/null
+++ ./distributed_file_system/distributed_file_system_database_schema_with_namenode_and_datanode_components.puml
@@ -0,0 +1,153 @@
+@startuml HDFS Data Model
+!define TABLE(name,desc) class name as "desc" << (T,#FFAAAA) >>
+!define PK(x) <u>x</u>
+!define FK(x) <i>x</i>
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam monochrome false
+skinparam lineType ortho
+skinparam shadowing false
+skinparam class {
+    BackgroundColor #E0F2F1
+    ArrowColor #4A4A4A
+    BorderColor #1A237E
+    FontName Arial
+    FontSize 10
+}
+skinparam note {
+    BackgroundColor #FFF9C4
+    BorderColor #FBC02D
+}
+' NameNode Components
+rectangle "NameNode" as NameNode #E8F5E9 {
+    TABLE(FileMetadata, "File Metadata") {
+        PK(file_path): STRING
+        file_name: STRING
+        file_size: LONG
+        owner: STRING
+        permissions: STRING
+        created_at: LONG
+        updated_at: LONG
+        is_directory: BOOLEAN
+        replication_factor: SHORT
+    }
+    TABLE(BlockMetadata, "Block Metadata") {
+        PK(block_id): LONG
+        FK(file_path): STRING
+        block_size: LONG
+        block_locations: LIST<DataNodeID>
+        generation_stamp: LONG
+    }
+    TABLE(DataNodeInfo, "DataNode Info") {
+        PK(node_id): STRING
+        hostname: STRING
+        ip_address: STRING
+        total_space: LONG
+        used_space: LONG
+        last_heartbeat: LONG
+        status: STRING
+        rack_id: STRING
+    }
+}
+
+note right of NameNode
+    NameNode is the core of HDFS, responsible for:
+    1. Managing the file system namespace
+    2. Maintaining metadata for files and directories
+    3. Managing the mapping of data blocks to DataNodes
+    4. Handling client read/write requests
+end note
+
+' NameNode Persistent Storage
+rectangle "NameNode Persistent Storage" as NameNodeStorage #FFF3E0 {
+    TABLE(EditLog, "Edit Log") {
+        PK(transaction_id): LONG
+        operation_type: STRING
+        operation_details: STRING
+        timestamp: LONG
+    }
+    TABLE(FSImage, "FSImage") {
+        PK(checkpoint_txid): LONG
+        file_system_metadata: BYTE[]
+        timestamp: LONG
+    }
+}
+
+note bottom of NameNodeStorage
+    Persistent storage ensures system recoverability:
+    - EditLog records all file system operations
+    - FSImage is a snapshot of file system metadata
+    Periodically merge EditLog into FSImage for efficiency
+end note
+
+' DataNode Components
+rectangle "DataNode" as DataNode #E1F5FE {
+    TABLE(DataBlocks, "Data Blocks") {
+        PK(block_id): LONG
+        data: BYTE[]
+        checksum: BYTE[]
+    }
+    TABLE(BlockReport, "Block Report") {
+        FK(node_id): STRING
+        FK(block_id): LONG
+        block_length: LONG
+        generation_stamp: LONG
+    }
+}
+
+note left of DataNode
+    DataNode is responsible for:
+    1. Storing actual data blocks
+    2. Handling read/write requests for data blocks
+    3. Periodically reporting block status to NameNode
+    4. Executing block replication and deletion
+end note
+
+' Cache Layer
+rectangle "Cache Layer" as CacheLayer #F3E5F5 {
+    TABLE(RedisCache, "Redis Cache Cluster") {
+        key: STRING
+        value: STRING
+        expiration: INT
+    }
+}
+
+note bottom of RedisCache
+    Keys:
+    file_metadata:{file_path}
+    block_locations:{block_id}
+end note
+
+note right of CacheLayer
+    Redis cache layer is used for:
+    1. Accelerating metadata access
+    2. Reducing NameNode load
+    3. Improving read performance for hot data
+end note
+
+' Relationships
+FileMetadata "1" -- "0..*" BlockMetadata
+BlockMetadata "0..*" -- "1..*" DataNodeInfo
+DataNodeInfo "1" -- "0..*" BlockReport
+BlockMetadata "1" -- "1" DataBlocks
+EditLog "1..*" -- "0..1" FSImage
+FileMetadata "1" -- "0..*" EditLog
+BlockMetadata "1" -- "0..*" EditLog
+RedisCache "0..*" -- "1" FileMetadata
+RedisCache "0..*" -- "1" BlockMetadata
+
+' Layout
+NameNode -[hidden]right- DataNode
+NameNode -[hidden]down- NameNodeStorage
+NameNodeStorage -[hidden]right- CacheLayer
+
+note as PerformanceNote
+Performance optimization and scalability considerations:
+1. Use Redis cache to reduce NameNode load
+2. Improve availability and read performance through data block replication
+3. Optimize data placement using rack awareness strategy
+4. Implement Secondary NameNode for faster failure recovery
+5. Consider HDFS Federation for horizontal scaling
+end note
+PerformanceNote -[hidden]right- CacheLayer
+@enduml
diff --git a/distributed_file_system/distributed_file_system_overview_with_components_and_data_flow.puml b/distributed_file_system/distributed_file_system_overview_with_components_and_data_flow.puml
new file mode 100644
--- /dev/null
+++ ./distributed_file_system/distributed_file_system_overview_with_components_and_data_flow.puml
@@ -0,0 +1,91 @@
+@startuml dfs-overview
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #D0D0D0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundCorner 15
+skinparam Padding 30
+skinparam ParticipantPadding 40
+skinparam BoxPadding 40
+skinparam shadowing false
+skinparam ArrowThickness 1.5
+skinparam NoteBackgroundColor #FFFAB1
+skinparam NoteBorderColor #7F7F00
+skinparam linetype ortho
+skinparam ArrowColor #2C3E50
+
+rectangle "Client Applications" as Client #87CEEB
+rectangle "Load Balancer Cluster" as LB #FFA500 {
+    component "Load Balancer 1" as LB1
+    component "Load Balancer 2" as LB2
+}
+rectangle "Caching Layer" as CacheLayer #4169E1 {
+    component "Redis Cluster" as Redis
+    component "Memcached" as Memcached
+}
+rectangle "Name Node Cluster" as NameNodeCluster #32CD32 {
+    component "Primary Name Node" as PNN
+    component "Secondary Name Node" as SNN
+}
+rectangle "Data Node Cluster 1" as DataNodeCluster1 #FF6347 {
+    component "Data Node 1.1" as DN11
+    component "Data Node 1.2" as DN12
+}
+rectangle "Data Node Cluster 2" as DataNodeCluster2 #FF6347 {
+    component "Data Node 2.1" as DN21
+    component "Data Node 2.2" as DN22
+}
+rectangle "Admin Node" as AdminNode #DDA0DD {
+    component "Monitoring System" as Monitor
+    component "Configuration Manager" as Config
+}
+
+Client -[#0000FF,thickness=2]-> LB : <color:#0000FF>1. Client Request</color>
+LB .[#006400,thickness=2]..> CacheLayer : <color:#006400>2a. Read Cache</color>
+LB -[#006400,thickness=2]right-> NameNodeCluster : <color:#006400>2b. Write/Read Metadata</color>
+CacheLayer .[#B8860B,thickness=2].> NameNodeCluster : <color:#B8860B>3. Cache Miss</color>
+NameNodeCluster -[#8B0000,thickness=2]down-> DataNodeCluster1 : <color:#8B0000>4a. Data Operations</color>
+NameNodeCluster -[#8B0000,thickness=2]down-> DataNodeCluster2 : <color:#8B0000>4b. Data Operations</color>
+DataNodeCluster1 .[#4B0082,thickness=2].> CacheLayer : <color:#4B0082>5a. Update Cache</color>
+DataNodeCluster2 .[#4B0082,thickness=2].> CacheLayer : <color:#4B0082>5b. Update Cache</color>
+AdminNode -[#FF4500,thickness=2]up-> NameNodeCluster : <color:#FF4500>6a. Manage & Monitor</color>
+AdminNode -[#FF4500,thickness=2]up-> LB : <color:#FF4500>6b. Manage & Monitor</color>
+DataNodeCluster1 <-[#006400,thickness=2]right-> DataNodeCluster2 : <color:#006400>7. Replicate Data</color>
+
+note bottom of Client #FFFACD
+    <color:#000000>Unified API for read/write operations
+    Supports multiple client protocols (e.g., REST, gRPC)</color>
+end note
+
+note bottom of LB
+    Distributes requests and ensures high availability
+    Implements health checks and automatic failover
+end note
+
+note right of CacheLayer
+    Improves read performance
+    Cache: key: file_id, value: file_content
+end note
+
+note right of NameNodeCluster
+    Manages metadata and ensures data consistency
+    Implements leader election for high availability
+end note
+
+note bottom of DataNodeCluster1
+    Stores file data with built-in replication
+    Implements data integrity checks
+end note
+
+note left of AdminNode
+    System health monitoring and configuration management
+    Provides centralized logging and alerting
+end note
+
+note bottom of DataNodeCluster1
+    Performance bottleneck: I/O operations
+    Optimization: SSD caching, data compression
+end note
+
+@enduml
diff --git a/distributed_id_generator_system_with_snowflake_algorithm_zookeeper_coordination_and_performance_monitoring.puml b/distributed_id_generator_system_with_snowflake_algorithm_zookeeper_coordination_and_performance_monitoring.puml
new file mode 100644
--- /dev/null
+++ ./distributed_id_generator_system_with_snowflake_algorithm_zookeeper_coordination_and_performance_monitoring.puml
@@ -0,0 +1,63 @@
+@startuml Distributed ID Generator System Design
+allowmixing
+
+!define RECTANGLE class
+!define STORAGE database
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Distributed ID Generator System Design
+
+rectangle "Client Applications" as ClientApps #E1F5FE
+
+rectangle "Load Balancer" as LB #B3E5FC
+
+rectangle "ID Generator Service Cluster" as IDGenCluster #81D4FA {
+    component "ID Generator Node 1" as IDGen1
+    component "ID Generator Node 2" as IDGen2
+    component "ID Generator Node N" as IDGenN
+}
+
+database "Zookeeper Cluster" as ZK #4FC3F7 {
+    component "Node Coordination"
+    component "Worker ID Assignment"
+}
+
+rectangle "Monitoring & Alerting" as Monitoring #03A9F4
+
+ClientApps -[#FF5722,thickness=2]down-> LB : <back:#FFFFFF><color:#FF5722>1. Request ID</color></back>
+LB -[#FF9800,thickness=2]down-> IDGenCluster : <back:#FFFFFF><color:#FF9800>2. Route Request</color></back>
+IDGenCluster -[#FFC107,thickness=2]right-> ZK : <back:#FFFFFF><color:#FFC107>3. Get Worker ID</color></back>
+IDGenCluster -[#4CAF50,thickness=2]up-> LB : <back:#FFFFFF><color:#4CAF50>4. Return Generated ID</color></back>
+LB -[#8BC34A,thickness=2]up-> ClientApps : <back:#FFFFFF><color:#8BC34A>5. Respond with ID</color></back>
+IDGenCluster -[#9C27B0,thickness=2]down-> Monitoring : <back:#FFFFFF><color:#9C27B0>6. Report Metrics</color></back>
+
+note right of IDGenCluster
+  ID Structure (64 bits):
+  - Timestamp: 41 bits
+  - Worker ID: 10 bits
+  - Sequence: 12 bits
+  - Sign bit: 1 bit (always 0)
+end note
+
+note bottom of ZK
+  Ensures unique worker IDs across the cluster:
+  1. Creates a sequential znode for each node
+  2. Assigns the znode's sequence number as Worker ID
+  3. Monitors node health with ephemeral znodes
+  4. Reassigns IDs on node failures
+  5. Prevents ID conflicts during scaling
+end note
+
+note right of Monitoring
+  Tracks ID generation rate,
+  system health, and alerts
+  on anomalies
+end note
+
+@enduml
diff --git a/distributed_rate_limiter_system_with_multiple_algorithms_and_redis.puml b/distributed_rate_limiter_system_with_multiple_algorithms_and_redis.puml
new file mode 100644
--- /dev/null
+++ ./distributed_rate_limiter_system_with_multiple_algorithms_and_redis.puml
@@ -0,0 +1,77 @@
+@startuml
+skinparam componentStyle rectangle
+
+package "Rate Limiter Service" {
+    component "API Gateway" as Gateway
+    
+    package "Rate Limiter Core" {
+        component "Token Bucket" as TokenBucket
+        component "Leaky Bucket" as LeakyBucket
+        component "Sliding Window" as SlidingWindow
+        component "Fixed Window" as FixedWindow
+        component "Algorithm Selector" as AlgorithmSelector
+    }
+    
+    package "Redis Cluster" {
+        component "Counter" as Counter
+        component "Window" as Window
+        component "Limits" as Limits
+    }
+    
+    package "Configuration Service" {
+        component "Rate Limit Rules" as Rules
+        component "Service Configs" as ServiceConfigs
+    }
+    
+    package "Monitoring & Alerts" {
+        component "Prometheus" as Prom
+        component "Grafana" as Graf
+        component "Alert Manager" as Alert
+    }
+}
+
+cloud "Client Applications" as Clients
+cloud "Protected Services" as Services
+
+Clients --> Gateway : HTTP/gRPC Requests
+Gateway --> TokenBucket : Rate Check
+TokenBucket --> Counter : Store/Retrieve
+TokenBucket --> Rules : Load Rules
+Alert --> TokenBucket : Collect Metrics
+Gateway --> Services : Forward if Allowed
+
+AlgorithmSelector --> TokenBucket
+AlgorithmSelector --> LeakyBucket
+AlgorithmSelector --> SlidingWindow
+AlgorithmSelector --> FixedWindow
+
+note right of TokenBucket
+  Refills tokens at fixed rate
+  Best for burst traffic
+end note
+
+note right of LeakyBucket
+  Processes requests at constant rate
+  Smooths out traffic spikes
+end note
+
+legend right
+Implementation Details:
+==
+Redis Implementation:
+- INCR for counters
+- EXPIRE for window reset
+- Lua scripts for atomic ops
+
+High Availability:
+- Redis cluster with replicas
+- Circuit breaker pattern
+- Fallback to local cache
+
+Performance Optimizations:
+- Local caching layer
+- Batch counter updates
+- Async monitoring
+end legend
+
+@enduml
\ No newline at end of file
diff --git a/distributed_session_management_system_with_redis_cluster.puml b/distributed_session_management_system_with_redis_cluster.puml
new file mode 100644
--- /dev/null
+++ ./distributed_session_management_system_with_redis_cluster.puml
@@ -0,0 +1,81 @@
+@startuml
+skinparam componentStyle rectangle
+
+package "Distributed Session Management System" {
+    component "Load Balancer" as LB
+    
+    package "Session Service Cluster" {
+        component "Session Manager" as SessionManager
+        component "Session Validator" as SessionValidator
+        component "Session Cleanup" as SessionCleanup
+        component "Session Encryption" as SessionEncryption
+    }
+    
+    package "Redis Cluster" {
+        component "Sessions" as Sessions
+        component "Expiry" as Expiry
+        component "UserIndex" as UserIndex
+    }
+    
+    package "Security Layer" {
+        component "Token Generator" as TokenGen
+        component "Encryption Service" as EncryptService
+        component "JWT Service" as JWTService
+    }
+    
+    package "Monitoring & Analytics" {
+        component "Session Analytics" as Analytics
+        component "Health Monitor" as Health
+        component "Alert System" as Alert
+    }
+}
+
+cloud "Client Applications" as Clients
+cloud "Backend Services" as Services
+
+Clients --> LB : Request with SessionID
+LB --> SessionManager : Route Request
+SessionManager --> Sessions : Validate/Update Session
+SessionManager --> TokenGen : Token Operations
+Services --> SessionValidator : Session Validation
+Alert --> SessionManager : Collect Metrics
+
+SessionManager --> SessionValidator
+SessionManager --> SessionCleanup
+SessionManager --> SessionEncryption
+
+note right of SessionManager
+  Handles session CRUD operations
+  Implements session policies
+end note
+
+note right of Sessions
+  Distributed session storage
+  Session expiry management
+end note
+
+legend right
+Implementation Details:
+==
+Session Storage:
+- Redis cluster with replication
+- Session data encryption at rest
+- Automatic session cleanup
+
+High Availability:
+- Multiple Redis nodes
+- Session service redundancy
+- Fallback mechanisms
+
+Security Features:
+- Token rotation
+- Session hijacking prevention
+- Rate limiting
+
+Performance Optimizations:
+- Local caching layer
+- Session data compression
+- Lazy session cleanup
+end legend
+
+@enduml
\ No newline at end of file
diff --git a/distributed_task_scheduling_system_architecture.puml b/distributed_task_scheduling_system_architecture.puml
new file mode 100644
--- /dev/null
+++ ./distributed_task_scheduling_system_architecture.puml
@@ -0,0 +1,77 @@
+@startuml distributed_task_scheduling_system_architecture
+
+!pragma layout smetana
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+
+rectangle "Client Applications" as CA #E1F5FE
+
+rectangle "API Gateway" as AG #B3E5FC
+
+rectangle "Task Scheduler" as TS #81D4FA {
+    component "Task Queue" as TQ
+    component "Scheduling Algorithm" as SA
+}
+
+rectangle "Task Executor Cluster" as TEC #4FC3F7 {
+    component "Executor Node 1" as EN1
+    component "Executor Node 2" as EN2
+    component "Executor Node N" as ENN
+}
+
+rectangle "Task Store" as TST #03A9F4 {
+    database "Task Metadata DB" as TMDB
+    database "Task History DB" as THDB
+}
+
+rectangle "Monitoring & Logging" as ML #0288D1 {
+    component "Metrics Collector" as MC
+    component "Log Aggregator" as LA
+}
+
+rectangle "Fault Tolerance" as FT #01579B {
+    component "Task Rebalancer" as TR
+    component "Failure Detector" as FD
+}
+
+cloud "Distributed File System" as DFS #E1F5FE
+
+CA -[#FF5722,thickness=2]-> AG : <back:#FFFFFF><color:#FF5722>1. Submit Task</color></back>
+AG -[#FF5722,thickness=2]-> TS : <back:#FFFFFF><color:#FF5722>2. Route Task</color></back>
+TS -[#4CAF50,thickness=2]-> TQ : <back:#FFFFFF><color:#4CAF50>3. Enqueue Task</color></back>
+SA -[#4CAF50,thickness=2]-> TQ : <back:#FFFFFF><color:#4CAF50>4. Prioritize Tasks</color></back>
+TQ -[#2196F3,thickness=2]-> TEC : <back:#FFFFFF><color:#2196F3>5. Assign Tasks</color></back>
+TEC -[#9C27B0,thickness=2]-> TST : <back:#FFFFFF><color:#9C27B0>6. Update Status</color></back>
+TEC -[#795548,thickness=2]-> DFS : <back:#FFFFFF><color:#795548>7. Store Results</color></back>
+ML -[#FFC107,thickness=2]-> TEC : <back:#FFFFFF><color:#FFC107>8. Collect Metrics</color></back>
+FT -[#607D8B,thickness=2]-> TEC : <back:#FFFFFF><color:#607D8B>9. Detect Failures</color></back>
+FT -[#009688,thickness=2]-> TS : <back:#FFFFFF><color:#009688>10. Rebalance Tasks</color></back>
+
+note right of TS
+  Implements various scheduling algorithms:
+  - FIFO
+  - Priority-based
+  - Fair scheduling
+  - Deadline-based
+end note
+
+note bottom of TEC
+  Scalable cluster of worker nodes
+  that execute assigned tasks
+end note
+
+note right of TST
+  Stores task definitions, metadata,
+  execution history, and results
+end note
+
+note bottom of FT
+  Ensures system reliability:
+  - Detects node failures
+  - Reschedules failed tasks
+  - Rebalances workload
+end note
+
+@enduml
diff --git a/distributed_transaction_management_system_with_2pc_and_saga.puml b/distributed_transaction_management_system_with_2pc_and_saga.puml
new file mode 100644
--- /dev/null
+++ ./distributed_transaction_management_system_with_2pc_and_saga.puml
@@ -0,0 +1,89 @@
+@startuml Distributed Transaction Management System
+
+!pragma layout dot
+
+skinparam backgroundColor #CCE8CF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+allowmixing
+
+title Distributed Transaction Management System with 2PC and SAGA
+
+rectangle "Client Applications" as ClientApps PRIMARY_COLOR {
+    component "Web App" as WebApp
+    component "Mobile App" as MobileApp
+}
+
+rectangle "API Gateway" as APIGateway SECONDARY_COLOR {
+    component "Request Router" as RequestRouter
+    component "Authentication" as Auth
+    component "Load Balancer" as LoadBalancer
+}
+
+rectangle "Transaction Coordinator" as TxCoordinator TERTIARY_COLOR {
+    component "2PC Manager" as TwoPCManager
+    component "SAGA Orchestrator" as SAGAOrchestrator
+    component "Transaction Log" as TxLog
+}
+
+rectangle "Microservices" as Microservices QUATERNARY_COLOR {
+    component "Order Service" as OrderService
+    component "Payment Service" as PaymentService
+    component "Inventory Service" as InventoryService
+}
+
+database "Distributed Database" as DistributedDB QUINARY_COLOR {
+    component "Transaction Store" as TxStore
+    component "State Store" as StateStore
+}
+
+ClientApps -[PRIMARY_COLOR,thickness=2]down-> APIGateway : <back:#FFFFFF><color:PRIMARY_COLOR>1. Initiate Transaction</color></back>
+APIGateway -[SECONDARY_COLOR,thickness=2]down-> TxCoordinator : <back:#FFFFFF><color:SECONDARY_COLOR>2. Coordinate Transaction</color></back>
+TxCoordinator -[TERTIARY_COLOR,thickness=2]right-> Microservices : <back:#FFFFFF><color:TERTIARY_COLOR>3. Execute Operations</color></back>
+Microservices -[QUATERNARY_COLOR,thickness=2]down-> DistributedDB : <back:#FFFFFF><color:QUATERNARY_COLOR>4. Update State</color></back>
+TxCoordinator -[QUINARY_COLOR,thickness=2]down-> DistributedDB : <back:#FFFFFF><color:QUINARY_COLOR>5. Log Transaction</color></back>
+
+note right of TwoPCManager
+  Two-Phase Commit (2PC):
+  1. Prepare Phase
+  2. Commit/Abort Phase
+  Ensures atomicity across services
+end note
+
+note right of SAGAOrchestrator
+  SAGA Pattern:
+  - Sequence of local transactions
+  - Compensating transactions for rollback
+  Maintains data consistency in long-running transactions
+end note
+
+note bottom of DistributedDB
+  Distributed Database:
+  - Stores transaction states
+  - Maintains consistency across nodes
+  - Supports horizontal scaling
+end note
+
+note left of APIGateway
+  Performance bottleneck:
+  Consider caching and
+  rate limiting strategies
+end note
+
+note bottom of Microservices
+  Scalability concern:
+  Implement service discovery
+  and circuit breakers
+end note
+
+@enduml
diff --git a/distributed_transaction_system_with_saga_pattern.puml b/distributed_transaction_system_with_saga_pattern.puml
new file mode 100644
--- /dev/null
+++ ./distributed_transaction_system_with_saga_pattern.puml
@@ -0,0 +1,86 @@
+@startuml
+skinparam componentStyle rectangle
+
+package "Distributed Transaction System" {
+    component "API Gateway" as Gateway
+    
+    package "Transaction Coordinator" {
+        component "Saga Orchestrator" as SagaOrchestrator
+        component "2PC Coordinator" as TwoPCCoordinator
+        component "Transaction Manager" as TxManager
+        component "State Machine" as StateMachine
+    }
+    
+    package "Message Queue" {
+        component "Command Queue" as CommandQueue
+        component "Compensation Queue" as CompensationQueue
+        component "Event Queue" as EventQueue
+    }
+    
+    package "Participant Services" {
+        component "Order Service" as OrderService
+        component "Payment Service" as PaymentService
+        component "Inventory Service" as InventoryService
+        component "Shipping Service" as ShippingService
+    }
+    
+    package "Transaction Log" {
+        component "Transaction States" as States
+        component "Compensation Events" as CompEvents
+    }
+    
+    package "Monitoring & Recovery" {
+        component "Transaction Monitor" as TxMonitor
+        component "Recovery Manager" as RecoveryManager
+        component "Alert System" as AlertSystem
+    }
+}
+
+cloud "Client Applications" as Clients
+component "Service Databases" as ServiceDBs
+
+Clients --> Gateway : Transaction Request
+Gateway --> SagaOrchestrator : Initiate Transaction
+SagaOrchestrator --> CommandQueue : Dispatch Commands
+CommandQueue --> OrderService : Execute Operations
+OrderService --> ServiceDBs : Update Data
+AlertSystem --> SagaOrchestrator : Monitor Status
+
+SagaOrchestrator --> CompensationQueue : Compensate
+TwoPCCoordinator --> EventQueue : Coordinate
+
+note right of SagaOrchestrator
+  Manages saga workflow
+  Handles compensations
+end note
+
+note right of TwoPCCoordinator
+  Handles prepare phase
+  Manages commit/rollback
+end note
+
+legend right
+Implementation Details:
+==
+Saga Pattern:
+- Event-driven choreography
+- Compensation handling
+- State machine tracking
+
+2PC Implementation:
+- Prepare phase timeout
+- Participant management
+- Recovery procedures
+
+Consistency Guarantees:
+- Eventually consistent
+- Compensation tracking
+- Idempotency support
+
+Performance Features:
+- Parallel execution
+- Timeout management
+- Retry mechanisms
+end legend
+
+@enduml
\ No newline at end of file
diff --git a/distributed_web_crawler_with_url_frontier_content_fetching_parsing_and_indexing_pipeline.puml b/distributed_web_crawler_with_url_frontier_content_fetching_parsing_and_indexing_pipeline.puml
new file mode 100644
--- /dev/null
+++ ./distributed_web_crawler_with_url_frontier_content_fetching_parsing_and_indexing_pipeline.puml
@@ -0,0 +1,61 @@
+@startuml Distributed Web Crawler System Architecture
+
+!define RECTANGLE class
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundCorner 10
+skinparam componentStyle uml2
+allowmixing
+
+rectangle "Distributed Web Crawler System" {
+    RECTANGLE "URL Frontier" as frontier #D6EAF8
+    RECTANGLE "Crawler Manager" as manager #D5F5E3
+    RECTANGLE "DNS Resolver" as dns #FDEBD0
+    RECTANGLE "Fetcher" as fetcher #F5B7B1
+    RECTANGLE "Content Parser" as parser #D7BDE2
+    RECTANGLE "Content Store" as store #FAD7A0
+    RECTANGLE "Indexer" as indexer #AED6F1
+    RECTANGLE "Duplicate Detector" as dedup #F9E79F
+    
+    database "URL Database" as urldb #D6DBDF
+    database "Content Database" as contentdb #D6DBDF
+    database "Index Database" as indexdb #D6DBDF
+}
+
+frontier -[#4CAF50,thickness=2]-> manager : <color:#4CAF50>1. Provide URLs</color>
+manager -[#2196F3,thickness=2]-> dns : <color:#2196F3>2. Resolve DNS</color>
+manager -[#FF5722,thickness=2]-> fetcher : <color:#FF5722>3. Fetch content</color>
+fetcher -[#9C27B0,thickness=2]-> parser : <color:#9C27B0>4. Parse content</color>
+parser -[#795548,thickness=2]-> store : <color:#795548>5. Store content</color>
+parser -[#FFC107,thickness=2]-> indexer : <color:#FFC107>6. Index content</color>
+parser -[#00BCD4,thickness=2]-> dedup : <color:#00BCD4>7. Check duplicates</color>
+dedup -[#607D8B,thickness=2]-> frontier : <color:#607D8B>8. Add new URLs</color>
+
+frontier -[#E91E63,dashed]-> urldb : Store/Retrieve
+store -[#E91E63,dashed]-> contentdb : Store
+indexer -[#E91E63,dashed]-> indexdb : Store
+
+note right of frontier
+  Prioritizes and
+  schedules URLs
+end note
+
+note right of manager
+  Coordinates crawling
+  tasks and resources
+end note
+
+note bottom of fetcher
+  Respects robots.txt
+  and crawl delays
+end note
+
+note bottom of dedup
+  Prevents crawling
+  duplicate content
+end note
+
+@enduml
diff --git a/dynamodb_distributed_storage_system_with_partitioning_consistency_models_streams_and_lambda_integration.puml b/dynamodb_distributed_storage_system_with_partitioning_consistency_models_streams_and_lambda_integration.puml
new file mode 100644
--- /dev/null
+++ ./dynamodb_distributed_storage_system_with_partitioning_consistency_models_streams_and_lambda_integration.puml
@@ -0,0 +1,94 @@
+@startuml
+skinparam componentStyle rectangle
+skinparam backgroundColor #FAFAFA
+skinparam defaultFontSize 14
+skinparam defaultFontName Arial
+skinparam ArrowColor #2C3E50
+skinparam PackageBackgroundColor #FAFAFA
+skinparam PackageBorderColor #CCCCCC
+
+title DynamoDB-based Distributed Data Storage System
+
+package "Client Layer" {
+    component "Web Application" as WebApp
+    component "Mobile Application" as MobileApp
+    component "IoT Device" as IoTDevice
+}
+
+package "API Gateway" {
+    component "Request Routing" as RequestRouting
+    component "Authentication" as Auth
+    component "Traffic Control" as TrafficControl
+}
+
+package "Application Service" {
+    component "Business Logic" as BusinessLogic
+    component "Data Access Layer" as DataAccessLayer
+}
+
+package "DynamoDB" {
+    component "Table Design" as TableDesign
+    component "Partition Strategy" as PartitionStrategy
+    component "Consistency Model" as ConsistencyModel
+}
+
+package "DynamoDB Streams" {
+    component "Change Capture" as ChangeCapture
+    component "Event Processing" as EventProcessing
+}
+
+package "Lambda Functions" {
+    component "Stream Processing" as StreamProcessing
+    component "Data Transformation" as DataTransformation
+    component "Triggers" as Triggers
+}
+
+package "Cache Layer" {
+    component "ElastiCache" as ElastiCache
+    component "DAX" as DAX
+}
+
+package "Data Analysis" {
+    component "Athena" as Athena
+    component "Redshift" as Redshift
+}
+
+package "Monitoring" {
+    component "CloudWatch" as CloudWatch
+    component "AWS Config" as AWSConfig
+}
+
+package "Security" {
+    component "IAM" as IAM
+    component "KMS" as KMS
+    component "VPC" as VPC
+}
+
+WebApp --> RequestRouting : 1. API Request
+MobileApp --> RequestRouting
+IoTDevice --> RequestRouting
+
+RequestRouting --> BusinessLogic : 2. Route Request
+Auth --> BusinessLogic
+TrafficControl --> BusinessLogic
+
+BusinessLogic --> TableDesign : 3. Query/Update Data
+DataAccessLayer --> TableDesign
+
+BusinessLogic --> ElastiCache : 4. Cache Access
+BusinessLogic --> DAX
+
+TableDesign ..> ChangeCapture : 5. Stream Changes
+ChangeCapture ..> StreamProcessing : 6. Trigger Functions
+
+TableDesign ..> Athena : 7. Analyze Data
+TableDesign ..> Redshift
+
+TableDesign ..> CloudWatch : 8. Monitor Performance
+TableDesign ..> AWSConfig
+
+IAM --> TableDesign : 9. Secure Access
+KMS --> TableDesign
+VPC --> TableDesign
+
+@enduml
diff --git a/e_commerce_system_architecture_with_microservices_payment_and_recommendation.puml b/e_commerce_system_architecture_with_microservices_payment_and_recommendation.puml
new file mode 100644
--- /dev/null
+++ ./e_commerce_system_architecture_with_microservices_payment_and_recommendation.puml
@@ -0,0 +1,84 @@
+@startuml E-commerce System Architecture
+!theme toy
+allowmixing
+
+' Color definitions
+!define BACKGROUND_COLOR E6E6FA
+!define MICROSERVICES_COLOR FFF0F5
+!define API_GATEWAY_COLOR 98FB98
+!define DATABASE_COLOR B0E0E6
+!define EXTERNAL_SERVICES_COLOR FFE4B5
+
+skinparam backgroundColor BACKGROUND_COLOR
+skinparam shadowing false
+skinparam RoundCorner 10
+skinparam ArrowColor 454645
+skinparam DefaultFontName Arial
+skinparam DefaultFontSize 12
+
+' Components
+component "API Gateway" as APIGateway #API_GATEWAY_COLOR
+rectangle "Microservices" as Microservices #MICROSERVICES_COLOR {
+    component "User Service" as UserService
+    component "Product Service" as ProductService
+    component "Cart Service" as CartService
+    component "Order Service" as OrderService
+    component "Inventory Service" as InventoryService
+    component "Search Service" as SearchService
+    component "Recommendation Service" as RecommendationService
+}
+database "Database" as Database #DATABASE_COLOR
+rectangle "External Services" as ExternalServices #EXTERNAL_SERVICES_COLOR {
+    component "Payment Gateway" as PaymentGateway
+    component "Shipping Service" as ShippingService
+    component "Email Service" as EmailService
+}
+component "Monitoring" as Monitoring
+component "Search Engine" as SearchEngine
+component "Data Analysis" as DataAnalysis
+
+' Relationships
+APIGateway -[#4169E1]down-> UserService : "1. Authenticate user"
+APIGateway -[#4169E1]down-> ProductService : "2. Browse products"
+APIGateway -[#4169E1]down-> SearchService : "3. Search products"
+SearchService -[#FFD700]right-> SearchEngine : "4. Index/Query"
+APIGateway -[#4169E1]down-> CartService : "5. Add to cart"
+CartService -[#228B22]right-> InventoryService : "6. Check stock"
+APIGateway -[#4169E1]down-> OrderService : "7. Place order"
+OrderService -[#FF69B4]right-> PaymentGateway : "8. Process payment"
+OrderService -[#FF69B4]right-> ShippingService : "9. Arrange shipping"
+OrderService -[#FF69B4]down-> EmailService : "10. Send confirmation"
+RecommendationService -[#A9A9A9]left-> DataAnalysis : "11. Generate recommendations"
+
+Microservices -[#228B22]down-> Database : "CRUD operations"
+Microservices -[#CD853F]up-> Monitoring : "Log/Metrics"
+
+' Notes
+note right of APIGateway
+  API Gateway:
+  - Request routing
+  - Load balancing
+  - Auth & authorization
+  - Rate limiting
+  - Response caching
+end note
+
+note bottom of CartService
+  Cart Service:
+  - High concurrency
+  - Data consistency
+  - Cart merging
+  - Product validation
+end note
+
+note bottom of Database
+  Database:
+  - Scalable
+  - Redundant
+  - Backup & Recovery
+end note
+
+note "Performance bottleneck:\nOptimize database queries\nand implement caching" as PerformanceNote
+Database .. PerformanceNote
+
+@enduml
diff --git a/ecommerce_order_payment_shipping_architecture_with_performance_optimizations_and_microservices.puml b/ecommerce_order_payment_shipping_architecture_with_performance_optimizations_and_microservices.puml
new file mode 100644
--- /dev/null
+++ ./ecommerce_order_payment_shipping_architecture_with_performance_optimizations_and_microservices.puml
@@ -0,0 +1,122 @@
+@startuml Order Payment and Shipping System Architecture
+
+skinparam backgroundColor #F0F0F0
+allowmixing
+!pragma layout dot
+
+rectangle "E-commerce System" {
+    rectangle "API Gateway" as apiGateway #A9CCE3
+    
+    rectangle "Order Domain" {
+        component "Order Service" as orderService #D6EAF8
+        component "Order Cache" as orderCache #D6EAF8
+        component "Order Database" as orderDB #D6DBDF
+    }
+    
+    rectangle "Payment Domain" {
+        component "Payment Service" as paymentService #D5F5E3
+        component "Payment Gateway" as paymentGateway #D5F5E3
+        component "Fraud Detection" as fraudDetection #F5B7B1
+        component "Payment Database" as paymentDB #D6DBDF
+    }
+    
+    rectangle "Inventory Domain" {
+        component "Inventory Service" as inventoryService #FAD7A0
+        component "Inventory Cache" as inventoryCache #FAD7A0
+        component "Inventory Database" as inventoryDB #D6DBDF
+    }
+    
+    rectangle "Shipping Domain" {
+        component "Shipping Service" as shippingService #D7BDE2
+        component "Shipping Database" as shippingDB #D6DBDF
+    }
+    
+    rectangle "Notification Domain" {
+        component "Notification Service" as notificationService #AED6F1
+    }
+    
+    rectangle "Message Queue" as messageQueue #F9E79F
+}
+
+actor "Customer" as customer
+actor "Merchant" as merchant
+
+customer -[#4CAF50,thickness=2]-> apiGateway : <color:#4CAF50>1. Place Order</color>
+apiGateway -[#2196F3,thickness=2]-> orderService : <color:#2196F3>2. Process Order</color>
+orderService -[#FF5722,thickness=2]-> inventoryService : <color:#FF5722>3. Check Stock</color>
+inventoryService -[#9C27B0,thickness=2]-> inventoryCache : <color:#9C27B0>4. Verify Inventory</color>
+orderService -[#795548,thickness=2]-> paymentService : <color:#795548>5. Initiate Payment</color>
+paymentService -[#FFC107,thickness=2]-> fraudDetection : <color:#FFC107>6. Check for Fraud</color>
+paymentService -[#00BCD4,thickness=2]-> paymentGateway : <color:#00BCD4>7. Process Payment</color>
+paymentGateway -[#E91E63,thickness=2]-> paymentService : <color:#E91E63>8. Payment Result</color>
+paymentService -[#9E9E9E,thickness=2]-> orderService : <color:#9E9E9E>9. Confirm Payment</color>
+orderService -[#3F51B5,thickness=2]-> messageQueue : <color:#3F51B5>10. Order Confirmed</color>
+messageQueue -[#009688,thickness=2]-> shippingService : <color:#009688>11. Create Shipment</color>
+shippingService -[#FF9800,thickness=2]-> shippingDB : <color:#FF9800>12. Update Shipping Status</color>
+messageQueue -[#8BC34A,thickness=2]-> notificationService : <color:#8BC34A>13. Send Notifications</color>
+notificationService -[#607D8B,thickness=2]-> customer : <color:#607D8B>14. Notify Customer</color>
+shippingService -[#673AB7,thickness=2]-> merchant : <color:#673AB7>15. Shipping Instructions</color>
+
+note right of apiGateway
+Performance Optimization:
+• Implement rate limiting
+• Use caching for frequently accessed data
+• Load balancing for horizontal scaling
+end note
+
+note right of orderService
+Performance Optimization:
+• Use distributed caching (Redis)
+• Implement CQRS pattern
+• Event sourcing for order history
+end note
+
+note right of inventoryService
+Performance Optimization:
+• Real-time inventory updates with cache
+• Eventual consistency for non-critical updates
+• Optimize database queries and indexing
+end note
+
+note right of paymentService
+Performance Optimization:
+• Implement circuit breaker pattern
+• Use connection pooling
+• Asynchronous processing for non-critical operations
+end note
+
+note bottom of shippingService
+Performance Optimization:
+• Batch processing for shipments
+• Caching for shipping rates and tracking info
+• Asynchronous updates to shipping status
+end note
+
+note bottom of notificationService
+Performance Optimization:
+• Use message queue for async processing
+• Implement rate limiting
+• Batch notifications where possible
+end note
+
+note bottom of messageQueue
+Message Types:
+• OrderCreated
+• PaymentProcessed
+• InventoryUpdated
+• ShipmentCreated
+• NotificationSent
+end note
+
+note right of messageQueue
+Message Structure:
+{
+  "type": "EventType",
+  "timestamp": "ISO8601 DateTime",
+  "payload": {
+    // Event-specific data
+  }
+}
+end note
+
+@enduml
diff --git a/elastic_cloud_computing_platform_with_resource_management_virtualization_and_monitoring.puml b/elastic_cloud_computing_platform_with_resource_management_virtualization_and_monitoring.puml
new file mode 100644
--- /dev/null
+++ ./elastic_cloud_computing_platform_with_resource_management_virtualization_and_monitoring.puml
@@ -0,0 +1,106 @@
+@startuml Elastic Cloud Computing Platform
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam roundcorner 20
+skinparam shadowing false
+skinparam linetype ortho
+
+title Elastic Cloud Computing Platform
+
+' User Interface
+rectangle "User Interface" as UI #D6EAF8 {
+    [Web Console]
+    [CLI]
+    [API]
+}
+
+' Resource Management
+rectangle "Resource Management" as RM #AED6F1 {
+    [Resource Allocator]
+    [Resource Monitor]
+    [Capacity Planner]
+}
+
+' Virtualization Layer
+rectangle "Virtualization Layer" as VL #D5F5E3 {
+    [Hypervisor]
+    [Container Orchestrator]
+}
+
+' Physical Infrastructure
+rectangle "Physical Infrastructure" as PI #FDEBD0 {
+    [Compute Nodes]
+    [Storage Nodes]
+    [Network Devices]
+}
+
+' Elasticity Engine
+rectangle "Elasticity Engine" as EE #F5B041 {
+    [Auto Scaler]
+    [Load Balancer]
+    [Fault Tolerator]
+}
+
+' Billing and Metering
+rectangle "Billing and Metering" as BM #85C1E9 {
+    [Usage Tracker]
+    [Cost Calculator]
+    [Billing Service]
+}
+
+' Security and Compliance
+rectangle "Security and Compliance" as SC #F1948A {
+    [Identity Manager]
+    [Access Controller]
+    [Compliance Checker]
+}
+
+' Monitoring and Analytics
+rectangle "Monitoring and Analytics" as MA #82E0AA {
+    [Performance Monitor]
+    [Log Analyzer]
+    [Predictive Analytics]
+}
+
+' Data Flow
+UI --> RM : Request resources
+RM --> VL : Allocate resources
+VL --> PI : Manage physical resources
+EE --> RM : Trigger scaling
+EE --> VL : Adjust resource allocation
+BM --> RM : Track resource usage
+SC --> RM : Enforce security policies
+MA --> RM : Provide insights
+MA --> EE : Inform scaling decisions
+
+' Notes
+note right of RM
+  Key Features:
+  - Dynamic resource allocation
+  - Multi-tenancy support
+  - Resource isolation
+end note
+
+note right of EE
+  Elasticity Mechanisms:
+  - Horizontal scaling (add/remove instances)
+  - Vertical scaling (resize instances)
+  - Migration (move workloads)
+end note
+
+note right of VL
+  Supports:
+  - Virtual Machines
+  - Containers
+  - Serverless Functions
+end note
+
+note bottom of PI
+  Distributed across multiple data centers
+  for high availability and disaster recovery
+end note
+
+@enduml
diff --git a/event_driven_architecture_with_kafka_and_microservices.puml b/event_driven_architecture_with_kafka_and_microservices.puml
new file mode 100644
--- /dev/null
+++ ./event_driven_architecture_with_kafka_and_microservices.puml
@@ -0,0 +1,85 @@
+@startuml Event-Driven Architecture with Kafka and Microservices
+
+!define RECTANGLE class
+!define STORAGE database
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Event-Driven Architecture with Kafka and Microservices
+
+rectangle "Client Applications" as ClientApps #E1F5FE
+
+rectangle "API Gateway" as APIGateway #B3E5FC {
+    component "Request Routing" as RequestRouting
+    component "Authentication" as Auth
+    component "Rate Limiting" as RateLimit
+}
+
+rectangle "Event Producer Services" as ProducerServices #81D4FA {
+    component "Order Service" as OrderService
+    component "User Service" as UserService
+    component "Product Service" as ProductService
+}
+
+queue "Apache Kafka" as Kafka #4FC3F7 {
+    component "Topics" as Topics
+}
+
+rectangle "Event Consumer Services" as ConsumerServices #03A9F4 {
+    component "Inventory Service" as InventoryService
+    component "Notification Service" as NotificationService
+    component "Analytics Service" as AnalyticsService
+}
+
+database "Data Stores" as DataStores #0288D1 {
+    STORAGE "Order DB" as OrderDB
+    STORAGE "User DB" as UserDB
+    STORAGE "Product DB" as ProductDB
+    STORAGE "Inventory DB" as InventoryDB
+}
+
+rectangle "Monitoring & Logging" as Monitoring #01579B {
+    component "ELK Stack" as ELK
+    component "Prometheus" as Prometheus
+    component "Grafana" as Grafana
+}
+
+ClientApps -[#FF5722,thickness=2]-> APIGateway : <back:#FFFFFF><color:#FF5722>1. API Request</color></back>
+APIGateway -[#FF9800,thickness=2]-> ProducerServices : <back:#FFFFFF><color:#FF9800>2. Route Request</color></back>
+ProducerServices -[#FFC107,thickness=2]-> Kafka : <back:#FFFFFF><color:#FFC107>3. Produce Event</color></back>
+Kafka -[#4CAF50,thickness=2]-> ConsumerServices : <back:#FFFFFF><color:#4CAF50>4. Consume Event</color></back>
+ProducerServices -[#2196F3,thickness=2]-> DataStores : <back:#FFFFFF><color:#2196F3>5. Read/Write Data</color></back>
+ConsumerServices -[#3F51B5,thickness=2]-> DataStores : <back:#FFFFFF><color:#3F51B5>6. Read/Write Data</color></back>
+ProducerServices -[#9C27B0,thickness=2]-> Monitoring : <back:#FFFFFF><color:#9C27B0>7. Log & Metrics</color></back>
+ConsumerServices -[#E91E63,thickness=2]-> Monitoring : <back:#FFFFFF><color:#E91E63>8. Log & Metrics</color></back>
+
+note right of Kafka
+  Event Topics:
+  - OrderCreated
+  - UserRegistered
+  - ProductUpdated
+  - InventoryChanged
+  - NotificationSent
+end note
+
+note right of ConsumerServices
+  Event Consumers:
+  - Process events asynchronously
+  - Scale independently
+  - Loose coupling with producers
+end note
+
+note bottom of Monitoring
+  Monitoring & Logging:
+  - Centralized logging with ELK
+  - Metrics collection with Prometheus
+  - Visualization with Grafana
+end note
+
+@enduml
+
diff --git a/fault_tolerant_high_availability_system_design_with_automated_recovery_and_data_consistency_management.puml b/fault_tolerant_high_availability_system_design_with_automated_recovery_and_data_consistency_management.puml
new file mode 100644
--- /dev/null
+++ ./fault_tolerant_high_availability_system_design_with_automated_recovery_and_data_consistency_management.puml
@@ -0,0 +1,83 @@
+@startuml Fault Tolerant and High Availability System Design
+
+!define RECTANGLE class
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing true
+
+title Fault Tolerant and High Availability System Design
+
+rectangle "Primary System" as Primary #E1F5FE
+rectangle "Backup System" as Backup #FFEBEE
+rectangle "Health Checker" as HealthChecker #E8F5E9
+rectangle "Fault Detector" as FaultDetector #FFF3E0
+rectangle "Recovery Controller" as RecoveryController #F3E5F5
+rectangle "Consistency Manager" as ConsistencyManager #FAFAFA
+rectangle "Load Balancer" as LoadBalancer #FFFDE7
+rectangle "Data Replication Service" as DataReplication #E0F2F1
+
+Primary -[#1E88E5,thickness=2]-> HealthChecker : <color:#1E88E5>1. Send health data
+HealthChecker -[#FFA000,thickness=2]-> FaultDetector : <color:#FFA000>2. Report anomalies
+FaultDetector -[#D81B60,thickness=2]-> RecoveryController : <color:#D81B60>3. Trigger recovery
+RecoveryController -[#7CB342,thickness=2]-> Backup : <color:#7CB342>4. Activate backup
+RecoveryController -[#5E35B1,thickness=2]-> ConsistencyManager : <color:#5E35B1>5. Ensure data consistency
+ConsistencyManager -[#00897B,thickness=2]-> Primary : <color:#00897B>6. Sync data (Primary)
+ConsistencyManager -[#00897B,thickness=2]-> Backup : <color:#00897B>7. Sync data (Backup)
+LoadBalancer -[#FFB300,thickness=2]-> Primary : <color:#FFB300>8. Route traffic
+LoadBalancer -[#FFB300,thickness=2]-> Backup : <color:#FFB300>9. Route traffic (failover)
+Primary -[#00BCD4,thickness=2]-> DataReplication : <color:#00BCD4>10. Replicate data
+DataReplication -[#00BCD4,thickness=2]-> Backup : <color:#00BCD4>11. Update backup
+
+note right of HealthChecker
+  Continuously monitors system health
+  using various metrics and log analysis
+  - CPU, Memory, Disk I/O
+  - Network latency and throughput
+  - Application-specific health checks
+end note
+
+note right of FaultDetector
+  Uses ML algorithms to detect anomalies
+  and predict potential failures
+  - Analyzes historical and real-time data
+  - Implements predictive maintenance
+end note
+
+note right of ConsistencyManager
+  Implements distributed consistency protocols
+  - Two-Phase Commit (2PC) for strong consistency
+  - Eventual consistency with conflict resolution
+  - Quorum-based consistency for read/write operations
+end note
+
+note right of RecoveryController
+  Automated recovery process:
+  1. Detect failure
+  2. Activate backup system
+  3. Transfer state and traffic
+  4. Verify system integrity
+  5. Notify administrators
+  - Implements circuit breaker pattern
+  - Manages graceful degradation
+end note
+
+note bottom of LoadBalancer
+  Intelligent traffic routing:
+  - Health-check based routing
+  - Sticky sessions for stateful applications
+  - Implements rate limiting and DDoS protection
+end note
+
+note bottom of DataReplication
+  Ensures data consistency across systems:
+  - Synchronous replication for critical data
+  - Asynchronous replication for less critical data
+  - Implements Write-Ahead Logging (WAL)
+  - Manages replication lag and catch-up
+end note
+
+@enduml
diff --git a/flight_booking/flight_booking_system_architecture_with_caching_api_integration_and_database.puml b/flight_booking/flight_booking_system_architecture_with_caching_api_integration_and_database.puml
new file mode 100644
--- /dev/null
+++ ./flight_booking/flight_booking_system_architecture_with_caching_api_integration_and_database.puml
@@ -0,0 +1,113 @@
+@startuml
+!pragma layout dot
+allowmixing
+
+' Component definitions
+rectangle "User Interface" as UI #E6F3FF
+component "Flight Search Service" as FSS #E1F5FE
+component "Booking Service" as BS #E8F5E9
+component "Payment Service" as PS #FFF3E0
+component "Notification Service" as NS #F3E5F5
+component "Airline API Integrator" as AAI #FFEBEE
+rectangle "Database" as DB #ECEFF1
+component "Scheduled Data Fetcher" as SDF #F1F8E9
+component "Cache" as CACHE #E0F2F1
+
+' External component
+rectangle "Airline APIs" as APIS #FAFAFA
+
+' Notes
+note right of UI
+  - Search flights
+  - Display options
+  - Handle booking
+end note
+
+note right of FSS
+  - Process search requests
+  - Query Cache first
+  - Fallback to Database
+end note
+
+note right of BS
+  - Manage bookings
+  - Verify availability
+  - Interact with Payment
+end note
+
+note right of PS
+  - Secure payment processing
+  - Store transactions
+end note
+
+note right of NS
+  - Send confirmations
+  - Update booking status
+end note
+
+note right of AAI
+  - Integrate multiple APIs
+  - Fetch real-time data
+  - Update Database
+end note
+
+note left of DB
+  - Store flight, booking, payment data
+  - Provide data retrieval
+end note
+
+note bottom of SDF
+  - Periodic data prefetching
+  - Cache flight information
+  - Optimize for peak times
+end note
+
+note left of CACHE
+  key: value pairs
+  - Frequent flight data
+  - Reduce DB/API load
+end note
+
+' Connections
+UI -[#1E88E5,thickness=2]down-> FSS : <back:#FFFFFF><color:#1E88E5>1. Search Flights</color></back>
+FSS -[#00897B,thickness=2]right-> CACHE : <back:#FFFFFF><color:#00897B>2. Query Cache</color></back>
+CACHE -[#FFA000,thickness=2]down-> DB : <back:#FFFFFF><color:#FFA000>3. Query DB if not in Cache</color></back>
+FSS -[#E53935,thickness=2]down-> AAI : <back:#FFFFFF><color:#E53935>4. Fetch Flight Info</color></back>
+AAI -[#8E24AA,thickness=2]right-> APIS : <back:#FFFFFF><color:#8E24AA>5. Fetch Data</color></back>
+AAI -[#43A047,thickness=2]left-> DB : <back:#FFFFFF><color:#43A047>6. Store Data</color></back>
+DB -[#F4511E,thickness=2]up-> CACHE : <back:#FFFFFF><color:#F4511E>7. Update Cache</color></back>
+FSS -[#3949AB,thickness=2]up-> UI : <back:#FFFFFF><color:#3949AB>8. Return Flights</color></back>
+
+UI -[#D81B60,thickness=2]right-> BS : <back:#FFFFFF><color:#D81B60>9. Select Flight</color></back>
+BS -[#00ACC1,thickness=2]up-> FSS : <back:#FFFFFF><color:#00ACC1>10. Verify Availability</color></back>
+BS -[#7CB342,thickness=2]down-> DB : <back:#FFFFFF><color:#7CB342>11. Store Booking</color></back>
+
+BS -[#FF7043,thickness=2]right-> PS : <back:#FFFFFF><color:#FF7043>12. Process Payment</color></back>
+PS -[#5E35B1,thickness=2]up-> DB : <back:#FFFFFF><color:#5E35B1>13. Store Payment</color></back>
+PS -[#039BE5,thickness=2]left-> BS : <back:#FFFFFF><color:#039BE5>14. Confirm Payment</color></back>
+
+BS -[#C0CA33,thickness=2]down-> NS : <back:#FFFFFF><color:#C0CA33>15. Send Confirmation</color></back>
+NS -[#FB8C00,thickness=2]left-> UI : <back:#FFFFFF><color:#FB8C00>16. Notify User</color></back>
+
+SDF -[#8D6E63,thickness=2]right-> AAI : <back:#FFFFFF><color:#8D6E63>17. Prefetch Data</color></back>
+SDF -[#26A69A,thickness=2]down-> DB : <back:#FFFFFF><color:#26A69A>18. Cache Data</color></back>
+
+' Performance bottleneck and optimization suggestions
+note as N1
+  Performance Bottleneck:
+  - High load on Airline API during peak times
+  Optimization:
+  - Implement rate limiting and caching strategies
+end note
+
+note as N2
+  Performance Bottleneck:
+  - Database queries for frequent searches
+  Optimization:
+  - Optimize cache usage and implement query caching
+end note
+
+N1 .. AAI
+N2 .. DB
+
+@enduml
diff --git a/flight_booking/flight_booking_system_database_sql_nosql_hybrid_with_sharding_and_indexing.puml b/flight_booking/flight_booking_system_database_sql_nosql_hybrid_with_sharding_and_indexing.puml
new file mode 100644
--- /dev/null
+++ ./flight_booking/flight_booking_system_database_sql_nosql_hybrid_with_sharding_and_indexing.puml
@@ -0,0 +1,116 @@
+@startuml
+!define RECTANGLE class
+skinparam backgroundColor #EEEBDC
+skinparam handwritten false
+skinparam linetype ortho
+
+skinparam rectangle {
+  BackgroundColor<<SQL>> LightYellow
+  BackgroundColor<<NoSQL>> LightGreen
+  BorderColor<<SQL>> DarkOrange
+  BorderColor<<NoSQL>> DarkGreen
+}
+
+' 尝试使用特殊颜色标记分片键
+skinparam class {
+  AttributeFontColor<<shardingKey>> #FF4500
+  AttributeIconColor<<shardingKey>> #FF4500
+}
+
+rectangle "User Service" {
+  entity "User [SQL]" as user <<SQL>> {
+    * user_id : INT <<PK>> <<shardingKey>>
+    --
+    * name : VARCHAR(100)
+    * email : VARCHAR(100) <<index>>
+    * password : VARCHAR(100)
+    * phone : VARCHAR(15)
+    --
+    * created_at : DATETIME
+    * updated_at : DATETIME
+  }
+  note bottom of user : Sharding Key: user_id
+}
+
+rectangle "Flight Service" {
+  entity "Flight [SQL]" as flight <<SQL>> {
+    * flight_id : INT <<PK>>
+    --
+    * airline : VARCHAR(100)
+    * flight_number : VARCHAR(20) <<index>>
+    * departure_airport : VARCHAR(100) <<index>>
+    * arrival_airport : VARCHAR(100) <<index>>
+    * departure_date : DATE <<shardingKey>> <<index>>
+    * departure_time : DATETIME
+    * arrival_time : DATETIME
+    * price : DECIMAL(10,2)
+    * available_seats : INT
+    --
+    * created_at : DATETIME
+    * updated_at : DATETIME
+  }
+  note bottom of flight : Sharding Key: departure_date
+}
+
+rectangle "Booking Service" {
+  entity "Booking [NoSQL]" as booking <<NoSQL>> {
+    * booking_id : STRING <<PK>> <<shardingKey>>
+    --
+    * user_id : STRING <<shardingKey>>
+    * flight_id : STRING
+    * booking_date : DATETIME
+    * status : STRING
+    --
+    * created_at : DATETIME
+    * updated_at : DATETIME
+  }
+  note bottom of booking : Sharding Key: user_id + booking_id (Composite Key)
+}
+
+rectangle "Payment Service" {
+  entity "Payment [SQL]" as payment <<SQL>> {
+    * payment_id : INT <<PK>>
+    --
+    * booking_id : STRING <<FK>> <<index>>
+    * amount : DECIMAL(10,2)
+    * payment_date : DATETIME
+    * payment_method : VARCHAR(50)
+    * payment_status : VARCHAR(50) <<index>>
+    --
+    * created_at : DATETIME
+    * updated_at : DATETIME
+  }
+}
+
+rectangle "Notification Service" {
+  entity "Notification [NoSQL]" as notification <<NoSQL>> {
+    * notification_id : STRING <<PK>>
+    --
+    * user_id : STRING <<shardingKey>> <<index>>
+    * message : TEXT
+    * sent_date : DATETIME
+    * status : VARCHAR(50) <<index>>
+    --
+    * created_at : DATETIME
+    * updated_at : DATETIME
+  }
+  note bottom of notification : Sharding Key: user_id
+}
+
+rectangle "Cache Service" {
+  entity "Cache [Redis]" as cache <<NoSQL>> {
+    * key : STRING <<PK>>
+    * value : BLOB
+    * ttl : INT
+    --
+    * created_at : DATETIME
+  }
+  note bottom of cache : Key format: entity:id:field\nExample: user:1234:profile
+}
+
+user ||--o{ booking : "has"
+flight ||--o{ booking : "contains"
+booking ||--o{ payment : "has"
+user ||--o{ notification : "receives"
+
+@enduml
diff --git a/google_docs_architecture/google_docs_architecture_with_real_time_collaboration_and_version_control.puml b/google_docs_architecture/google_docs_architecture_with_real_time_collaboration_and_version_control.puml
new file mode 100644
--- /dev/null
+++ ./google_docs_architecture/google_docs_architecture_with_real_time_collaboration_and_version_control.puml
@@ -0,0 +1,78 @@
+@startuml Google Docs Architecture
+
+skinparam backgroundColor #F5F5F5
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam titleFontSize 18
+skinparam titleFontColor #333333
+skinparam padding 5
+skinparam roundCorner 10
+
+title Google Docs Architecture
+
+rectangle "Client Side" as ClientSide {
+    [Web UI]
+    [Mobile App]
+    [Desktop App]
+}
+
+rectangle "Load Balancer" as LB
+
+rectangle "Application Layer" {
+    [Authentication Service]
+    [Document Service]
+    [Collaboration Service]
+    [Search Service]
+    [Notification Service]
+    [Version Control Service]
+}
+
+rectangle "Real-time Collaboration" as RTC {
+    [Operational Transform Engine]
+    [WebSocket Server]
+}
+
+rectangle "Data Storage" {
+    database "Document DB (NoSQL)" {
+        [Document Data]
+        note right: Sharding Key: doc_id
+    }
+    
+    database "User DB (SQL)" {
+        [User Data]
+        note right: PK: user_id\nIndexes: email, username
+    }
+    
+    database "Redis Cache" {
+        [Session Data]
+        [Document Cache]
+        note right: Key: doc_id:version
+    }
+}
+
+queue "Message Queue" {
+    [RabbitMQ]
+}
+
+cloud "External Services" {
+    [Email Service]
+    [Analytics Service]
+}
+
+ClientSide -[#4CAF50,thickness=2]-> LB : <color:#4CAF50>1. HTTPS</color>
+LB -[#2196F3,thickness=2]-> [Authentication Service] : <color:#2196F3>2. Auth</color>
+LB -[#FF5722,thickness=2]-> [Document Service] : <color:#FF5722>3. Doc Ops</color>
+[Document Service] -[#9C27B0,thickness=2]-> RTC : <color:#9C27B0>4. Real-time Updates</color>
+[Document Service] -[#795548,thickness=2]-> [Document Data] : <color:#795548>5. CRUD</color>
+[Document Service] -[#FFC107,thickness=2]-> [Document Cache] : <color:#FFC107>6. Cache</color>
+[Collaboration Service] -[#00BCD4,thickness=2]-> [WebSocket Server] : <color:#00BCD4>7. Sync</color>
+[Notification Service] -[#607D8B,thickness=2]-> [RabbitMQ] : <color:#607D8B>8. Queue</color>
+[RabbitMQ] -[#8BC34A,thickness=2]-> [Email Service] : <color:#8BC34A>9. Send</color>
+
+note right of RTC
+  Ensures consistency and
+  real-time collaboration
+  across multiple users
+end note
+
+@enduml
diff --git a/google_docs_architecture/real_time_collaborative_editing_flow.puml b/google_docs_architecture/real_time_collaborative_editing_flow.puml
new file mode 100644
--- /dev/null
+++ ./google_docs_architecture/real_time_collaborative_editing_flow.puml
@@ -0,0 +1,83 @@
+@startuml Real-time Collaborative Editing Flow
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam monochrome false
+skinparam shadowing false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam sequenceArrowThickness 2
+skinparam roundcorner 10
+skinparam maxMessageSize 200
+
+skinparam sequence {
+    ArrowColor #2C3E50
+    ActorBorderColor #2980B9
+    LifeLineBorderColor #2980B9
+    LifeLineBackgroundColor #A9CCE3
+    
+    ParticipantBorderColor #2980B9
+    ParticipantBackgroundColor #E8F8F5
+    ParticipantFontName Arial
+    ParticipantFontSize 16
+    ParticipantFontColor #2C3E50
+    
+    ActorBackgroundColor #85C1E9
+    ActorFontColor #2C3E50
+    ActorFontSize 16
+    ActorFontName Arial
+}
+
+actor "User A" as UserA
+actor "User B" as UserB
+participant "Client A" as ClientA
+participant "Client B" as ClientB
+participant "WebSocket Server" as WSS
+participant "Operational Transform Engine" as OTE
+participant "Conflict Resolution" as CR
+participant "Document Service" as DS
+database "Document DB" as DB
+
+UserA -> ClientA: Edit document
+activate ClientA
+
+ClientA -> ClientA: Generate local change
+ClientA -> WSS: Send change
+activate WSS
+
+WSS -> OTE: Apply operational transform
+activate OTE
+
+OTE -> CR: Check for conflicts
+activate CR
+
+CR -> DS: Fetch latest document state
+activate DS
+
+DS -> DB: Read document
+activate DB
+DB --> DS: Return document
+deactivate DB
+
+DS --> CR: Return latest state
+deactivate DS
+
+CR -> CR: Resolve conflicts (if any)
+CR --> OTE: Return resolved change
+deactivate CR
+
+OTE -> WSS: Return transformed change
+deactivate OTE
+
+WSS -> ClientB: Broadcast change to other clients
+WSS -> ClientA: Acknowledge change
+deactivate WSS
+
+ClientA -> ClientA: Apply change locally
+ClientA --> UserA: Update UI
+deactivate ClientA
+
+ClientB -> ClientB: Apply received change
+ClientB -> UserB: Update UI
+
+@enduml
diff --git a/high_traffic_calendar/high_traffic_calendar_backend_overview.puml b/high_traffic_calendar/high_traffic_calendar_backend_overview.puml
new file mode 100644
--- /dev/null
+++ ./high_traffic_calendar/high_traffic_calendar_backend_overview.puml
@@ -0,0 +1,79 @@
+@startuml
+actor User
+
+package "Front End" {
+  rectangle "Load Balancer" {
+    [Nginx/HAProxy]
+  }
+  rectangle "API Gateway" {
+    [Create Event]
+    [Update Event]
+    [Delete Event]
+    [Get Events]
+  }
+}
+
+package "Back End" {
+  rectangle "Web Server" {
+    [Business Logic]
+  }
+  
+  queue "Message Queue" {
+    [Kafka]
+  }
+  
+  rectangle "Workers" {
+    [Send Notification]
+    [Process Recurring Events]
+    [Data Analysis & Reporting]
+    [Cache Update]
+    [Search Index Update]
+  }
+  
+  rectangle "Worker Nodes" {
+    [Kafka Consumer]
+  }
+}
+
+package "Data Storage" {
+  database "Databases" {
+    [Primary DB: MongoDB]
+    [Cache: Redis]
+    [Search Engine: Elasticsearch]
+  }
+}
+
+User --> "Load Balancer"
+"Load Balancer" --> "API Gateway"
+"API Gateway" --> "Web Server"
+
+"Web Server" --> [Kafka]
+[Kafka] --> "Workers"
+"Workers" --> [Kafka]: Results
+
+"Web Server" <--> [Cache: Redis]: Check/Update
+"Web Server" <--> [Primary DB: MongoDB]: Query/Update
+"Web Server" --> [Search Engine: Elasticsearch]: Search
+
+[Kafka] --> "Worker Nodes"
+"Worker Nodes" <--> [Primary DB: MongoDB]: Query/Update
+"Worker Nodes" --> [Cache: Redis]: Invalidate/Update
+"Worker Nodes" --> [Search Engine: Elasticsearch]: Update Index
+
+note right of "Web Server"
+  Handles core business logic,
+  interacts with database and cache,
+  publishes messages to Kafka
+end note
+
+note right of [Kafka]
+  Enables loose coupling 
+  and high-throughput 
+  event streaming
+end note
+
+note right of [Primary DB: MongoDB]
+  Flexible, scalable
+  document-based storage
+end note
+@enduml
diff --git a/high_traffic_calendar/high_traffic_calendar_data_flow.puml b/high_traffic_calendar/high_traffic_calendar_data_flow.puml
new file mode 100644
--- /dev/null
+++ ./high_traffic_calendar/high_traffic_calendar_data_flow.puml
@@ -0,0 +1,58 @@
+@startuml
+actor User
+
+rectangle "API Gateway" {
+  [Create Event]
+  [Update Event]
+  [Delete Event]
+  [Get Events]
+}
+
+rectangle "Web Server" {
+  [Read Handler]
+  [Write Handler]
+}
+
+rectangle "Kafka" {
+  queue "Write Events"
+  queue "Cache Invalidation"
+}
+
+rectangle "Cache" {
+  [Redis]
+}
+
+rectangle "MongoDB Cluster" {
+  [Query Routers (mongos)]
+  [Primary Node]
+  [Secondary Node 1]
+  [Secondary Node 2]
+}
+
+rectangle "Prefetch Service" {
+  [Data Analyzer]
+  [Prefetch Worker]
+}
+
+User --> "API Gateway": Request
+"API Gateway" --> "Web Server": Forward Request
+
+"Web Server" --> "Cache": Check Cache (for Get Events)
+"Cache" --> "Web Server": Cache Hit/Miss
+
+"Read Handler" --> "Query Routers (mongos)": Read Request
+"Query Routers (mongos)" --> "Secondary Node 1": Distribute Read
+"Query Routers (mongos)" --> "Secondary Node 2": Distribute Read
+
+"Write Handler" --> "Kafka": Publish Write Event
+"Kafka" --> "Primary Node": Consume Write Event
+"Primary Node" --> "Secondary Node 1": Replicate Data
+"Primary Node" --> "Secondary Node 2": Replicate Data
+
+"Write Handler" --> "Kafka": Publish Cache Invalidation
+"Kafka" --> "Cache": Consume and Invalidate
+
+"Prefetch Service" --> "MongoDB Cluster": Analyze Data Patterns
+"Prefetch Service" --> "Cache": Prefetch Likely Needed Data
+
+@enduml
diff --git a/high_traffic_calendar/high_traffic_calendar_mongodb_architecture.puml b/high_traffic_calendar/high_traffic_calendar_mongodb_architecture.puml
new file mode 100644
--- /dev/null
+++ ./high_traffic_calendar/high_traffic_calendar_mongodb_architecture.puml
@@ -0,0 +1,87 @@
+@startuml
+
+package "MongoDB Cluster" {
+  package "Config Servers" {
+    [Config Server 1]
+    [Config Server 2]
+    [Config Server 3]
+  }
+  
+  package "Query Routers" {
+    [mongos 1]
+    [mongos 2]
+  }
+  
+  package "Shards" {
+    package "Shard 1" {
+      [Primary Node 1]
+      [Secondary Node 1]
+      [Secondary Node 2]
+    }
+    package "Shard 2" {
+      [Primary Node 2]
+      [Secondary Node 3]
+      [Secondary Node 4]
+    }
+    package "Shard 3" {
+      [Primary Node 3]
+      [Secondary Node 5]
+      [Secondary Node 6]
+    }
+  }
+}
+
+actor User
+
+rectangle "Web Server" {
+  [API Gateway]
+  [Business Logic]
+}
+
+database "Cache" {
+  [Redis]
+}
+
+queue "Message Queue" {
+  [Kafka]
+}
+
+database "Search Engine" {
+  [Elasticsearch]
+}
+
+User --> "API Gateway": Request
+"API Gateway" --> "Business Logic": Forward Request
+"Business Logic" --> "Redis": Check Cache
+"Redis" --> "Business Logic": Cache Hit/Miss
+
+"Business Logic" --> "Query Routers (mongos)": Read Request
+"Query Routers (mongos)" --> "Secondary Node 1": Read Data
+"Query Routers (mongos)" --> "Secondary Node 3": Read Data
+"Query Routers (mongos)" --> "Secondary Node 5": Read Data
+
+"Business Logic" --> "Kafka": Publish Write Request
+"Kafka" --> "Primary Node 1": Consume Write Request
+"Kafka" --> "Primary Node 2": Consume Write Request
+"Kafka" --> "Primary Node 3": Consume Write Request
+
+"Primary Node 1" --> "Secondary Node 1": Replicate Data
+"Primary Node 2" --> "Secondary Node 3": Replicate Data
+"Primary Node 3" --> "Secondary Node 5": Replicate Data
+
+"Business Logic" --> "Kafka": Publish Cache Invalidation
+"Kafka" --> "Redis": Consume and Invalidate Cache
+
+"Business Logic" --> "Elasticsearch": Search/Index
+
+note right of "Kafka"
+  Enables asynchronous processing
+  and decouples components
+end note
+
+note right of "MongoDB Cluster"
+  Provides high availability
+  and horizontal scalability
+end note
+
+@enduml
diff --git a/high_traffic_calendar/high_traffic_calendar_mongodb_design_with_sharding_and_indexing.puml b/high_traffic_calendar/high_traffic_calendar_mongodb_design_with_sharding_and_indexing.puml
new file mode 100644
--- /dev/null
+++ ./high_traffic_calendar/high_traffic_calendar_mongodb_design_with_sharding_and_indexing.puml
@@ -0,0 +1,80 @@
+@startuml
+
+entity "Users" as U {
+  *_id: ObjectId (PK)
+  --
+  username: String
+  email: String
+  password: String
+}
+
+entity "Events" as E {
+  *_id: ObjectId (PK)
+  --
+  owner_id: ObjectId (FK to Users)
+  event_name: String
+  description: String
+  start_time: Date
+  end_time: Date
+  recurrence_id: ObjectId (FK to Events)
+  recurrence_rule: String
+  status: String
+  participants: Array<Participant>
+}
+
+entity "Participants" as P {
+  *participant_id: ObjectId (PK)
+  --
+  event_id: ObjectId (FK to Events)
+  user_id: ObjectId (FK to Users)
+  status: String
+}
+
+entity "Reminders" as R {
+  *_id: ObjectId (PK)
+  --
+  event_id: ObjectId (FK to Events)
+  user_id: ObjectId (FK to Users)
+  reminder_time: Date
+}
+
+entity "Redis Cache" as C {
+  --
+  user_calendar_{user_id}_{date_range}: List of Events
+  event_detail_{event_id}: Event Details
+  event_participants_{event_id}: List of Participants
+  upcoming_reminders_{user_id}: List of Reminders
+  frequently_accessed_events: List of Event IDs
+  user_preferences_{user_id}: User Preferences
+}
+
+note right of E
+  Indexes:
+  - owner_id: 1, start_time: 1, end_time: 1
+  - start_time: 1, end_time: 1
+  - event_name: "text", description: "text"
+  Sharding Key:
+  - owner_id: 1, start_time: 1, end_time: 1
+end note
+
+note right of R
+  Indexes:
+  - user_id: 1
+  - event_id: 1
+  Sharding Key:
+  - hash(user_id)
+end note
+
+note "Kafka Topics:\n- event_updates\n- cache_invalidations\n- reminder_notifications" as KT
+
+note "Elasticsearch Index:\n- events_index" as ESI
+
+U ||--o{ E : creates
+E ||--o{ P : includes
+E ||--o{ R : has
+U ||--o{ C : caches
+E ||--o{ C : caches
+P ||--o{ C : caches
+R ||--o{ C : caches
+
+@enduml
diff --git a/high_traffic_calendar/high_traffic_calendar_optimized_architecture.puml b/high_traffic_calendar/high_traffic_calendar_optimized_architecture.puml
new file mode 100644
--- /dev/null
+++ ./high_traffic_calendar/high_traffic_calendar_optimized_architecture.puml
@@ -0,0 +1,58 @@
+@startuml
+actor User
+
+rectangle "API Gateway" {
+  [Create Event]
+  [Update Event]
+  [Delete Event]
+  [Get Events]
+}
+
+rectangle "Web Server" {
+  [Read Handler]
+  [Write Handler]
+}
+
+rectangle "Kafka" {
+  queue "Write Events"
+  queue "Cache Invalidation"
+}
+
+rectangle "Cache" {
+  [Redis]
+}
+
+rectangle "MongoDB Cluster" {
+  [Query Routers (mongos)]
+  [Primary Node]
+  [Secondary Node 1]
+  [Secondary Node 2]
+}
+
+rectangle "Prefetch Service" {
+  [Data Analyzer]
+  [Prefetch Worker]
+}
+
+User --> "API Gateway": Request
+"API Gateway" --> "Web Server": Forward Request
+
+"Web Server" --> "Cache": Check Cache (for Get Events)
+"Cache" --> "Web Server": Cache Hit/Miss
+
+"Read Handler" --> "Query Routers (mongos)": Read Request
+"Query Routers (mongos)" --> "Secondary Node 1": Distribute Read
+"Query Routers (mongos)" --> "Secondary Node 2": Distribute Read
+
+"Write Handler" --> "Kafka": Publish Write Event
+"Kafka" --> "Primary Node": Consume Write Event
+"Primary Node" --> "Secondary Node 1": Replicate Data
+"Primary Node" --> "Secondary Node 2": Replicate Data
+
+"Write Handler" --> "Kafka": Publish Cache Invalidation
+"Kafka" --> "Cache": Consume and Invalidate
+
+"Prefetch Service" --> "MongoDB Cluster": Analyze Data Patterns
+"Prefetch Service" --> "Cache": Prefetch Likely Needed Data
+
+@enduml
diff --git a/high_traffic_calendar/process_modifying_recurrence_rules.puml b/high_traffic_calendar/process_modifying_recurrence_rules.puml
new file mode 100644
--- /dev/null
+++ ./high_traffic_calendar/process_modifying_recurrence_rules.puml
@@ -0,0 +1,55 @@
+@startuml
+
+actor User
+participant "API Gateway"
+participant "Business Logic"
+participant "Kafka"
+participant "Worker"
+database "MongoDB"
+database "Redis Cache"
+database "Elasticsearch"
+
+User -> "API Gateway": Modify Recurrence Rule Request
+"API Gateway" -> "Business Logic": Forward Request
+"Business Logic" -> "Kafka": Publish Modification Request
+
+"Kafka" -> Worker: Consume Modification Request
+Worker -> MongoDB: Find all related events by recurrence_id
+MongoDB -> Worker: Return related events
+
+alt Events Found
+    Worker -> MongoDB: Update events with new rules
+    Worker -> MongoDB: Delete future events not matching new rules
+    MongoDB -> Worker: Acknowledge updates
+
+    Worker -> MongoDB: Read updated events
+    MongoDB -> Worker: Return updated events
+
+    Worker -> "Kafka": Publish Cache Invalidation
+    "Kafka" -> "Redis Cache": Consume and Invalidate Cache
+    "Redis Cache" -> Worker: Acknowledge Cache Invalidation
+
+    Worker -> "Kafka": Publish Search Index Update
+    "Kafka" -> Elasticsearch: Consume and Update Index
+    Elasticsearch -> Worker: Acknowledge Index Update
+
+    Worker -> "Kafka": Publish Operation Result
+else No Events Found
+    Worker -> "Kafka": Publish Error or Empty Response
+end
+
+"Kafka" -> "Business Logic": Consume Operation Result
+"Business Logic" -> "API Gateway": Return response
+"API Gateway" -> User: Return result
+
+note right of "Kafka"
+  Enables asynchronous processing
+  and decouples components
+end note
+
+note right of Worker
+  Handles complex operations
+  asynchronously
+end note
+
+@enduml
diff --git a/hybrid_distributed_lock_service_design_with_redis_zookeeper_and_deadlock_prevention.puml b/hybrid_distributed_lock_service_design_with_redis_zookeeper_and_deadlock_prevention.puml
new file mode 100644
--- /dev/null
+++ ./hybrid_distributed_lock_service_design_with_redis_zookeeper_and_deadlock_prevention.puml
@@ -0,0 +1,74 @@
+@startuml Distributed Lock Service Design
+
+!define RECTANGLE class
+!define STORAGE database
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Distributed Lock Service Design with Redis and Zookeeper
+
+rectangle "Client Applications" as ClientApps #E1F5FE
+
+rectangle "Distributed Lock Service" as LockService #B3E5FC {
+    component "Lock Manager" as LockManager
+    component "Lock Monitor" as LockMonitor
+    component "Deadlock Detector" as DeadlockDetector
+}
+
+rectangle "Redis Cluster" as RedisCluster #81D4FA {
+    STORAGE "Lock Data" as RedisLockData
+}
+
+rectangle "Zookeeper Ensemble" as ZookeeperEnsemble #4FC3F7 {
+    STORAGE "Lock Znodes" as ZKLockData
+}
+
+rectangle "Metrics & Monitoring" as Monitoring #03A9F4 {
+    component "Lock Acquisition Metrics" as LockMetrics
+    component "Performance Monitor" as PerfMonitor
+}
+
+ClientApps -[#FF5722,thickness=2]-> LockService : <back:#FFFFFF><color:#FF5722>1. Request Lock</color></back>
+LockService -[#FF9800,thickness=2]-> RedisCluster : <back:#FFFFFF><color:#FF9800>2a. Acquire Redis Lock</color></back>
+LockService -[#FFC107,thickness=2]-> ZookeeperEnsemble : <back:#FFFFFF><color:#FFC107>2b. Create Znode</color></back>
+LockMonitor -[#4CAF50,thickness=2]-> RedisCluster : <back:#FFFFFF><color:#4CAF50>3a. Monitor Lock TTL</color></back>
+LockMonitor -[#8BC34A,thickness=2]-> ZookeeperEnsemble : <back:#FFFFFF><color:#8BC34A>3b. Watch Znode</color></back>
+DeadlockDetector -[#2196F3,thickness=2]-> LockService : <back:#FFFFFF><color:#2196F3>4. Detect & Resolve Deadlocks</color></back>
+LockService -[#3F51B5,thickness=2]-> ClientApps : <back:#FFFFFF><color:#3F51B5>5. Grant/Deny Lock</color></back>
+LockService -[#9C27B0,thickness=2]-> Monitoring : <back:#FFFFFF><color:#9C27B0>6. Report Metrics</color></back>
+
+note right of RedisCluster
+  Redis-based Lock:
+  - Fast, low-latency operations
+  - Automatic expiration with TTL
+  - Suitable for high-concurrency
+end note
+
+note right of ZookeeperEnsemble
+  Zookeeper-based Lock:
+  - Strong consistency
+  - Built-in leader election
+  - Suitable for long-lived locks
+end note
+
+note bottom of LockService
+  Hybrid Approach:
+  1. Use Redis for short-lived, high-concurrency locks
+  2. Use Zookeeper for long-lived, critical locks
+  3. Implement lock escalation strategy
+end note
+
+note bottom of DeadlockDetector
+  Deadlock Prevention:
+  1. Implement timeout mechanism
+  2. Use global lock ordering
+  3. Detect cycles in lock graph
+end note
+
+@enduml
+
diff --git a/hybrid_multi_cloud_architecture_with_data_sync_and_failover.puml b/hybrid_multi_cloud_architecture_with_data_sync_and_failover.puml
new file mode 100644
--- /dev/null
+++ ./hybrid_multi_cloud_architecture_with_data_sync_and_failover.puml
@@ -0,0 +1,96 @@
+@startuml Hybrid and Multi-Cloud Architecture
+
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #CCE8CF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+title Hybrid and Multi-Cloud Architecture with Data Sync and Failover
+
+rectangle "On-Premises" as OnPrem PRIMARY_COLOR {
+    component "Legacy Systems" as LegacySystems
+    component "Private Cloud" as PrivateCloud
+    database "On-Prem Database" as OnPremDB
+}
+
+cloud "Public Cloud Provider A" as CloudA SECONDARY_COLOR {
+    component "Cloud Services A" as ServicesA
+    database "Cloud Database A" as CloudDBA
+    component "Load Balancer A" as LBA
+}
+
+cloud "Public Cloud Provider B" as CloudB TERTIARY_COLOR {
+    component "Cloud Services B" as ServicesB
+    database "Cloud Database B" as CloudDBB
+    component "Load Balancer B" as LBB
+}
+
+rectangle "Global Traffic Manager" as GTM QUATERNARY_COLOR {
+    component "DNS-based Routing" as DNSRouting
+    component "Health Checking" as HealthCheck
+}
+
+rectangle "Data Sync & Replication" as DataSync QUINARY_COLOR {
+    component "Data Synchronization Service" as SyncService
+    component "Conflict Resolution" as ConflictResolution
+}
+
+GTM -[QUATERNARY_COLOR,thickness=2]down-> OnPrem : <back:#FFFFFF><color:QUATERNARY_COLOR>1. Route Traffic</color></back>
+GTM -[QUATERNARY_COLOR,thickness=2]down-> CloudA : <back:#FFFFFF><color:QUATERNARY_COLOR>1. Route Traffic</color></back>
+GTM -[QUATERNARY_COLOR,thickness=2]down-> CloudB : <back:#FFFFFF><color:QUATERNARY_COLOR>1. Route Traffic</color></back>
+
+OnPrem -[PRIMARY_COLOR,thickness=2]right-> DataSync : <back:#FFFFFF><color:PRIMARY_COLOR>2. Sync Data</color></back>
+CloudA -[SECONDARY_COLOR,thickness=2]down-> DataSync : <back:#FFFFFF><color:SECONDARY_COLOR>2. Sync Data</color></back>
+CloudB -[TERTIARY_COLOR,thickness=2]left-> DataSync : <back:#FFFFFF><color:TERTIARY_COLOR>2. Sync Data</color></back>
+
+DataSync -[QUINARY_COLOR,thickness=2]-> OnPremDB : <back:#FFFFFF><color:QUINARY_COLOR>3. Replicate</color></back>
+DataSync -[QUINARY_COLOR,thickness=2]-> CloudDBA : <back:#FFFFFF><color:QUINARY_COLOR>3. Replicate</color></back>
+DataSync -[QUINARY_COLOR,thickness=2]-> CloudDBB : <back:#FFFFFF><color:QUINARY_COLOR>3. Replicate</color></back>
+
+note right of GTM
+  Global Traffic Manager:
+  - Geo-based routing
+  - Failover management
+  - Load distribution
+end note
+
+note right of DataSync
+  Data Sync & Replication:
+  - Multi-master replication
+  - Eventual consistency
+  - Conflict detection and resolution
+end note
+
+note bottom of OnPrem
+  On-Premises:
+  - Gradual cloud migration
+  - Compliance-sensitive workloads
+  - Legacy system integration
+end note
+
+note bottom of CloudA
+  Cloud Provider A:
+  - Primary public cloud
+  - Scalable compute resources
+  - Managed services utilization
+end note
+
+note bottom of CloudB
+  Cloud Provider B:
+  - Secondary public cloud
+  - Disaster recovery site
+  - Vendor lock-in prevention
+end note
+
+@enduml
diff --git a/imvu_system_architecture/avatar_loading_optimization_process.puml b/imvu_system_architecture/avatar_loading_optimization_process.puml
new file mode 100644
--- /dev/null
+++ ./imvu_system_architecture/avatar_loading_optimization_process.puml
@@ -0,0 +1,42 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+actor User
+participant "Chat Room Service" as ChatRoomService
+database "Product Database" as ProductDB
+participant "Metadata Analysis" as MetadataAnalysis
+participant "Sorting System" as SortingSystem
+database "Cache" as Cache
+participant "Avatar Loading Module" as AvatarLoading
+
+User -> ChatRoomService: Request product IDs
+ChatRoomService -> ProductDB: Fetch product IDs
+    alt Product IDs not found
+        ProductDB -> ChatRoomService: Return error (Product IDs not found)
+        ChatRoomService -> User: Return error (Product IDs not found)
+    else Product IDs found
+ProductDB -> ChatRoomService: Return product IDs
+ChatRoomService -> MetadataAnalysis: Analyze product IDs metadata
+loop Each Product ID
+    MetadataAnalysis -> MetadataAnalysis: Determine overlaps and visibility
+end
+MetadataAnalysis -> SortingSystem: Provide product IDs with visibility and overlap info
+SortingSystem -> SortingSystem: Canonical sort of product IDs
+SortingSystem -> Cache: Check cached avatars for sorted product IDs
+alt Cache hit
+    Cache -> ChatRoomService: Provide cached avatars
+    ChatRoomService -> User: Display cached avatars
+else Cache miss
+    SortingSystem -> AvatarLoading: Request avatar generation for sorted product IDs
+            alt Avatar generation failed
+                AvatarLoading -> SortingSystem: Return error (Avatar generation failed)
+                SortingSystem -> ChatRoomService: Return error (Avatar generation failed)
+                ChatRoomService -> User: Return error (Avatar generation failed)
+            else Avatar generation successful
+    AvatarLoading -> Cache: Update cache with new avatars
+    Cache -> ChatRoomService: Provide newly generated avatars
+    ChatRoomService -> User: Display newly generated avatars
+end
+        end
+    end
+@enduml
diff --git a/imvu_system_architecture/imvu_data_storage_overview.puml b/imvu_system_architecture/imvu_data_storage_overview.puml
new file mode 100644
--- /dev/null
+++ ./imvu_system_architecture/imvu_data_storage_overview.puml
@@ -0,0 +1,73 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+package "Relational Database" as RDB {
+    [User Basic Information]
+    [Transaction Records]
+    [Virtual Goods Catalog]
+    [Social Relationship Network]
+}
+note right of RDB
+  Suitable for structured and
+  transactional data
+  Supports complex relationship queries
+end note
+
+package "NoSQL Database" as NoSQL {
+    [User Behavior Data]
+    [Social Interaction Content]
+    [User Preferences and Settings]
+}
+note left of NoSQL
+  Flexible data models
+  Suitable for personalized services
+  and large data sets
+  Suitable for User Preferences
+  and Settings
+end note
+
+package "Elasticsearch" as ES {
+    [User Browsing History]
+    [Search Queries]
+    [Click Data]
+    [Purchase History]
+    [Product Descriptions]
+    [Tags and Keywords]
+    [Ratings and Reviews]
+    [User Activity Logs and System Metrics]
+}
+note right of ES
+  Fast searching and real-time analysis
+  Suitable for logs and large data sets
+  Ideal for analyzing user data and behavior
+  for recommendations
+end note
+
+package "Redis" as Redis {
+    [User Session Data]
+    [Frequently Accessed Data Cache]
+    [Real-time Leaderboards]
+    [Counters and Rate Limiters]
+}
+note left of Redis
+  Fast access and caching
+  Suitable for sessions, leaderboards,
+  messaging, and rate limiting
+end note
+
+package "Apache Kafka" as Kafka {
+    [Publish/Subscribe Messaging]
+    [Task Queues]
+}
+note right of Kafka
+  High-throughput, distributed messaging
+  Suitable for large-scale message processing
+  and task queues
+end note
+
+RDB --> NoSQL: Export relational data
+NoSQL --> ES: Index for fast search
+ES --> Redis: Cache frequently accessed data
+Redis --> Kafka: Stream updates
+
+@enduml
diff --git a/imvu_system_architecture/imvu_elasticsearch_architecture_diagram.puml b/imvu_system_architecture/imvu_elasticsearch_architecture_diagram.puml
new file mode 100644
--- /dev/null
+++ ./imvu_system_architecture/imvu_elasticsearch_architecture_diagram.puml
@@ -0,0 +1,61 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+component "Logstash" as Logstash
+component "Fluentd" as Fluentd
+component "Custom API" as CustomAPI
+component "Beats" as Beats
+
+component "Apache Kafka" as Kafka {
+    note right of Kafka
+      Real-time data stream processing
+      High-throughput data transfer
+      Connecting data sources and targets
+    end note
+}
+component "Apache Spark" as Spark {
+    note right of Spark
+      Data processing and analysis
+      Batch and stream processing capabilities
+      Large-scale dataset processing
+    end note
+}
+
+component "TensorFlow" as TensorFlow {
+    note right of TensorFlow
+      Deep learning and machine learning library
+      Image recognition, natural language processing
+      Recommendation systems, anomaly detection
+    end note
+}
+component "Elasticsearch SQL" as ESSQL {
+    note right of ESSQL
+      Use SQL to query Elasticsearch
+      Log analysis, business intelligence reporting
+      Security analysis, real-time monitoring
+    end note
+}
+component "Kibana" as Kibana
+component "Grafana" as Grafana
+
+database "Elasticsearch" {
+    [Elasticsearch DB]
+}
+
+Beats --> Logstash : Collect logs
+Logstash --> Kafka : Ingest logs
+Fluentd --> Kafka : Ingest logs
+CustomAPI --> Kafka : Ingest custom data
+
+Kafka --> Spark : Stream data
+
+Spark --> [Elasticsearch DB] : Process data &\nStore results
+
+[Elasticsearch DB] --> TensorFlow : Source data for analysis
+TensorFlow --> [Elasticsearch DB] : Store analysis results
+[Elasticsearch DB] --> ESSQL : Query execution
+ESSQL --> [Elasticsearch DB] : Store query results
+Kibana -.-> [Elasticsearch DB] : Visualize data
+Grafana -.-> [Elasticsearch DB] : Visualize data
+
+@enduml
diff --git a/imvu_system_architecture/imvu_kafka_spark_real_time_processing_for_user_activity_and_analytics.puml b/imvu_system_architecture/imvu_kafka_spark_real_time_processing_for_user_activity_and_analytics.puml
new file mode 100644
--- /dev/null
+++ ./imvu_system_architecture/imvu_kafka_spark_real_time_processing_for_user_activity_and_analytics.puml
@@ -0,0 +1,59 @@
+@startuml
+skinparam backgroundColor #D3D3D3
+
+package "IMVU Data Sources" {
+    [User Activity]
+    [Transaction Records]
+    [System Logs]
+}
+
+package "Kafka Cluster" {
+    [User Events Topic]
+    [Transaction Topic]
+    [Log Topic]
+}
+
+package "Spark Cluster" {
+    package "Spark Streaming" {
+        [Data Receiver] as Receiver
+        [Data Transformation] as Transformation
+        [Data Aggregation] as Aggregation
+        [Output Processor] as Output
+
+        Receiver --> Transformation : Streams raw data
+        Transformation --> Aggregation : Transforms data
+        Aggregation --> Output : Aggregates data
+    }
+}
+
+package "Data Storage" {
+    [MongoDB]
+    [Amazon S3 Data Lake]
+    [Elasticsearch]
+}
+
+package "Monitoring & Logging" {
+    [Monitoring System]
+    [Logging System]
+}
+
+[User Activity] --> [User Events Topic] : Streams data
+[Transaction Records] --> [Transaction Topic] : Streams data
+[System Logs] --> [Log Topic] : Streams data
+
+[User Events Topic] --> Receiver
+[Transaction Topic] --> Receiver
+[Log Topic] --> Receiver
+
+Output --> [MongoDB] : Stores processed data
+Output --> [Amazon S3 Data Lake] : Stores processed data
+Output --> [Elasticsearch] : Indexes data for search
+
+[Spark Streaming] ..> [Monitoring System] : Reports status
+[Spark Streaming] ..> [Logging System] : Logs activities
+
+' 说明注释
+note top of Receiver : "Receiver接收来自Kafka Topic的流数据，并传递给Transformation进行处理。"
+note top of Output : "Output将处理后的数据存储到不同的存储系统中，以便进一步分析和查询。"
+
+@enduml
diff --git a/interview_questions/distributed_password_cracker_architecture.puml b/interview_questions/distributed_password_cracker_architecture.puml
new file mode 100644
--- /dev/null
+++ ./interview_questions/distributed_password_cracker_architecture.puml
@@ -0,0 +1,85 @@
+@startuml
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #F0F0F0
+
+title Optimized Distributed Password Cracker System Architecture
+caption Enhanced system for recovering a lost Threads account password
+
+rectangle "Private Cloud Infrastructure" as PrivateCloud #E6F3FF {
+    component "Central Coordination Node" as Coordinator #87CEFA
+    
+    rectangle "Apache Spark Cluster" as SparkCluster #FFE6CC {
+        component "Distributed Producer Node 1" as DistProducer1 #FFA07A
+        component "Distributed Producer Node 2" as DistProducer2 #FFA07A
+        component "Kubernetes Autoscaler" as Autoscaler #98FB98
+        component "Dynamic Consumer Node" as DynamicConsumer #F0E68C
+    }
+    
+    component "Central Pub/Sub Messaging System" as PubSub #DDA0DD
+    component "Load Balancer" as LoadBalancer #20B2AA
+    component "Caching Layer" as Cache #FF69B4
+    note right of Cache
+        key: attempted_passwords
+        value: API_response
+    end note
+}
+
+rectangle "Free Compute Resources" as FreeResources #E6E6FA {
+    component "Consumer Node 1" as FreeConsumer1 #87CEEB
+    component "Consumer Node 2" as FreeConsumer2 #87CEEB
+}
+
+component "RESTful Thread API" as API #FF6347
+component "Monitoring & Logging" as Monitoring #7B68EE
+
+Coordinator -[#0000FF,thickness=2]right-> DistProducer1 : <back:#FFFFFF><color:#0000FF>1. Assign Range & Manage</color></back>
+Coordinator -[#0000FF,thickness=2]right-> DistProducer2 : <back:#FFFFFF><color:#0000FF>1. Assign Range & Manage</color></back>
+Coordinator -[#800080,thickness=2]down-> PubSub : <back:#FFFFFF><color:#800080>2. System Management & Configuration</color></back>
+
+DistProducer1 -[#008000,thickness=2]down-> PubSub : <back:#FFFFFF><color:#008000>3. Publish Passwords</color></back>
+DistProducer2 -[#008000,thickness=2]down-> PubSub : <back:#FFFFFF><color:#008000>3. Publish Passwords</color></back>
+
+PubSub -[#FFA500,thickness=2]down-> LoadBalancer : <back:#FFFFFF><color:#FFA500>4. Distribute Workload</color></back>
+
+LoadBalancer -[#FF1493,thickness=2]down-> FreeConsumer1 : <back:#FFFFFF><color:#FF1493>5. Assign Tasks</color></back>
+LoadBalancer -[#FF1493,thickness=2]down-> FreeConsumer2 : <back:#FFFFFF><color:#FF1493>5. Assign Tasks</color></back>
+LoadBalancer -[#FF1493,thickness=2]right-> DynamicConsumer : <back:#FFFFFF><color:#FF1493>5. Assign Tasks</color></back>
+
+FreeConsumer1 -[#4B0082,thickness=2]down-> Cache : <back:#FFFFFF><color:#4B0082>6. Check Cache</color></back>
+FreeConsumer2 -[#4B0082,thickness=2]down-> Cache : <back:#FFFFFF><color:#4B0082>6. Check Cache</color></back>
+DynamicConsumer -[#4B0082,thickness=2]left-> Cache : <back:#FFFFFF><color:#4B0082>6. Check Cache</color></back>
+
+Cache -[#8B4513,thickness=2]down-> API : <back:#FFFFFF><color:#8B4513>7. Validate Password (if not in cache)</color></back>
+
+API -[#DAA520,thickness=2]up-> Cache : <back:#FFFFFF><color:#DAA520>8. Update Cache</color></back>
+
+Cache -[#2E8B57,thickness=2]up-> FreeConsumer1 : <back:#FFFFFF><color:#2E8B57>9. Return Result</color></back>
+Cache -[#2E8B57,thickness=2]up-> FreeConsumer2 : <back:#FFFFFF><color:#2E8B57>9. Return Result</color></back>
+Cache -[#2E8B57,thickness=2]right-> DynamicConsumer : <back:#FFFFFF><color:#2E8B57>9. Return Result</color></back>
+
+Autoscaler -[#FF4500,thickness=2]down-> PubSub : <back:#FFFFFF><color:#FF4500>10. Monitor Queue Length</color></back>
+Autoscaler -[#1E90FF,thickness=2]right-> DistProducer1 : <back:#FFFFFF><color:#1E90FF>11. Scale In/Out</color></back>
+Autoscaler -[#1E90FF,thickness=2]right-> DistProducer2 : <back:#FFFFFF><color:#1E90FF>11. Scale In/Out</color></back>
+Autoscaler -[#1E90FF,thickness=2]down-> DynamicConsumer : <back:#FFFFFF><color:#1E90FF>11. Scale In/Out</color></back>
+
+Coordinator -[#FF0000,thickness=2]-> Monitoring : <back:#FFFFFF><color:#FF0000>12. Log & Monitor</color></back>
+PubSub -[#FF0000,thickness=2]-> Monitoring : <back:#FFFFFF><color:#FF0000>12. Log & Monitor</color></back>
+LoadBalancer -[#FF0000,thickness=2]-> Monitoring : <back:#FFFFFF><color:#FF0000>12. Log & Monitor</color></back>
+Cache -[#FF0000,thickness=2]-> Monitoring : <back:#FFFFFF><color:#FF0000>12. Log & Monitor</color></back>
+
+note right of API
+    Performance bottleneck:
+    Implement rate limiting
+    and consider caching
+    frequent API responses
+end note
+
+note bottom of Cache
+    Optimization:
+    Use distributed cache
+    for better performance
+    and scalability
+end note
+
+@enduml
diff --git a/iot_log_collection_processing_and_alerting_system_architecture.puml b/iot_log_collection_processing_and_alerting_system_architecture.puml
new file mode 100644
--- /dev/null
+++ ./iot_log_collection_processing_and_alerting_system_architecture.puml
@@ -0,0 +1,96 @@
+@startuml Distributed Log Collection, Processing, and Analysis System
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam monochrome false
+skinparam packageStyle rectangle
+skinparam shadowing false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 16
+skinparam roundCorner 10
+skinparam ArrowColor #2C3E50
+skinparam ArrowThickness 1.5
+
+rectangle "Log Sources" as Sources #3498DB {
+    [IoT Devices]
+    [Web Servers]
+    [Databases]
+    [Applications]
+}
+
+rectangle "Log Collection" as Collection #2ECC71 {
+    [Log Agents]
+    [Log Shippers]
+    [API Endpoints]
+}
+
+rectangle "Message Queue" as Queue #F1C40F {
+    [Kafka]
+    [RabbitMQ]
+}
+
+rectangle "Processing Layer" as Processing #E67E22 {
+    rectangle "Stream Processing" as StreamProcessing {
+        [Apache Flink]
+        [Apache Spark Streaming]
+    }
+    rectangle "Batch Processing" as BatchProcessing {
+        [Hadoop MapReduce]
+        [Apache Spark]
+    }
+}
+
+database "Data Storage" as Storage #9B59B6 {
+    [Elasticsearch]
+    [HDFS]
+    [S3]
+}
+
+rectangle "Analysis & Insights" as Analysis #E74C3C {
+    [Machine Learning Models]
+    [Statistical Analysis]
+    [Log Correlation Engine]
+}
+
+rectangle "Visualization & Alerting" as Visualization #95A5A6 {
+    [Kibana]
+    [Grafana]
+    [Custom Dashboards]
+    [Alert Manager]
+}
+
+Sources --> Collection : Generate
+Collection --> Queue : Collect & Ship
+Queue --> StreamProcessing : Real-time
+Queue --> BatchProcessing : Batch
+StreamProcessing --> Storage : Store
+BatchProcessing --> Storage : Store
+Storage --> Analysis : Analyze
+Analysis --> Visualization : Visualize & Alert
+
+note bottom of Sources #3498DB
+  Diverse log sources including IoT devices,
+  servers, databases, and applications
+end note
+
+note bottom of Queue #F1C40F
+  Ensures scalability and decouples
+  collection from processing
+end note
+
+note right of StreamProcessing #E67E22
+  Handles real-time log analysis
+  for immediate insights and alerts
+end note
+
+note right of BatchProcessing #E67E22
+  Processes large volumes of
+  historical data for deep analysis
+end note
+
+note bottom of Analysis #E74C3C
+  Applies advanced analytics to detect
+  patterns, anomalies, and generate insights
+end note
+
+@enduml
diff --git a/itutorgroup_h2h_platform_architecture.puml b/itutorgroup_h2h_platform_architecture.puml
new file mode 100644
--- /dev/null
+++ ./itutorgroup_h2h_platform_architecture.puml
@@ -0,0 +1,76 @@
+@startuml
+
+skinparam packageStyle rect
+
+package "User Interaction" {
+    [User Interface]
+    [AV Stream]
+    [Multi-user Chat]
+    [Websocket]
+    [Whiteboard]
+}
+
+package "Frontend" {
+    [Frontend Logic]
+}
+
+package "Backend" {
+    [Backend Logic]
+}
+
+package "Communication Components" {
+    [Strophe]
+    [Vysper Server]
+    [WebRTC]
+    [Kurento server/client]
+    [Socket.io]
+}
+
+package "AWS Infrastructure" {
+    [EC2]
+    [S3]
+    [Lambda]
+    [RDS]
+    [ElastiCache]
+    [CloudFront]
+}
+
+package "Development & Deployment" {
+    [Docker]
+    [Shell Scripts]
+    [Auto-deployment]
+    [Auto-provisioning]
+}
+
+' Connections between User Interaction and Frontend
+[User Interface] --> [Frontend Logic]
+[Frontend Logic] --> [AV Stream]
+[Frontend Logic] --> [Multi-user Chat]
+[Frontend Logic] --> [Websocket]
+[Frontend Logic] --> [Whiteboard]
+
+' Connections between Frontend and Backend
+[Frontend Logic] --> [Backend Logic]
+
+' Connections between Backend and AWS Infrastructure
+[Backend Logic] --> [EC2]
+[Backend Logic] --> [S3]
+[Backend Logic] --> [Lambda]
+[Backend Logic] --> [RDS]
+[Backend Logic] --> [ElastiCache]
+[Backend Logic] --> [CloudFront]
+
+' Connections between Backend and Communication Components
+[Backend Logic] --> [Strophe]
+[Backend Logic] --> [Vysper Server]
+[Backend Logic] --> [WebRTC]
+[Backend Logic] --> [Kurento server/client]
+[Backend Logic] --> [Socket.io]
+
+' Connections between Backend and Development & Deployment
+[Backend Logic] --> [Docker]
+[Backend Logic] --> [Shell Scripts]
+[Backend Logic] --> [Auto-deployment]
+[Backend Logic] --> [Auto-provisioning]
+
+@enduml
diff --git a/kubernetes_based_container_orchestration_system_architecture.puml b/kubernetes_based_container_orchestration_system_architecture.puml
new file mode 100644
--- /dev/null
+++ ./kubernetes_based_container_orchestration_system_architecture.puml
@@ -0,0 +1,124 @@
+@startuml Container Orchestration System Design
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam roundcorner 20
+skinparam shadowing true
+
+title Container Orchestration System Design (Based on Kubernetes)
+
+' User Interaction
+rectangle "User Interaction" as UserInteraction {
+    component "kubectl CLI"
+    component "Web UI"
+    component "API Client"
+}
+
+' Control Plane
+rectangle "Control Plane" as ControlPlane {
+    rectangle "API Server" as APIServer {
+        component "Authentication"
+        component "Authorization"
+        component "Admission Control"
+    }
+    rectangle "Scheduler" as Scheduler {
+        component "Node Selection"
+        component "Resource Allocation"
+    }
+    rectangle "Controller Manager" as ControllerManager {
+        component "Replication Controller"
+        component "Node Controller"
+        component "Service Controller"
+    }
+    database "etcd" as ETCD {
+        component "Cluster State"
+        component "Configuration Data"
+    }
+}
+
+' Worker Nodes
+rectangle "Worker Nodes" as WorkerNodes {
+    rectangle "Node 1" as Node1 {
+        component "kubelet"
+        component "Container Runtime"
+        component "kube-proxy"
+    }
+    rectangle "Node 2" as Node2 {
+        component "kubelet"
+        component "Container Runtime"
+        component "kube-proxy"
+    }
+    rectangle "Node N" as NodeN {
+        component "kubelet"
+        component "Container Runtime"
+        component "kube-proxy"
+    }
+}
+
+' Networking
+rectangle "Networking" as Networking {
+    component "CNI Plugin"
+    component "Service Network"
+    component "Pod Network"
+}
+
+' Storage
+rectangle "Storage" as Storage {
+    component "CSI Plugin"
+    component "Persistent Volumes"
+    component "Storage Classes"
+}
+
+' Monitoring and Logging
+rectangle "Monitoring and Logging" as Monitoring {
+    component "Prometheus"
+    component "Grafana"
+    component "ELK Stack"
+}
+
+' Security
+rectangle "Security" as Security {
+    component "RBAC"
+    component "Network Policies"
+    component "Secret Management"
+}
+
+' Connections
+UserInteraction -down-> APIServer : Request
+APIServer <-right-> ETCD : Read/Write State
+Scheduler -up-> APIServer : Listen/Update
+ControllerManager -up-> APIServer : Listen/Update
+APIServer -down-> WorkerNodes : Manage
+Networking -up-> WorkerNodes : Provide Network
+Storage -up-> WorkerNodes : Provide Storage
+Monitoring -up-> ControlPlane : Monitor
+Monitoring -up-> WorkerNodes : Monitor
+Security -up-> ControlPlane : Protect
+Security -up-> WorkerNodes : Protect
+
+note right of ControlPlane
+  Control Plane Responsibilities:
+  1. API Server: Unified entry point for the cluster
+  2. Scheduler: Decides Pod deployment location
+  3. Controller: Maintains desired state
+  4. etcd: Stores cluster state and configuration
+end note
+
+note left of WorkerNodes
+  Worker Node Components:
+  1. kubelet: Manages containers on the node
+  2. Container Runtime: Runs containers (e.g., Docker)
+  3. kube-proxy: Manages network rules
+end note
+
+note bottom of Networking
+  Network Functions:
+  1. Pod-to-Pod communication
+  2. Service abstraction
+  3. Load balancing
+  4. Network policies
+end note
+
+@enduml
diff --git a/large_scale_geographic_information_system_architecture_with_microservices_and_spatial_data_processing.puml b/large_scale_geographic_information_system_architecture_with_microservices_and_spatial_data_processing.puml
new file mode 100644
--- /dev/null
+++ ./large_scale_geographic_information_system_architecture_with_microservices_and_spatial_data_processing.puml
@@ -0,0 +1,101 @@
+@startuml Large Scale Geographic Information System Architecture
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #CCE8CF
+
+rectangle "Client Applications" as ClientApps PRIMARY_COLOR {
+    component "Web Interface" as WebUI
+    component "Mobile App" as MobileApp
+    component "Desktop GIS Tool" as DesktopGIS
+}
+
+rectangle "API Gateway" as APIGateway SECONDARY_COLOR {
+    component "Authentication" as Auth
+    component "Rate Limiting" as RateLimit
+    component "Request Routing" as RequestRouting
+}
+
+rectangle "GIS Services" as GISServices TERTIARY_COLOR {
+    component "Spatial Query Service" as SpatialQuery
+    component "Geocoding Service" as Geocoding
+    component "Map Rendering Service" as MapRendering
+    component "Data Import/Export Service" as DataIO
+}
+
+rectangle "Data Processing" as DataProcessing QUATERNARY_COLOR {
+    component "ETL Pipeline" as ETL
+    component "Spatial Index Builder" as IndexBuilder
+    component "Data Validation" as DataValidation
+}
+
+rectangle "Storage" as Storage QUINARY_COLOR {
+    database "Spatial Database" as SpatialDB {
+        component "PostGIS" as PostGIS
+    }
+    database "Object Storage" as ObjectStorage {
+        component "S3 Compatible" as S3
+    }
+    component "Distributed Cache" as Cache {
+        component "Redis" as Redis
+    }
+}
+
+rectangle "Analytics" as Analytics PRIMARY_COLOR {
+    component "Spatial Analysis" as SpatialAnalysis
+    component "Reporting Engine" as Reporting
+}
+
+ClientApps -[PRIMARY_COLOR,thickness=2]down-> APIGateway : <back:#FFFFFF><color:#E67E22>1. Send requests</color></back>
+APIGateway -[SECONDARY_COLOR,thickness=2]down-> GISServices : <back:#FFFFFF><color:#3498DB>2. Route requests</color></back>
+GISServices -[TERTIARY_COLOR,thickness=2]right-> DataProcessing : <back:#FFFFFF><color:#F1C40F>3. Process data</color></back>
+GISServices -[TERTIARY_COLOR,thickness=2]down-> Storage : <back:#FFFFFF><color:#F1C40F>4. Query/Store data</color></back>
+DataProcessing -[QUATERNARY_COLOR,thickness=2]down-> Storage : <back:#FFFFFF><color:#8E44AD>5. Read/Write data</color></back>
+GISServices -[TERTIARY_COLOR,thickness=2]left-> Analytics : <back:#FFFFFF><color:#F1C40F>6. Perform analysis</color></back>
+Analytics -[PRIMARY_COLOR,thickness=2]down-> Storage : <back:#FFFFFF><color:#E67E22>7. Read data for analysis</color></back>
+
+note right of ClientApps
+  Multiple interfaces for
+  different user needs
+end note
+
+note bottom of APIGateway
+  Centralized entry point
+  for all client requests
+end note
+
+note top of GISServices
+  Core GIS functionalities
+  as microservices
+end note
+
+note bottom of DataProcessing
+  Handles data ingestion
+  and preprocessing
+end note
+
+note bottom of Storage
+  Optimized for spatial
+  data storage and retrieval
+end note
+
+note top of Analytics
+  Advanced spatial analysis
+  and reporting capabilities
+end note
+
+note "Performance bottleneck:\nOptimize spatial queries\nand indexing for large datasets" as PerformanceNote
+PerformanceNote .. SpatialQuery
+PerformanceNote .. IndexBuilder
+
+note "Scalability concern:\nImplement sharding for\nhorizontal scaling of spatial data" as ScalabilityNote
+ScalabilityNote .. SpatialDB
+
+@enduml
diff --git a/map.txt b/map.txt
new file mode 100644
--- /dev/null
+++ ./map.txt
@@ -0,0 +1,197 @@
+1. 分布式系统
+    blockchain_system_detailed_architecture_with_components.puml
+    kubernetes_based_container_orchestration_system_architecture.puml
+    distributed_cache_system_with_consistency_strategies.puml
+    distributed_file_system/distributed_file_system_architecture_with_replication_and_fault_tolerance.puml
+    distributed_file_system/distributed_file_system_overview_with_components_and_data_flow.puml
+    distributed_system_cap_theorem_illustration.puml
+    distributed_system_monitoring_and_alerting_architecture.puml
+    fault_tolerant_and_high_availability_system_design.puml
+    distributed_message_queue_system_with_pub_sub_and_persistence.puml
+    distributed_web_crawler_system_architecture.puml
+    container_orchestration_system_design_with_scaling_and_load_balancing.puml
+    distributed_cache_system_with_consistency_eviction_cache_warming_and_bloom_filter.puml
+    distributed_configuration_center_with_version_control_and_real_time_updates.puml
+    distributed_configuration_center_with_caching_and_change_notification.puml
+    distributed_file_storage_and_sync_system_with_metadata_management_and_analytics.puml
+    distributed_task_scheduling_system_architecture.puml
+    distributed_id_generator_system_design_with_snowflake_algorithm_zookeeper_and_monitoring.puml
+
+2. 微服务架构
+    microservices_architecture_design.puml
+    api_gateway_design.puml
+    service_discovery_and_registration_design.puml
+
+3. 数据处理和分析
+    real_time_data_ingestion_processing_and_analysis_architecture.puml
+    distributed_tracing_system_design.puml
+    iot_log_collection_processing_and_alerting_system_architecture.puml
+    imvu_system_architecture/imvu_kafka_spark_real_time_processing.puml
+    real_time_data_stream_processing_system_with_kafka_and_spark.puml
+
+4. 全球化部署
+    global_deployment_architecture_design_with_cdn_and_geo_routing.puml
+
+5. 分布式协调
+    distributed_lock_service_design_with_redis_zookeeper_and_deadlock_prevention.puml
+
+6. 内容分发网络
+    cdn/cdn_performance_optimization_strategies.puml
+
+7. 社交媒体系统
+    twitter/timeline_update_service_detailed_architecture.puml
+    twitter/tweet_system_detailed_architecture.puml
+    twitter/twitter_recommendation_service_detailed.puml
+    twitter_comment_system_with_ml_moderation_and_real_time_processing.puml
+    twitter/twitter_search_service_detailed.puml
+    twitter/twitter_content_publishing_and_management.puml
+    twitter/twitter_performance_bottlenecks_and_optimizations.puml
+
+8. 文件传输和存储
+    telegram/uml_diagrams/telegram_group_channel_management_and_moderation_system.puml
+    big_file_upload_system/big_file_upload_system_architecture.puml
+    telegram/uml_diagrams/Telegram_File_Transfer_And_Storage.puml
+    telegram/uml_diagrams/Telegram_Group_And_Channel_Management.puml
+
+9. 视频处理和流媒体
+    YouTube/YouTube_Video_Upload_and_Processing_Schema.puml
+    YouTube/YouTube_Video_Playback_and_Interaction_Schema.puml
+    youtube_video_metadata_and_user_interaction_data_schema.puml
+    youtube_video_playback_and_recommendation_system_with_cdn_analytics_ml_personalization_and_content_delivery_optimization.puml
+    youtube_data_schema_with_video_metadata_and_user_interactions.puml
+
+10. 在线广告系统
+    online_advertising_system_architecture_with_optimizations.puml
+
+11. 分布式计算
+    distributed_computing_system_for_password_cracking_with_gpu_acceleration.puml
+
+12. 高并发系统
+    seckill_system_frontend_architecture_with_caching_and_rate_limiting.puml
+    seckill_system_backend_architecture_with_queue_and_inventory_management.puml
+    seckill_system_data_layer_architecture_with_sharding_and_replication.puml
+    seckill_system_architecture/high_concurrency_seckill_system_backend_architecture.puml
+
+13. 数据库优化
+    database_performance_optimization_strategies_and_techniques.puml
+    dynamodb_distributed_storage_system_with_partitioning_and_consistency_models.puml
+
+14. 搜索系统
+    twitter/twitter_search_service_detailed.puml
+
+15. 日历系统
+    high_traffic_calendar/high_traffic_calendar_optimized_architecture.puml
+    high_traffic_calendar/high_traffic_calendar_mongodb_design_with_sharding_and_indexing.puml
+
+16. URL缩短服务
+    url_shortener_system_architecture/url_shortener_system_architecture_with_caching_and_analytics.puml
+
+17. 协同编辑
+    google_docs_architecture/google_docs_architecture_with_real_time_collaboration_and_version_control.puml
+
+18. 打车系统
+    uber_system_architecture/uber_driver_rider_matching_algorithm_detailed_flow.puml
+    uber_system_architecture/uber_system_architecture.puml
+    uber_system_architecture/uber_matching_algorithm.puml
+    uber_system_architecture/uber_real_time_location_tracking.puml
+    uber_system_architecture/uber_system_database_design.puml
+
+19. 数据存储选项
+    architecture_diagrams/data_storage_options_comparison_and_use_cases.puml
+
+20. 实时通讯
+    telegram/uml_diagrams/Telegram_Message_Prioritization_Architecture.puml
+    telegram_real_time_chat_system_with_message_queuing_and_delivery_guarantees.puml
+
+21. 系统监控和告警
+    distributed_system_monitoring_and_alerting_architecture.puml
+    distributed_task_scheduling_system_with_fault_tolerance_and_load_balancing.puml
+    comprehensive_distributed_system_monitoring_alerting_and_log_analysis_architecture.puml
+
+22. 性能优化
+    twitter/twitter_performance_bottlenecks_and_optimizations.puml
+    cdn/cdn_performance_optimization_strategies.puml
+
+23. 缓存系统
+    distributed_cache_system_with_consistency_strategies.puml
+    distributed_cache_system_with_consistency_eviction_cache_warming_and_bloom_filter.puml
+
+24. 容器编排
+    container_orchestration_system_design_with_scaling_and_load_balancing.puml
+    kubernetes_based_container_orchestration_system_architecture.puml
+
+25. 消息队列
+    distributed_message_queue_system_with_pub_sub_and_persistence.puml
+
+26. 负载均衡
+    global_deployment_architecture_design_with_cdn_and_geo_routing.puml
+
+27. 数据一致性
+    distributed_cache_system_with_consistency_strategies.puml
+    distributed_cache_system_with_consistency_eviction_cache_warming_and_bloom_filter.puml
+
+28. 实时处理
+    real_time_data_stream_processing_system_with_kafka_and_spark.puml
+    imvu_system_architecture/imvu_kafka_spark_real_time_processing.puml
+
+29. 大规模数据处理
+    distributed_web_crawler_system_architecture.puml
+    youtube_video_playback_and_recommendation_system_with_cdn_analytics_ml_personalization_and_content_delivery_optimization.puml
+
+30. 电子商务系统
+    ecommerce_order_payment_shipping_architecture_with_inventory_management.puml
+
+31. 支付系统
+    ecommerce_order_payment_shipping_architecture_with_inventory_management.puml
+    uber_payment_and_ride_management_system_with_fraud_detection.puml
+
+32. 新闻推送系统
+    real_time_news_feed_architecture_with_personalization_and_caching.puml
+
+33. 认证和授权
+    twitter/twitter_user_authentication_authorization_service_with_oauth_and_jwt.puml
+
+34. 评论系统
+    twitter/twitter_comment_system_architecture_with_moderation_and_threading.puml
+    twitter_comment_system_with_ml_moderation_and_real_time_processing.puml
+
+35. 推荐系统
+    twitter/twitter_recommendation_service_detailed.puml
+
+36. 数据库设计
+    flight_booking/flight_booking_system_database_with_inventory_and_pricing.puml
+    big_file_upload_system/database_schema_with_chunking_and_metadata.puml
+    seckill_system_architecture/database_design_with_inventory_locking_and_caching.puml
+
+37. 限流系统
+    architecture_diagrams/rate_limiting_system_design_with_token_bucket_and_leaky_bucket_algorithms.puml
+    distributed_rate_limiter_system_with_multiple_algorithms_and_redis.puml
+
+38. CAP理论
+    architecture_diagrams/cap_theory_diagram_with_practical_examples.puml
+    distributed_system_cap_theorem_illustration.puml
+
+39. 配置中心
+    distributed_configuration_center_with_version_control_and_real_time_updates.puml
+    distributed_configuration_center_with_caching_and_change_notification.puml
+
+40. 分布式ID生成
+    distributed_id_generator_system_design_with_snowflake_algorithm_zookeeper_and_monitoring.puml
+
+41. 任务调度
+    distributed_task_scheduling_system_architecture.puml
+
+42. 文件同步
+    distributed_file_storage_and_sync_system_with_metadata_management_and_analytics.puml
+
+43. 地理信息系统
+    large_scale_geographic_information_system_architecture_with_microservices_and_spatial_data_processing.puml
+
+44. Session Management
+    distributed_session_management_system_with_redis_cluster.puml
+
+45. Distributed Transactions
+    distributed_transaction_system_with_saga_pattern.puml
+
+46. Real-time Analytics
+    real_time_data_analytics_platform_with_stream_processing.puml
diff --git a/microservices_architecture_and_service_discovery.puml b/microservices_architecture_and_service_discovery.puml
new file mode 100644
--- /dev/null
+++ ./microservices_architecture_and_service_discovery.puml
@@ -0,0 +1,114 @@
+@startuml Microservices Architecture and Service Discovery
+
+skinparam {
+    backgroundColor #E0E0E0
+    handwritten false
+    defaultFontName Arial
+    defaultFontSize 18
+    roundcorner 20
+    shadowing true
+}
+
+title Microservices Architecture and Service Discovery Design
+
+rectangle "Clients" {
+    [Web Application]
+    [Mobile Application]
+    [Third-party Services]
+}
+
+rectangle "API Gateway" as APIGateway {
+    [Routing]
+    [Authentication]
+    [Rate Limiting]
+    [Caching]
+    [Protocol Conversion]
+}
+
+rectangle "Service Registry" as ServiceRegistry {
+    [Service Registry Center]
+    [Health Check]
+    [Load Balancing]
+}
+
+rectangle "Microservices Cluster" as Microservices {
+    RECTANGLE "User Service" as UserService {
+        [User Management]
+        [Authentication]
+    }
+    RECTANGLE "Order Service" as OrderService {
+        [Order Processing]
+        [Payment Integration]
+    }
+    RECTANGLE "Product Service" as ProductService {
+        [Product Management]
+        [Inventory]
+    }
+    RECTANGLE "Recommendation Service" as RecommendationService {
+        [Personalized Recommendations]
+        [Data Analysis]
+    }
+}
+
+' Data Storage
+database "Data Storage" as DataStorage {
+    [Relational Database]
+    [NoSQL Database]
+    [Cache]
+}
+
+' Message Queue
+queue "Message Queue" as MessageQueue {
+    [Event Bus]
+}
+
+' Monitoring and Logging
+RECTANGLE "Monitoring and Logging" as Monitoring {
+    [Distributed Tracing]
+    [Log Aggregation]
+    [Performance Monitoring]
+    [Alert System]
+}
+
+' Configuration Center
+RECTANGLE "Config Center" as ConfigCenter {
+    [Configuration Management]
+    [Dynamic Configuration]
+}
+
+' Connections
+Clients -down-> APIGateway : Request
+APIGateway -down-> ServiceRegistry : Service Discovery
+APIGateway -down-> Microservices : Route Requests
+Microservices <-right-> ServiceRegistry : Register/Discover
+Microservices -down-> DataStorage : Data Access
+Microservices <--> MessageQueue : Publish/Subscribe Events
+Monitoring -up-> Microservices : Monitor
+ConfigCenter -up-> Microservices : Manage Configurations
+
+note right of APIGateway
+  API Gateway Responsibilities:
+  1. Request Routing
+  2. Authentication and Authorization
+  3. Rate Limiting and Circuit Breaking
+  4. Request/Response Transformation
+  5. Caching
+end note
+
+note left of ServiceRegistry
+  Service Discovery Mechanisms:
+  1. Client-side Discovery
+  2. Server-side Discovery
+  3. Support for Multiple Registries
+     (e.g., Eureka, Consul, ZooKeeper)
+end note
+
+note bottom of Microservices
+  Microservices Characteristics:
+  1. Independent Deployment
+  2. Decentralized Data Management
+  3. Fault Isolation
+  4. Independent Scalability
+end note
+
+@enduml
diff --git a/microservices_architecture_design_with_service_discovery_and_api_gateway.puml b/microservices_architecture_design_with_service_discovery_and_api_gateway.puml
new file mode 100644
--- /dev/null
+++ ./microservices_architecture_design_with_service_discovery_and_api_gateway.puml
@@ -0,0 +1,126 @@
+@startuml Microservices Architecture Design
+
+allowmixing
+
+skinparam {
+    backgroundColor #F0F0F0
+    handwritten false
+    defaultFontName Arial
+    defaultFontSize 12
+    roundcorner 20
+    shadowing false
+    ArrowColor #2C3E50
+    ActorBorderColor #2C3E50
+    LifeLineBorderColor #2C3E50
+    LifeLineBackgroundColor #A9DCDF
+    
+    ParticipantBorderColor #2C3E50
+    ParticipantBackgroundColor #A9DCDF
+    ParticipantFontName Arial
+    ParticipantFontSize 12
+    ParticipantFontColor #2C3E50
+    
+    ActorBackgroundColor #A9DCDF
+    ActorFontColor #2C3E50
+    ActorFontSize 12
+    ActorFontName Arial
+    
+    RectangleBackgroundColor #FFFFFF
+    
+    NoteFontName Arial
+    NoteFontSize 11
+    NoteFontColor #333333
+    NoteBackgroundColor #FFFDE7
+    NoteBorderColor #FFC107
+}
+
+title Microservices Architecture Design
+
+' Client Layer
+rectangle "Client Layer" as ClientLayer #E1F5FE {
+    component "Web Application" as WebApp
+    component "Mobile App" as MobileApp
+    component "Third-party Clients" as ThirdPartyClients
+}
+
+' API Gateway
+RECTANGLE "API Gateway" as APIGateway #FFF3E0 {
+    component "Authentication" as Auth
+    component "Rate Limiting" as RateLimit
+    component "Request Routing" as RequestRouting
+    component "Response Caching" as ResponseCaching
+}
+
+' Service Discovery
+RECTANGLE "Service Discovery" as ServiceDiscovery #E8F5E9 {
+    component "Service Registry" as ServiceRegistry
+    component "Load Balancer" as LoadBalancer
+}
+
+' Microservices
+RECTANGLE "Microservices" as Microservices #FFEBEE {
+    component "User Service" as UserService
+    component "Product Service" as ProductService
+    component "Order Service" as OrderService
+    component "Payment Service" as PaymentService
+    component "Notification Service" as NotificationService
+}
+
+' Data Layer
+RECTANGLE "Data Layer" as DataLayer #E0F7FA {
+    database "User DB" as UserDB
+    database "Product DB" as ProductDB
+    database "Order DB" as OrderDB
+    database "Payment DB" as PaymentDB
+}
+
+' Message Queue
+queue "Message Queue" as MessageQueue #F3E5F5
+
+' Monitoring and Logging
+RECTANGLE "Monitoring & Logging" as MonitoringLogging #FFF8E1 {
+    component "Centralized Logging" as CentralizedLogging
+    component "Metrics Collection" as MetricsCollection
+    component "Distributed Tracing" as DistributedTracing
+}
+
+' Connections
+ClientLayer -down-> APIGateway : 1. API Requests
+APIGateway -down-> ServiceDiscovery : 2. Service Lookup
+ServiceDiscovery -right-> Microservices : 3. Route Requests
+Microservices -down-> DataLayer : 4. Data Operations
+Microservices -left-> MessageQueue : 5. Async Communication
+MonitoringLogging -up-> Microservices : 6. Collect Metrics & Logs
+
+' Notes
+note right of APIGateway
+  Handles cross-cutting concerns:
+  - Authentication
+  - Rate limiting
+  - Request routing
+  - Response caching
+end note
+
+note right of ServiceDiscovery
+  Enables dynamic service
+  registration and discovery
+end note
+
+note right of Microservices
+  Each service is:
+  - Independently deployable
+  - Loosely coupled
+  - Focused on specific domain
+end note
+
+note right of MessageQueue
+  Enables asynchronous
+  communication between services
+end note
+
+note right of MonitoringLogging
+  Centralized monitoring and
+  logging for all services
+end note
+
+@enduml
diff --git a/microservices_fault_tolerance_with_circuit_breaker_and_service_degradation.puml b/microservices_fault_tolerance_with_circuit_breaker_and_service_degradation.puml
new file mode 100644
--- /dev/null
+++ ./microservices_fault_tolerance_with_circuit_breaker_and_service_degradation.puml
@@ -0,0 +1,74 @@
+@startuml Microservices Fault Tolerance Design
+
+!define RECTANGLE class
+!define STORAGE database
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Microservices Fault Tolerance with Circuit Breaker and Service Degradation
+
+rectangle "Client Applications" as ClientApps #E1F5FE
+
+rectangle "API Gateway" as APIGateway #B3E5FC {
+    component "Request Router" as RequestRouter
+    component "Circuit Breaker" as CircuitBreaker
+    component "Fallback Handler" as FallbackHandler
+}
+
+rectangle "Service Registry" as ServiceRegistry #81D4FA
+
+rectangle "Microservices" as Microservices #4FC3F7 {
+    component "Service A" as ServiceA
+    component "Service B" as ServiceB
+    component "Service C" as ServiceC
+}
+
+rectangle "Monitoring & Alerting" as Monitoring #03A9F4 {
+    component "Health Checker" as HealthChecker
+    component "Metrics Collector" as MetricsCollector
+}
+
+database "Database" as Database #0288D1
+
+ClientApps -[#FF5722,thickness=2]-> APIGateway : <back:#FFFFFF><color:#FF5722>1. API Request</color></back>
+APIGateway -[#FF9800,thickness=2]-> ServiceRegistry : <back:#FFFFFF><color:#FF9800>2. Service Lookup</color></back>
+APIGateway -[#FFC107,thickness=2]-> Microservices : <back:#FFFFFF><color:#FFC107>3. Forward Request</color></back>
+Microservices -[#4CAF50,thickness=2]-> Database : <back:#FFFFFF><color:#4CAF50>4. Data Operations</color></back>
+Monitoring -[#2196F3,thickness=2]-> Microservices : <back:#FFFFFF><color:#2196F3>5. Monitor Health & Metrics</color></back>
+Monitoring -[#9C27B0,thickness=2]-> APIGateway : <back:#FFFFFF><color:#9C27B0>6. Update Circuit Breaker Status</color></back>
+
+note top of CircuitBreaker
+  Circuit Breaker States:
+  - Closed: Normal operation
+  - Open: Stop calls to failing service
+  - Half-Open: Test if service recovered
+end note
+
+note bottom of FallbackHandler
+  Fallback Strategies:
+  - Return cached data
+  - Degrade service functionality
+  - Redirect to alternative service
+end note
+
+note bottom of Microservices
+  Service Degradation:
+  - Disable non-critical features
+  - Limit request rate
+  - Return simplified responses
+end note
+
+note bottom of Monitoring
+  Monitoring Responsibilities:
+  - Track error rates and latency
+  - Detect service health issues
+  - Trigger circuit breaker state changes
+end note
+
+@enduml
+
diff --git a/online_advertising_system_architecture_with_targeting_bidding_analytics_and_ml_optimizations.puml b/online_advertising_system_architecture_with_targeting_bidding_analytics_and_ml_optimizations.puml
new file mode 100644
--- /dev/null
+++ ./online_advertising_system_architecture_with_targeting_bidding_analytics_and_ml_optimizations.puml
@@ -0,0 +1,127 @@
+@startuml Online Advertising System Architecture with Optimizations
+
+!define RECTANGLE class
+!define STORAGE database
+
+skinparam backgroundColor #FAFAFA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundCorner 10
+skinparam componentStyle uml2
+allowmixing
+
+
+rectangle "Online Advertising System" {
+    RECTANGLE "Ad Server" as adserver #D6EAF8
+    RECTANGLE "Real-Time Bidding (RTB)" as rtb #D5F5E3
+    RECTANGLE "User Profiling" as profiling #FDEBD0
+    RECTANGLE "Ad Targeting Engine" as targeting #F5B7B1
+    RECTANGLE "Campaign Manager" as campaign #D7BDE2
+    RECTANGLE "Analytics Engine" as analytics #FAD7A0
+    RECTANGLE "Reporting Dashboard" as dashboard #AED6F1
+    
+    STORAGE "User Data Store" as userdb #D6DBDF
+    STORAGE "Ad Inventory" as inventory #D6DBDF
+    STORAGE "Campaign Data" as campaigndb #D6DBDF
+    STORAGE "Analytics Data Lake" as datalake #D6DBDF
+}
+
+actor "Ad Company" as adcompany
+actor "Website/App" as website
+actor "User" as user
+
+website -[#4CAF50,thickness=2]-> adserver : <color:#4CAF50>1. Ad Request</color>
+adserver -[#2196F3,thickness=2]-> rtb : <color:#2196F3>2. Bid Request</color>
+rtb -[#FF5722,thickness=2]-> targeting : <color:#FF5722>3. Get Targeted Ads</color>
+targeting -[#9C27B0,thickness=2]-> profiling : <color:#9C27B0>4. Get User Profile</color>
+profiling -[#795548,thickness=2]-> userdb : <color:#795548>5. Fetch/Update Data</color>
+targeting -[#FFC107,thickness=2]-> inventory : <color:#FFC107>6. Check Inventory</color>
+rtb -[#00BCD4,thickness=2]-> adserver : <color:#00BCD4>7. Winning Bid</color>
+adserver -[#E91E63,thickness=2]-> website : <color:#E91E63>8. Serve Ad</color>
+website -[#9E9E9E,thickness=2]-> user : <color:#9E9E9E>9. Display Ad</color>
+user -[#3F51B5,thickness=2]-> adserver : <color:#3F51B5>10. Ad Interaction</color>
+adserver -[#009688,thickness=2]-> analytics : <color:#009688>11. Log Event</color>
+analytics -[#FF9800,thickness=2]-> datalake : <color:#FF9800>12. Store Data</color>
+analytics -[#8BC34A,thickness=2]-> dashboard : <color:#8BC34A>13. Update Metrics</color>
+dashboard -[#607D8B,thickness=2]-> adcompany : <color:#607D8B>14. View Reports</color>
+
+campaign -[#CDDC39,dashed]-> targeting : Configure Targeting
+campaign -[#CDDC39,dashed]-> rtb : Set Bid Strategies
+adcompany -[#CDDC39,dashed]-> campaign : Manage Campaigns
+
+note right of targeting
+  Implements advanced targeting algorithms:
+  - Behavioral targeting
+  - Contextual targeting
+  - Demographic targeting
+  - Retargeting
+end note
+
+note right of analytics
+  Performs:
+  - Click-through rate (CTR) analysis
+  - Conversion tracking
+  - Attribution modeling
+  - A/B testing
+end note
+
+note bottom of datalake
+  Stores:
+  - User behavior data
+  - Ad performance metrics
+  - Campaign data
+  - Conversion data
+end note
+
+note bottom of adserver
+  Optimizations:
+  - Load balancing
+  - Horizontal scaling
+  - Caching popular ads
+  - Database query optimization
+  - CDN for static content
+end note
+
+note bottom of rtb
+  Optimizations:
+  - High-performance computing cluster
+  - Pre-computation and caching
+  - Optimized bidding algorithms
+  - In-memory databases (e.g., Redis)
+end note
+
+note bottom of profiling
+  Optimizations:
+  - Incremental updates
+  - Distributed processing (e.g., Spark)
+  - Optimized data storage (columnar)
+  - Data sharding and parallel processing
+end note
+
+note bottom of targeting
+  Optimizations:
+  - Machine learning models
+  - Online learning and updates
+  - GPU acceleration
+  - Parallel feature engineering
+end note
+
+note bottom of analytics
+  Optimizations:
+  - Stream processing (e.g., Kafka Streams, Flink)
+  - Incremental computation
+  - Distributed computing (e.g., Hadoop, Spark)
+  - Optimized data storage (columnar, e.g., Parquet)
+end note
+
+note bottom of datalake
+  Optimizations:
+  - Data sharding
+  - NoSQL databases (e.g., Cassandra)
+  - Hot/cold data separation
+  - Optimized indexing
+  - Asynchronous writes and batch reads
+end note
+
+@enduml
diff --git a/plantumlmacviewer/.gitignore b/plantumlmacviewer/.gitignore
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/.gitignore
@@ -0,0 +1,4 @@
+go.sum
+plantumlviewerserver.log
+plantuml_viewer_env/
+send_paths.log
diff --git a/plantumlmacviewer/central_app.py b/plantumlmacviewer/central_app.py
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/central_app.py
@@ -0,0 +1,391 @@
+from PyQt5.QtWidgets import (
+    QDesktopWidget,
+    QApplication,
+    QMainWindow,
+    QScrollArea,
+    QLabel,
+    QShortcut,
+    QFileDialog,
+    QSizePolicy,
+)
+
+from PyQt5.QtCore import (
+    Qt,
+    pyqtSignal,
+    QEvent,
+    QCoreApplication,
+)
+from PyQt5.QtGui import QPixmap, QImage, QKeySequence
+from PyQt5.QtCore import Qt, pyqtSignal, QEvent, QCoreApplication
+from watchdog.observers import Observer
+import sys
+import tempfile
+import threading
+import socket
+import os
+from events import OpenWindowEvent
+from file_change_handler import FileChangeHandler
+from logger import setup_logging
+import logging
+import subprocess
+from PyQt5.QtWidgets import QApplication, QMainWindow
+from PyQt5.QtCore import QTimer
+import os
+import glob
+
+# 使用 pyobjc 导入 AppKit
+from AppKit import NSWorkspace, NSApplicationActivateIgnoringOtherApps
+
+setup_logging()
+logging.info("central_app.py 日志系统初始化完成")
+
+
+class CentralApp(QApplication):
+    def __init__(self, argv):
+        logging.debug("开始初始化 CentralApp")
+        super().__init__(argv)
+        self.windows = []
+        self.fileWindowMap = {}
+        self.observers = {}
+        self.socketThread = None
+        logging.debug("CentralApp 初始化完成")
+
+    def start(self):
+        logging.debug("准备启动套接字监听线程")
+        self.socketThread = threading.Thread(target=self.listenToSocket)
+        self.socketThread.daemon = True
+        self.socketThread.start()
+        logging.debug("套接字监听线程启动完成")
+
+    def createNewWindow(self, filePath):
+        logging.debug(f"创建新窗口，文件路径：{filePath}")
+        new_window = UMLViewer(self)
+        self.windows.append(new_window)
+        new_window.show()
+        new_window.raise_()
+        new_window.activateWindow()
+        new_window.loadAndDisplayUML(filePath)
+        self.fileWindowMap[filePath] = new_window
+        self.startFileWatcher(filePath, new_window)
+        return new_window
+
+    def listenToSocket(self):
+        host = "localhost"
+        port = 12345
+
+        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+            s.bind((host, port))
+            s.listen()
+            logging.info(f"Listening on {host}:{port}")
+
+            while True:
+                try:
+                    conn, addr = s.accept()
+                    with conn:
+                        logging.info(f"Connected by {addr}")
+                        data = conn.recv(4096).decode().strip()
+                        file_paths = data.split("\n")
+                        logging.info(f"Received file paths: {file_paths}")
+                        for file_path in file_paths:
+                            QCoreApplication.postEvent(
+                                self, OpenWindowEvent([file_path])
+                            )
+                except Exception as e:
+                    logging.error(f"Socket error: {str(e)}")
+
+    def customEvent(self, event):
+        logging.debug(f"customEvent triggered with event type: {event.type()}")
+        if event.type() == OpenWindowEvent.EVENT_TYPE:
+            for filePath in event.filePaths:
+                try:
+                    self.openOrActivateWindow(filePath)
+                except Exception as e:
+                    logging.error(
+                        f"Error opening or activating window for {filePath}: {str(e)}"
+                    )
+
+    def openOrActivateWindow(self, filePath):
+        logging.debug(f"openOrActivateWindow called with filePath: {filePath}")
+        try:
+            if filePath and not filePath.startswith("fugitive:///"):
+                filePath = os.path.abspath(filePath)
+
+            if filePath in self.fileWindowMap:
+                window = self.fileWindowMap[filePath]
+                logging.info(f"Activating existing window for {filePath}")
+                window.raise_()
+                window.activateWindow()
+                QCoreApplication.processEvents()
+            else:
+                self.createNewWindow(filePath)
+
+        except Exception as e:
+            logging.error(f"Error in openOrActivateWindow: {str(e)}")
+
+    def startFileWatcher(self, filePath, viewer):
+        logging.debug(f"startFileWatcher called with filePath: {filePath}")
+        if not filePath.startswith("fugitive:///"):
+            filePath = os.path.abspath(filePath)
+        directory = os.path.dirname(filePath)
+
+        if directory in self.observers:
+            event_handler = FileChangeHandler(viewer, filePath)
+            self.observers[directory].schedule(
+                event_handler, directory, recursive=False
+            )
+        else:
+            observer = Observer()
+            self.observers[directory] = observer
+            event_handler = FileChangeHandler(viewer, filePath)
+            observer.schedule(event_handler, directory, recursive=False)
+            observer.start()
+
+    def exec_(self):
+        logging.info("Entering main event loop")
+        try:
+            return super().exec_()
+        except Exception as e:
+            logging.critical(f"Unhandled exception in main loop: {str(e)}")
+            return 1
+
+
+class UMLViewer(QMainWindow):
+    focusSignal = pyqtSignal()
+
+    def setFocusToApp(self, appName):
+        try:
+            ws = NSWorkspace.sharedWorkspace()
+            running_apps = ws.runningApplications()
+            for app in running_apps:
+                if app.localizedName() == appName:
+                    app.activateWithOptions_(
+                        NSApplicationActivateIgnoringOtherApps
+                    )
+                    break
+        except Exception as e:
+            logging.error(f"Error setting focus to app {appName}: {str(e)}")
+
+    def __init__(self, centralApp):
+        logging.debug("开始初始化 UMLViewer")
+        super().__init__()
+        self.centralApp = centralApp
+        self.previousApp = None
+        self.initUI()
+        self.focusSignal.connect(self.postFocusProcessing)
+        logging.debug("UMLViewer 初始化完成")
+
+    def initUI(self):
+        logging.debug("开始初始化 UI")
+        self.setWindowTitle("PlantUML Viewer")
+        self.setGeometry(100, 100, 800, 600)
+        logging.debug("UI 初始化完成")
+
+        # 容纳UML图像的滚动区域
+        self.scrollArea = QScrollArea(self)
+        self.setCentralWidget(self.scrollArea)
+
+        # UML图像标签
+        self.imageLabel = QLabel()
+        self.imageLabel.setAlignment(Qt.AlignCenter)  # 居中对齐
+
+        # 设置图像的尺寸策略，使其能够根据可用空间自动调整大小
+        self.imageLabel.setSizePolicy(QSizePolicy.Ignored, QSizePolicy.Ignored)
+        self.imageLabel.setScaledContents(True)  # 允许图像根据标签大小缩放
+        self.scrollArea.setWidget(self.imageLabel)
+        self.scrollArea.setWidgetResizable(True)  # 允许滚动区域适应内容大小
+
+        # 设置快捷键
+        self.setupShortcuts()
+
+        # 将窗口放置在当前显示器上
+        self.move_to_current_screen()
+
+        # 窗口最大化
+        self.showMaximized()
+
+    def move_to_current_screen(self):
+        screen = QApplication.desktop().primaryScreen()
+        rect = QApplication.desktop().screenGeometry(screen)
+        self.setGeometry(rect)
+
+    def setupShortcuts(self):
+        # 打开文件快捷键
+        self.openFileShortcut = QShortcut(QKeySequence("Ctrl+O"), self)
+        self.openFileShortcut.activated.connect(self.openFile)
+        logging.debug("Open file shortcut (Ctrl+O) set up")
+
+        # 关闭窗口快捷键
+        self.closeWindowShortcut = QShortcut(QKeySequence("Ctrl+W"), self)
+        self.closeWindowShortcut.activated.connect(self.close)
+        logging.debug("Close window shortcut (Ctrl+W) set up")
+
+        # 添加复制窗口截图到剪贴板的快捷键（Command+C）
+        self.copyShortcut = QShortcut(QKeySequence("Ctrl+C"), self)
+        self.copyShortcut.activated.connect(self.copyWindowToClipboard)
+        logging.debug("Copy window shortcut (Command+C) set up")
+
+    def openFile(self):
+        # 修改 openFile 方法以支持在新窗口中打开文件
+        filePath, _ = QFileDialog.getOpenFileName(
+            self, "Open file", "", "PlantUML files (*.puml)"
+        )
+        if not filePath.startswith("fugitive:///"):
+            # 确保路径是规范化的
+            filePath = os.path.abspath(filePath)
+            self.centralApp.openNewWindow(filePath)
+
+    def loadAndDisplayUML(self, filePath):
+        temp_dir = None
+        try:
+            self.previousApp = self.getActiveAppName()
+            self.setFocusPolicy(Qt.NoFocus)
+            # 在加载 UML 之前，设置窗口标题为文件名
+            self.setWindowTitle(os.path.basename(filePath))
+            plantuml_jar_path = self.get_plantuml_jar_path()
+
+            temp_dir = tempfile.mkdtemp()
+            base_name = os.path.basename(filePath)
+            png_name = (
+                base_name.replace(".puml", "").replace(" ", "_") + ".png"
+            )
+            temp_png_path = os.path.join(temp_dir, png_name)
+
+            command = [
+                "java",
+                "-jar",
+                plantuml_jar_path,
+                "-tpng",
+                "-o",
+                temp_dir,
+                filePath,
+            ]
+            logging.info(f"Running command: {' '.join(command)}")
+            result = subprocess.run(command, capture_output=True, text=True)
+            logging.info(f"PlantUML return code: {result.returncode}")
+            logging.info(f"PlantUML Output: {result.stdout}")
+            logging.info(f"PlantUML Error Output: {result.stderr}")
+            logging.info(f"Temp directory contents: {os.listdir(temp_dir)}")
+
+            # 查找生成的PNG文件
+            generated_files = [
+                f for f in os.listdir(temp_dir) if f.endswith(".png")
+            ]
+            if not generated_files:
+                raise FileNotFoundError(f"No PNG file generated in {temp_dir}")
+
+            actual_png_path = os.path.join(temp_dir, generated_files[0])
+            logging.info(f"Found generated PNG file: {actual_png_path}")
+
+            if os.path.getsize(actual_png_path) == 0:
+                raise ValueError(
+                    f"Generated PNG file is empty: {actual_png_path}"
+                )
+
+            self.displayImage(actual_png_path)
+
+        except subprocess.CalledProcessError as e:
+            logging.exception(f"Error during subprocess execution: {e}")
+            self.imageLabel.setText(f"Error generating UML diagram: {e}")
+        except FileNotFoundError as e:
+            logging.exception(f"File not found: {e}")
+            self.imageLabel.setText(f"Error: {e}")
+        except ValueError as e:
+            logging.exception(f"Invalid file: {e}")
+            self.imageLabel.setText(f"Error: {e}")
+        except Exception as e:
+            logging.exception(f"Unexpected error: {e}")
+            self.imageLabel.setText(f"An unexpected error occurred: {e}")
+        finally:
+            self.focusSignal.emit()
+            if temp_dir and os.path.exists(temp_dir):
+                try:
+                    import shutil
+
+                    shutil.rmtree(temp_dir)
+                except Exception as e:
+                    logging.error(f"Error removing temp dir: {e}")
+
+    def get_plantuml_jar_path(self):
+        plantuml_base_path = "/usr/local/Cellar/plantuml/"
+        latest_version_path = max(
+            glob.glob(os.path.join(plantuml_base_path, "*/")),
+            key=os.path.getmtime,
+        )
+        return os.path.join(latest_version_path, "libexec/plantuml.jar")
+
+    def displayImage(self, imagePath):
+        logging.info(f"Loading PNG file: {imagePath}")
+        self.imageLabel.clear()  # 清空现有图像
+        image = QImage(imagePath)
+        if not image.isNull():
+            pixmap = QPixmap.fromImage(image)
+            self.imageLabel.setPixmap(pixmap)
+            logging.info("Image updated successfully.")
+        else:
+            error_msg = f"Failed to load the generated image: {imagePath}"
+            logging.error(error_msg)
+            self.imageLabel.setText(error_msg)
+
+    def postFocusProcessing(self):
+        try:
+            self.raise_()
+            self.activateWindow()
+            QApplication.processEvents()
+            if self.previousApp:
+                self.setFocusToApp(self.previousApp)
+            self.previousApp = None
+        except Exception as e:
+            logging.error(f"Error in postFocusProcessing: {str(e)}")
+
+    def getActiveAppName(self):
+        ws = NSWorkspace.sharedWorkspace()
+        frontmostApp = ws.frontmostApplication()
+        return frontmostApp.localizedName()
+
+    def copyWindowToClipboard(self):
+        try:
+            logging.info("开始复制窗口截图到剪贴板")
+            # 截取当前窗口的截图
+            pixmap = self.grab()
+            if pixmap.isNull():
+                logging.error("截图失败，pixmap 为空")
+                return
+
+            # 获取系统剪贴板
+            clipboard = QApplication.clipboard()
+            # 将截图复制到剪贴板
+            clipboard.setPixmap(pixmap, mode=clipboard.Clipboard)
+            logging.info("窗口截图已复制到剪贴板")
+        except Exception as e:
+            logging.exception(f"复制窗口截图到剪贴板时发生异常: {e}")
+
+    def closeEvent(self, event):
+        logging.debug("Close event triggered")
+        # Remove this window from the centralApp's windows list
+        if self in self.centralApp.windows:
+            self.centralApp.windows.remove(self)
+
+        # Remove this window from the fileWindowMap
+        for file_path, window in list(self.centralApp.fileWindowMap.items()):
+            if window == self:
+                del self.centralApp.fileWindowMap[file_path]
+
+        # Accept the close event
+        event.accept()
+
+
+def main():
+    logging.info("central_app.py 开始执行")
+    try:
+        app = CentralApp(sys.argv)
+        app.start()  # 只启动套接字监听线程，不创建初始窗口
+        return app.exec_()
+    except Exception as e:
+        logging.exception(f"发生未处理的异常: {str(e)}")
+        return 1
+    finally:
+        logging.info("central_app.py 执行结束")
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/plantumlmacviewer/events.py b/plantumlmacviewer/events.py
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/events.py
@@ -0,0 +1,9 @@
+from PyQt5.QtCore import QEvent
+
+
+class OpenWindowEvent(QEvent):
+    EVENT_TYPE = QEvent.Type(QEvent.registerEventType())
+
+    def __init__(self, filePaths):
+        super().__init__(OpenWindowEvent.EVENT_TYPE)
+        self.filePaths = filePaths  # filePaths 现在是一个列表
diff --git a/plantumlmacviewer/file_change_handler.py b/plantumlmacviewer/file_change_handler.py
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/file_change_handler.py
@@ -0,0 +1,12 @@
+from watchdog.events import FileSystemEventHandler
+
+
+class FileChangeHandler(FileSystemEventHandler):
+    def __init__(self, viewer, filePath):
+        super().__init__()
+        self.viewer = viewer
+        self.filePath = filePath
+
+    def on_modified(self, event):
+        if event.src_path == self.filePath:
+            self.viewer.loadAndDisplayUML(self.filePath)
diff --git a/plantumlmacviewer/go.mod b/plantumlmacviewer/go.mod
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/go.mod
@@ -0,0 +1,35 @@
+module myGoProject
+
+go 1.21.4
+
+require fyne.io/fyne/v2 v2.4.1
+
+require (
+	fyne.io/systray v1.10.1-0.20230722100817-88df1e0ffa9a // indirect
+	github.com/davecgh/go-spew v1.1.1 // indirect
+	github.com/fredbi/uri v1.0.0 // indirect
+	github.com/fsnotify/fsnotify v1.6.0 // indirect
+	github.com/fyne-io/gl-js v0.0.0-20220119005834-d2da28d9ccfe // indirect
+	github.com/fyne-io/glfw-js v0.0.0-20220120001248-ee7290d23504 // indirect
+	github.com/fyne-io/image v0.0.0-20220602074514-4956b0afb3d2 // indirect
+	github.com/go-gl/gl v0.0.0-20211210172815-726fda9656d6 // indirect
+	github.com/go-gl/glfw/v3.3/glfw v0.0.0-20221017161538-93cebf72946b // indirect
+	github.com/go-text/render v0.0.0-20230619120952-35bccb6164b8 // indirect
+	github.com/go-text/typesetting v0.0.0-20230616162802-9c17dd34aa4a // indirect
+	github.com/godbus/dbus/v5 v5.1.0 // indirect
+	github.com/gopherjs/gopherjs v1.17.2 // indirect
+	github.com/jsummers/gobmp v0.0.0-20151104160322-e2ba15ffa76e // indirect
+	github.com/pmezard/go-difflib v1.0.0 // indirect
+	github.com/srwiley/oksvg v0.0.0-20221011165216-be6e8873101c // indirect
+	github.com/srwiley/rasterx v0.0.0-20220730225603-2ab79fcdd4ef // indirect
+	github.com/stretchr/testify v1.8.4 // indirect
+	github.com/tevino/abool v1.2.0 // indirect
+	github.com/yuin/goldmark v1.5.5 // indirect
+	golang.org/x/image v0.11.0 // indirect
+	golang.org/x/mobile v0.0.0-20230531173138-3c911d8e3eda // indirect
+	golang.org/x/net v0.14.0 // indirect
+	golang.org/x/sys v0.11.0 // indirect
+	golang.org/x/text v0.12.0 // indirect
+	gopkg.in/yaml.v3 v3.0.1 // indirect
+	honnef.co/go/js/dom v0.0.0-20210725211120-f030747120f2 // indirect
+)
diff --git a/plantumlmacviewer/logger.py b/plantumlmacviewer/logger.py
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/logger.py
@@ -0,0 +1,18 @@
+import logging
+import os
+
+
+def setup_logging():
+    # 获取当前文件的绝对路径
+    current_file_path = os.path.abspath(__file__)
+    # 获取当前文件所在的目录
+    current_dir = os.path.dirname(current_file_path)
+    # 创建日志文件路径，位于当前目录下
+    log_file = os.path.join(current_dir, "plantumlviewerserver.log")
+
+    logging.basicConfig(
+        filename=log_file,
+        level=logging.DEBUG,  # 将日志级别设置为DEBUG以捕获所有日志
+        format="%(asctime)s %(levelname)s:%(message)s",
+        filemode="a",  # 追加模式
+    )
diff --git a/plantumlmacviewer/main.py b/plantumlmacviewer/main.py
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/main.py
@@ -0,0 +1,166 @@
+import os
+import sys
+import venv
+import time
+import subprocess
+from logger import setup_logging
+import logging
+import platform
+
+setup_logging()
+
+
+def get_python_executable():
+    """获取合适的Python可执行文件路径"""
+    # 首先尝试使用系统Python
+    system_python = "/usr/bin/python3"
+    if os.path.exists(system_python):
+        return system_python
+
+    # 如果系统Python不存在，尝试使用当前运行的Python
+    return sys.executable
+
+
+def check_venv_health(venv_path):
+    """检查虚拟环境是否健康"""
+    try:
+        python_path = get_venv_python(venv_path)
+        if not os.path.exists(python_path):
+            return False, "Python解释器不存在"
+
+        # 使用pip list检查包是否已安装
+        pip_path = os.path.join(os.path.dirname(python_path), "pip")
+        result = subprocess.run(
+            [pip_path, "list"],
+            capture_output=True,
+            text=True,
+        )
+        
+        if result.returncode != 0:
+            return False, f"无法获取已安装的包列表: {result.stderr}"
+            
+        installed_packages = result.stdout.lower()
+        required_packages = ['pyqt5', 'watchdog', 'pyobjc']
+        
+        for package in required_packages:
+            if package not in installed_packages:
+                return False, f"包 {package} 未安装"
+
+        return True, "虚拟环境正常"
+    except Exception as e:
+        return False, f"检查失败: {str(e)}"
+
+
+def create_or_get_venv(venv_name):
+    """创建或获取虚拟环境，只在必要时重新创建"""
+    current_dir = os.path.dirname(os.path.abspath(__file__))
+    venv_path = os.path.join(current_dir, venv_name)
+
+    if os.path.exists(venv_path):
+        # 检查现有环境是否健康
+        is_healthy, message = check_venv_health(venv_path)
+        if is_healthy:
+            logging.info("使用现有虚拟环境")
+            return venv_path, False
+        else:
+            logging.warning(f"虚拟环境不健康: {message}，将重新创建")
+            # 在删除之前确保没有进程在使用虚拟环境
+            if platform.system() == "Windows":
+                os.system(f'taskkill /F /IM python.exe')
+            else:
+                os.system('pkill -f "python3.*plantuml_viewer_env"')
+            import shutil
+            shutil.rmtree(venv_path)
+
+    logging.info(f"正在创建虚拟环境: {venv_name}")
+    try:
+        python_executable = get_python_executable()
+        subprocess.check_call([python_executable, "-m", "venv", venv_path])
+        return venv_path, True
+    except subprocess.CalledProcessError as e:
+        logging.error(f"创建虚拟环境失败: {str(e)}")
+        raise
+
+
+def get_venv_python(venv_path):
+    if platform.system() == "Windows":
+        return os.path.join(venv_path, "Scripts", "python.exe")
+    else:
+        return os.path.join(venv_path, "bin", "python3")
+
+
+def install_requirements(venv_python):
+    current_dir = os.path.dirname(os.path.abspath(__file__))
+    requirements_path = os.path.join(current_dir, "requirements.txt")
+    if os.path.exists(requirements_path):
+        logging.info("正在安装依赖包...")
+        try:
+            # 先升级pip
+            subprocess.check_call(
+                [venv_python, "-m", "pip", "install", "--upgrade", "pip"]
+            )
+            # 安装依赖
+            subprocess.check_call(
+                [venv_python, "-m", "pip", "install", "-r", requirements_path]
+            )
+        except subprocess.CalledProcessError as e:
+            logging.error(f"安装依赖失败: {str(e)}")
+            raise
+    else:
+        logging.warning("未找到requirements.txt文件。跳过包安装。")
+
+
+def main():
+    logging.info("开始执行main函数")
+    try:
+        venv_name = "plantuml_viewer_env"
+        current_dir = os.path.dirname(os.path.abspath(__file__))
+
+        venv_path, is_new_venv = create_or_get_venv(venv_name)
+        venv_python = get_venv_python(venv_path)
+
+        if is_new_venv:
+            install_requirements(venv_python)
+
+        central_app_path = os.path.join(current_dir, "central_app.py")
+        logging.info(f"准备运行central_app.py，路径：{central_app_path}")
+
+        env = os.environ.copy()
+        if platform.system() == "Windows":
+            site_packages = os.path.join(venv_path, "Lib", "site-packages")
+        else:
+            python_version = (
+                f"{sys.version_info.major}.{sys.version_info.minor}"
+            )
+            site_packages = os.path.join(
+                venv_path, "lib", f"python{python_version}", "site-packages"
+            )
+
+        env["PYTHONPATH"] = f"{site_packages}:{env.get('PYTHONPATH', '')}"
+
+        process = subprocess.Popen(
+            [venv_python, central_app_path],
+            env=env,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            universal_newlines=True,
+        )
+
+        time.sleep(2)
+        if process.poll() is not None:
+            stdout, stderr = process.communicate()
+            logging.error(f"central_app.py 启动失败")
+            logging.error(f"标准输出：{stdout}")
+            logging.error(f"错误输出：{stderr}")
+            return 1
+        else:
+            logging.info("central_app.py 成功启动")
+            return 0
+
+    except Exception as e:
+        logging.exception(f"main函数中发生异常: {str(e)}")
+        return 1
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/plantumlmacviewer/readme b/plantumlmacviewer/readme
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/readme
@@ -0,0 +1,9 @@
+上面是我的代码.
+他的主要功能是呈现一个或多个puml文件, 每个窗口呈现一个puml文件, 并在文件有修改时重新加载窗口.
+我现在想再加一个功能,
+我是用git来对puml文件进行版本控制, 我想比对修改前后所生成的图像. 
+我想做的功能是, 通过vim script来定义一个命令, 他会先发送一个命令给上面的python代码, 关闭所有其他窗口.
+然后再把当前文件用类似的命令:  git show HEAD:"$1" > "$1".bak 来生成一个临时文件, 
+然后把当前文件, 和临时文件, 两个发给上面的python代码, 让他来做呈现.
+生成需要修改的文件
+回答用中文.
diff --git a/plantumlmacviewer/requirements.txt b/plantumlmacviewer/requirements.txt
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/requirements.txt
@@ -0,0 +1,3 @@
+PyQt5
+watchdog
+pyobjc
diff --git a/plantumlmacviewer/run_tests.sh b/plantumlmacviewer/run_tests.sh
new file mode 100755
--- /dev/null
+++ ./plantumlmacviewer/run_tests.sh
@@ -0,0 +1,7 @@
+#!/bin/bash
+
+# 确保tests目录存在
+mkdir -p tests
+
+# 运行所有测试
+python3 -m unittest discover -s tests -p "test_*.py" -v 
\ No newline at end of file
diff --git a/plantumlmacviewer/tests/test_main.py b/plantumlmacviewer/tests/test_main.py
new file mode 100644
--- /dev/null
+++ ./plantumlmacviewer/tests/test_main.py
@@ -0,0 +1,105 @@
+import os
+import sys
+import unittest
+import shutil
+import tempfile
+from unittest.mock import patch, MagicMock
+
+# 添加父目录到Python路径以导入main模块
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
+import main
+
+class TestVirtualEnvironment(unittest.TestCase):
+    def setUp(self):
+        # 创建临时目录用于测试
+        self.test_dir = tempfile.mkdtemp()
+        self.venv_name = "test_venv"
+        self.venv_path = os.path.join(self.test_dir, self.venv_name)
+
+    def tearDown(self):
+        # 清理临时目录
+        shutil.rmtree(self.test_dir)
+
+    def test_get_python_executable(self):
+        # 测试获取Python可执行文件路径
+        python_path = main.get_python_executable()
+        self.assertTrue(os.path.exists(python_path))
+        self.assertTrue('python' in python_path.lower())
+
+    @patch('subprocess.run')
+    def test_check_venv_health_healthy(self, mock_run):
+        # 模拟健康的虚拟环境
+        mock_run.return_value = MagicMock(returncode=0)
+        os.makedirs(os.path.dirname(main.get_venv_python(self.venv_path)), exist_ok=True)
+        
+        is_healthy, message = main.check_venv_health(self.venv_path)
+        self.assertTrue(is_healthy)
+        self.assertEqual(message, "虚拟环境正常")
+
+    @patch('subprocess.run')
+    def test_check_venv_health_unhealthy(self, mock_run):
+        # 模拟不健康的虚拟环境
+        mock_run.return_value = MagicMock(returncode=1, stderr="Import Error")
+        os.makedirs(os.path.dirname(main.get_venv_python(self.venv_path)), exist_ok=True)
+        
+        is_healthy, message = main.check_venv_health(self.venv_path)
+        self.assertFalse(is_healthy)
+        self.assertTrue("依赖包导入失败" in message)
+
+    @patch('subprocess.check_call')
+    @patch('main.check_venv_health')
+    def test_create_or_get_venv_new(self, mock_check_health, mock_check_call):
+        # 测试创建新的虚拟环境
+        venv_path, is_new = main.create_or_get_venv(self.venv_name)
+        self.assertTrue(is_new)
+        mock_check_call.assert_called_once()
+
+    @patch('subprocess.check_call')
+    @patch('main.check_venv_health')
+    def test_create_or_get_venv_existing_healthy(self, mock_check_health, mock_check_call):
+        # 测试使用现有的健康虚拟环境
+        os.makedirs(self.venv_path)
+        mock_check_health.return_value = (True, "虚拟环境正常")
+        
+        venv_path, is_new = main.create_or_get_venv(self.venv_name)
+        self.assertFalse(is_new)
+        mock_check_call.assert_not_called()
+
+    @patch('subprocess.check_call')
+    @patch('main.check_venv_health')
+    def test_create_or_get_venv_existing_unhealthy(self, mock_check_health, mock_check_call):
+        # 测试重新创建不健康的虚拟环境
+        os.makedirs(self.venv_path)
+        mock_check_health.return_value = (False, "依赖包导入失败")
+        
+        venv_path, is_new = main.create_or_get_venv(self.venv_name)
+        self.assertTrue(is_new)
+        mock_check_call.assert_called_once()
+
+    @patch('subprocess.Popen')
+    @patch('main.create_or_get_venv')
+    def test_main_success(self, mock_create_venv, mock_popen):
+        # 测试主函数成功执行
+        mock_create_venv.return_value = (self.venv_path, False)
+        mock_process = MagicMock()
+        mock_process.poll.return_value = None
+        mock_popen.return_value = mock_process
+        
+        result = main.main()
+        self.assertEqual(result, 0)
+
+    @patch('subprocess.Popen')
+    @patch('main.create_or_get_venv')
+    def test_main_failure(self, mock_create_venv, mock_popen):
+        # 测试主函数失败情况
+        mock_create_venv.return_value = (self.venv_path, False)
+        mock_process = MagicMock()
+        mock_process.poll.return_value = 1
+        mock_process.communicate.return_value = ("", "Error")
+        mock_popen.return_value = mock_process
+        
+        result = main.main()
+        self.assertEqual(result, 1)
+
+if __name__ == '__main__':
+    unittest.main() 
\ No newline at end of file
diff --git a/rbac_abac_system_with_auditing_and_fine_grained_access_control.puml b/rbac_abac_system_with_auditing_and_fine_grained_access_control.puml
new file mode 100644
--- /dev/null
+++ ./rbac_abac_system_with_auditing_and_fine_grained_access_control.puml
@@ -0,0 +1,80 @@
+@startuml RBAC ABAC System with Auditing and Fine-Grained Access Control
+
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+
+skinparam backgroundColor #CCE8CF
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 20
+skinparam shadowing false
+
+title RBAC/ABAC System with Auditing and Fine-Grained Access Control
+
+rectangle "User Interface" as UI PRIMARY_COLOR
+
+rectangle "API Gateway" as APIGateway SECONDARY_COLOR {
+    component "Authentication" as Auth
+    component "Request Routing" as Routing
+}
+
+rectangle "Identity and Access Management" as IAM TERTIARY_COLOR {
+    component "User Management" as UserMgmt
+    component "Role Management" as RoleMgmt
+    component "Attribute Management" as AttrMgmt
+    component "Policy Engine" as PolicyEngine
+}
+
+rectangle "Resource Access Layer" as RAL QUATERNARY_COLOR {
+    component "Access Enforcement" as AccessEnforce
+    component "Resource Manager" as ResMgr
+}
+
+database "User Store" as UserDB PRIMARY_COLOR
+database "Role Store" as RoleDB SECONDARY_COLOR
+database "Attribute Store" as AttrDB TERTIARY_COLOR
+database "Policy Store" as PolicyDB QUATERNARY_COLOR
+
+rectangle "Audit System" as AuditSystem QUINARY_COLOR {
+    component "Audit Logger" as AuditLogger
+    component "Audit Analyzer" as AuditAnalyzer
+}
+
+database "Audit Logs" as AuditDB QUINARY_COLOR
+
+UI -[PRIMARY_COLOR,thickness=2]down-> APIGateway : <back:#FFFFFF><color:PRIMARY_COLOR>1. User Request</color></back>
+APIGateway -[SECONDARY_COLOR,thickness=2]down-> IAM : <back:#FFFFFF><color:SECONDARY_COLOR>2. Authenticate & Authorize</color></back>
+IAM -[TERTIARY_COLOR,thickness=2]right-> UserDB : <back:#FFFFFF><color:TERTIARY_COLOR>3. Verify User</color></back>
+IAM -[QUATERNARY_COLOR,thickness=2]right-> RoleDB : <back:#FFFFFF><color:QUATERNARY_COLOR>4. Get Roles</color></back>
+IAM -[QUINARY_COLOR,thickness=2]right-> AttrDB : <back:#FFFFFF><color:QUINARY_COLOR>5. Get Attributes</color></back>
+IAM -[PRIMARY_COLOR,thickness=2]down-> PolicyDB : <back:#FFFFFF><color:PRIMARY_COLOR>6. Evaluate Policies</color></back>
+IAM -[SECONDARY_COLOR,thickness=2]down-> RAL : <back:#FFFFFF><color:SECONDARY_COLOR>7. Access Decision</color></back>
+RAL -[TERTIARY_COLOR,thickness=2]down-> ResMgr : <back:#FFFFFF><color:TERTIARY_COLOR>8. Access Resource</color></back>
+APIGateway -[QUATERNARY_COLOR,thickness=2]left-> AuditSystem : <back:#FFFFFF><color:QUATERNARY_COLOR>9. Log Activity</color></back>
+AuditSystem -[QUINARY_COLOR,thickness=2]down-> AuditDB : <back:#FFFFFF><color:QUINARY_COLOR>10. Store Audit Log</color></back>
+
+note top of IAM
+  Combines RBAC and ABAC:
+  - Role-based for coarse-grained control
+  - Attribute-based for fine-grained control
+end note
+
+note bottom of PolicyEngine
+  Evaluates complex policies based on:
+  - User roles
+  - User attributes
+  - Resource attributes
+  - Environmental factors (time, location, etc.)
+end note
+
+note bottom of AuditSystem
+  - Logs all access attempts (successful and failed)
+  - Provides real-time alerts for suspicious activities
+  - Supports compliance reporting and forensic analysis
+end note
+
+@enduml
diff --git a/readme b/readme
new file mode 100644
--- /dev/null
+++ ./readme
@@ -0,0 +1,106 @@
+用中文回答
+使用PlantUML绘制系统架构图
+生成之后, 帮我验证一下生成的puml文件有没有语法错误, 能不能正确render.
+
+系统架构图
+    在当前项目下查找, 看看有没有接近的已有设计
+    回答用中文
+    不用画图, 先用文字给我讲一讲,
+    这个系统应该如何设计, 有哪些考虑要点？
+    不用画图, 先用文字给我讲一讲,
+    如何监测系统, 比方说系统反馈速度等等
+    puml里面用英文
+    使用PUML，画系统架构图，使用allowmixing
+    考虑高并发
+    识别性能瓶颈，添加新组件，调整连接
+    性能优化、瓶颈识别和扩展性
+    对系统进行优化和改进，通过note说明如何优化
+    再详细些
+    组件之间没有连接, 
+    字体太小
+    字体优化美化一下,
+    布局优化美化一下, 
+    充分利用空间.
+    减少线条和文字交叉重叠
+    给连线加上序号
+    使用相同的颜色来标识连线及其对应的文字
+    note的字体太难看
+    还有很多空白区域
+    使用note来注释数据库选型：列举关系型和NoSQL数据库，说明特点、应用场景及混合使用策略
+
+    解决截图里的错误
+    给我讲解这个设计
+    解决所有的类似错误
+    你没有做任何修改
+    重新设计
+    开启canvas功能
+
+    布局优化美观：
+        不使用白色背景（避免眼睛疲劳）
+        字体不要与背景同色，排列整齐
+    代码只使用英文
+    性能优化、瓶颈识别和扩展性
+    使用skinparam linetype ortho创建直角连接线，减少交叉
+    连线的文字尽量合并成一行
+    需求分析：
+        读写比例，性能分析及优化
+        估算性能和存储空间需求
+    高并发处理方案
+    基于原图做简单修改，不要大改
+    讨论系统的可扩展性策略（水平扩展vs垂直扩展）
+    考虑系统的容错性和灾难恢复策略
+    讨论数据一致性模型（强一致性vs最终一致性）
+    考虑认证和授权机制
+    讨论系统监控和日志策略
+
+数据库schema要求
+    使用PUML，画数据库schema，使用allowmixing
+    回答用中文
+    puml里面用英文
+    描述表结构，考虑分片分表以应对大系统需求
+    设计schema，包括普通数据表和NoSQL数据表
+    表之间的相互关系像一对一, 一对多, 多对一,多对多要表达出来
+    普通数据表：标注FK, PK, index, sort
+    NoSQL数据表：标注sharding key(s)
+    对于Redis，要把键表达出来
+    优化和改进，通过note说明优化点
+    性能优化、瓶颈识别和扩展性
+    注释的字体太难看
+
+
+    解决截图里的错误
+
+    识别性能瓶颈，添加新组件，调整连接
+    布局优化美观：
+        不使用白色背景（避免眼睛疲劳）
+        充分利用空间，减少线条和文字重叠
+        字体不要与背景同色，排列整齐
+    代码只使用英文
+    基于原图做简单修改，不要大改
+    讨论数据库选型（关系型vs非关系型）的理由
+    考虑数据备份和恢复策略
+    讨论数据库索引策略
+    考虑数据库连接池的设计
+
+系统设计通用问题
+    讨论系统的可用性（SLA）要求
+    考虑系统的安全性设计
+    讨论系统的国际化和本地化策略
+    考虑系统的API设计（RESTful, GraphQL等）
+    讨论缓存策略（客户端缓存、CDN、应用层缓存、数据库缓存）
+    考虑负载均衡策略
+    讨论异步处理和消息队列的使用
+    考虑微服务架构（如果适用）
+
+其他要求
+    识别图片中的内容
+    将账户密码替换为字符串谜题并重新描述
+
+注意事项
+    用来面试，要关注性能优化、瓶颈、扩展
+    不要写任何的代码
+    如果没有提问，就默认按照上述要求进行优化
+    生成优化后的新文件
+    准备讨论系统设计的权衡（trade-offs）
+    考虑系统的未来扩展性
+    准备讨论技术选型的理由
diff --git a/real_time_audio_video_communication_system_architecture_with_webrtc_and_media_servers.puml b/real_time_audio_video_communication_system_architecture_with_webrtc_and_media_servers.puml
new file mode 100644
--- /dev/null
+++ ./real_time_audio_video_communication_system_architecture_with_webrtc_and_media_servers.puml
@@ -0,0 +1,108 @@
+@startuml
+!pragma layout dot
+allowmixing
+
+' 定义更深的颜色
+!define PRIMARY_COLOR #E67E22
+!define SECONDARY_COLOR #3498DB
+!define TERTIARY_COLOR #F1C40F
+!define QUATERNARY_COLOR #8E44AD
+!define QUINARY_COLOR #16A085
+!define P2P_COLOR #27AE60
+
+' 背景颜色
+skinparam backgroundColor #CCE8CF
+
+rectangle "Client Devices" as ClientDevices PRIMARY_COLOR {
+    component "Web Browser" as WebBrowser
+    component "Mobile App" as MobileApp
+    component "Desktop App" as DesktopApp
+}
+
+rectangle "Frontend Services" as FrontendServices SECONDARY_COLOR {
+    component "User Interface" as UI
+    component "WebRTC Client" as WebRTCClient
+    component "Media Capture" as MediaCapture
+}
+
+rectangle "Signaling Server" as SignalingServer TERTIARY_COLOR {
+    component "WebSocket Handler" as WebSocketHandler
+    component "Session Management" as SessionManagement
+    component "Presence Service" as PresenceService
+}
+
+rectangle "Media Server" as MediaServer QUATERNARY_COLOR {
+    component "SFU (Selective Forwarding Unit)" as SFU
+    component "MCU (Multipoint Control Unit)" as MCU
+    component "Media Processing" as MediaProcessing
+}
+
+rectangle "Backend Services" as BackendServices QUINARY_COLOR {
+    component "User Authentication" as UserAuth
+    component "Call Management" as CallManagement
+    component "Analytics Service" as AnalyticsService
+}
+
+database "Database" as DB {
+    component "User Profiles" as UserProfiles
+    component "Call Logs" as CallLogs
+    component "Analytics Data" as AnalyticsData
+}
+
+cloud "TURN/STUN Servers" as TURNSTUNServers {
+    component "TURN Server" as TURNServer
+    component "STUN Server" as STUNServer
+}
+
+' 添加第二个客户端设备以表示P2P连接
+rectangle "Client Devices (Peer)" as ClientDevicesPeer PRIMARY_COLOR {
+    component "Peer Device" as PeerDevice
+}
+
+ClientDevices -[PRIMARY_COLOR,thickness=2]-> FrontendServices : <back:#FFFFFF><color:PRIMARY_COLOR>1. Access</color></back>
+FrontendServices -[SECONDARY_COLOR,thickness=2]-> SignalingServer : <back:#FFFFFF><color:SECONDARY_COLOR>2. Establish connection</color></back>
+SignalingServer -[TERTIARY_COLOR,thickness=2]-> BackendServices : <back:#FFFFFF><color:TERTIARY_COLOR>3. Authenticate & manage calls</color></back>
+BackendServices -[QUINARY_COLOR,thickness=2]-> DB : <back:#FFFFFF><color:QUINARY_COLOR>4. Store/retrieve data</color></back>
+FrontendServices -[SECONDARY_COLOR,thickness=2]-> MediaServer : <back:#FFFFFF><color:SECONDARY_COLOR>5. Media transmission</color></back>
+MediaServer -[QUATERNARY_COLOR,thickness=2]-> TURNSTUNServers : <back:#FFFFFF><color:QUATERNARY_COLOR>6. NAT traversal</color></back>
+SignalingServer -[TERTIARY_COLOR,thickness=2]-> MediaServer : <back:#FFFFFF><color:TERTIARY_COLOR>7. Coordinate media flow</color></back>
+
+' 添加P2P连接
+ClientDevices <-[P2P_COLOR,thickness=2]-> ClientDevicesPeer : <back:#FFFFFF><color:P2P_COLOR>8. P2P Communication</color></back>
+
+note right of ClientDevices
+  Supports various client types
+  for wide accessibility
+end note
+
+note bottom of FrontendServices
+  Handles user interactions and
+  media capture/rendering
+end note
+
+note right of SignalingServer
+  Manages real-time communication
+  setup and tear-down
+end note
+
+note right of MediaServer
+  Optimizes media routing and
+  processing for scalability
+end note
+
+note bottom of BackendServices
+  Manages user accounts, call
+  records, and system analytics
+end note
+
+note bottom of TURNSTUNServers
+  Facilitates peer connections
+  through firewalls and NATs
+end note
+
+note bottom of ClientDevices
+  Direct P2P connection established
+  after initial signaling
+end note
+
+@enduml
diff --git a/real_time_data_analytics_platform_with_stream_processing.puml b/real_time_data_analytics_platform_with_stream_processing.puml
new file mode 100644
--- /dev/null
+++ ./real_time_data_analytics_platform_with_stream_processing.puml
@@ -0,0 +1,93 @@
+@startuml
+skinparam componentStyle rectangle
+
+package "Real-time Data Analytics Platform" {
+    package "Data Ingestion Layer" {
+        component "Kafka Cluster" as Kafka
+        component "Event Collector" as Collector
+        component "Schema Registry" as SchemaRegistry
+    }
+    
+    package "Stream Processing" {
+        component "Apache Flink" as Flink
+        component "Stream Processor" as Processor
+        component "State Manager" as StateManager
+        component "Window Manager" as WindowManager
+    }
+    
+    package "Storage Layer" {
+        component "Time Series DB" as TSDB
+        component "Data Lake" as DataLake
+        component "Hot Storage" as HotStorage
+        component "Cold Storage" as ColdStorage
+    }
+    
+    package "Analytics Engine" {
+        component "Real-time Analytics" as RealTimeAnalytics
+        component "ML Pipeline" as MLPipeline
+        component "Query Engine" as QueryEngine
+    }
+    
+    package "Serving Layer" {
+        component "API Gateway" as Gateway
+        component "Cache Layer" as Cache
+        component "Query Router" as Router
+    }
+    
+    package "Monitoring & Alerts" {
+        component "Metrics Collector" as Metrics
+        component "Alert Manager" as Alerts
+        component "Dashboard" as Dashboard
+    }
+}
+
+cloud "Data Sources" as Sources
+cloud "Client Applications" as Clients
+
+Sources --> Collector : Raw Data
+Collector --> Kafka : Process Events
+Kafka --> TSDB : Store Results
+RealTimeAnalytics --> TSDB : Query Data
+Gateway --> RealTimeAnalytics : Serve Results
+Metrics --> Flink : Track Metrics
+
+Kafka --> SchemaRegistry : Validate
+Flink --> StateManager : Manage State
+Flink --> WindowManager : Window Ops
+
+note right of Collector
+  High throughput data collection
+  Schema validation
+end note
+
+note right of Flink
+  Real-time processing
+  Stateful operations
+end note
+
+legend right
+Implementation Details:
+==
+Stream Processing:
+- Exactly-once semantics
+- Watermark handling
+- Checkpointing
+- State backends
+
+Storage Strategy:
+- Hot/warm/cold tiers
+- Data lifecycle
+- Compression policies
+
+Performance Features:
+- Adaptive batching
+- Query optimization
+- Cache management
+
+Scalability:
+- Horizontal scaling
+- Partition management
+- Load balancing
+end legend
+
+@enduml
\ No newline at end of file
diff --git a/real_time_data_pipeline_with_ingestion_processing_analysis_and_visualization.puml b/real_time_data_pipeline_with_ingestion_processing_analysis_and_visualization.puml
new file mode 100644
--- /dev/null
+++ ./real_time_data_pipeline_with_ingestion_processing_analysis_and_visualization.puml
@@ -0,0 +1,135 @@
+@startuml real_time_data_analysis_platform_architecture
+
+!pragma layout dot
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 18
+skinparam noteFontSize 18
+skinparam arrowFontSize 18
+
+rectangle "Data Sources" as DS #E1F5FE {
+    component "IoT Devices" as IOT
+    component "Web Applications" as WA
+    component "Mobile Apps" as MA
+    component "Databases" as DB
+}
+
+rectangle "Data Ingestion Layer" as DIL #B3E5FC {
+    queue "Apache Kafka" as AK
+    component "Kafka Connect" as KC
+    component "Flume Agents" as FA
+}
+
+rectangle "Stream Processing Layer" as SPL #81D4FA {
+    component "Apache Flink" as AF
+    component "Spark Streaming" as SS
+    component "Apache Storm" as AS
+}
+
+rectangle "Real-time Storage" as RTS #4FC3F7 {
+    database "Apache Cassandra" as AC
+    database "Apache HBase" as AH
+    database "Redis" as RD
+}
+
+rectangle "Batch Processing Layer" as BPL #03A9F4 {
+    component "Apache Hadoop" as AHD
+    component "Apache Spark" as ASP
+}
+
+rectangle "Data Warehouse" as DW #0288D1 {
+    database "Amazon Redshift" as AR
+    database "Google BigQuery" as GB
+}
+
+rectangle "Analytics Engine" as AE #01579B {
+    component "Apache Druid" as AD
+    component "Apache Pinot" as AP
+}
+
+rectangle "Visualization Layer" as VL #80DEEA {
+    component "Grafana" as GF
+    component "Apache Superset" as ASS
+    component "Tableau" as TB
+}
+
+rectangle "Machine Learning" as ML #26C6DA {
+    component "TensorFlow" as TF
+    component "Scikit-learn" as SKL
+}
+
+rectangle "Metadata Management" as MM #00BCD4 {
+    component "Apache Atlas" as AA
+    database "Metadata Store" as MS
+}
+
+rectangle "Monitoring & Alerting" as MA #00ACC1 {
+    component "Prometheus" as PR
+    component "Alertmanager" as AM
+    component "ELK Stack" as ELK
+}
+
+actor "Data Analyst" as DA
+actor "Business User" as BU
+
+DS -[#FF5722,thickness=2]-> DIL : <back:#FFFFFF><color:#FF5722>1. Ingest Data</color></back>
+DIL -[#FF5722,thickness=2]-> SPL : <back:#FFFFFF><color:#FF5722>2. Stream Data</color></back>
+SPL -[#4CAF50,thickness=2]-> RTS : <back:#FFFFFF><color:#4CAF50>3. Store Real-time Data</color></back>
+SPL -[#4CAF50,thickness=2]-> AE : <back:#FFFFFF><color:#4CAF50>4. Real-time Analytics</color></back>
+DIL -[#2196F3,thickness=2]-> BPL : <back:#FFFFFF><color:#2196F3>5. Batch Processing</color></back>
+BPL -[#2196F3,thickness=2]-> DW : <back:#FFFFFF><color:#2196F3>6. Store Processed Data</color></back>
+AE -[#9C27B0,thickness=2]-> VL : <back:#FFFFFF><color:#9C27B0>7. Visualize Results</color></back>
+VL -[#9C27B0,thickness=2]up-> DA : <back:#FFFFFF><color:#9C27B0>8. Analyze Data</color></back>
+VL -[#9C27B0,thickness=2]up-> BU : <back:#FFFFFF><color:#9C27B0>8. View Dashboards</color></back>
+AE -[#795548,thickness=2]-> ML : <back:#FFFFFF><color:#795548>9. Feed ML Models</color></back>
+ML -[#795548,thickness=2]-> AE : <back:#FFFFFF><color:#795548>10. Predictions</color></back>
+MM -[#FFC107,thickness=2]-> DIL : <back:#FFFFFF><color:#FFC107>11. Manage Metadata</color></back>
+MM -[#FFC107,thickness=2]-> SPL : <back:#FFFFFF><color:#FFC107>11. Manage Metadata</color></back>
+MM -[#FFC107,thickness=2]-> BPL : <back:#FFFFFF><color:#FFC107>11. Manage Metadata</color></back>
+MA -[#607D8B,thickness=2]-> DIL : <back:#FFFFFF><color:#607D8B>12. Monitor & Alert</color></back>
+MA -[#607D8B,thickness=2]-> SPL : <back:#FFFFFF><color:#607D8B>12. Monitor & Alert</color></back>
+MA -[#607D8B,thickness=2]-> AE : <back:#FFFFFF><color:#607D8B>12. Monitor & Alert</color></back>
+DS -[#009688,thickness=2]-> ML : <back:#FFFFFF><color:#009688>13. Develop Models</color></back>
+
+note top of DIL
+  Handles high-volume, 
+  high-velocity data ingestion
+end note
+
+note right of SPL
+  Processes data in real-time
+  for immediate insights
+end note
+
+note bottom of RTS
+  Stores real-time data
+  for quick access and analysis
+end note
+
+note bottom of BPL
+  Processes large volumes of
+  historical data for deep analysis
+end note
+
+note bottom of AE
+  Provides fast querying and
+  aggregation capabilities
+end note
+
+note bottom of ML
+  Applies machine learning
+  for predictive analytics
+end note
+
+note bottom of MM
+  Manages data lineage,
+  governance, and catalog
+end note
+
+note bottom of MA
+  Ensures system health
+  and performance
+end note
+
+@enduml
diff --git a/real_time_news_feed_architecture_with_ml_personalization_caching_and_event_streaming.puml b/real_time_news_feed_architecture_with_ml_personalization_caching_and_event_streaming.puml
new file mode 100644
--- /dev/null
+++ ./real_time_news_feed_architecture_with_ml_personalization_caching_and_event_streaming.puml
@@ -0,0 +1,89 @@
+@startuml Real-time News Feed Architecture
+
+!define RECTANGLE class
+skinparam backgroundColor #F0F0F0
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam arrowColor #033277
+skinparam arrowThickness 1.5
+skinparam packageBackgroundColor #E0E0E0
+
+allowmixing
+
+actor "User" as user
+
+rectangle "Client" {
+    component "Consumer App" as consumer
+    component "Push Notification\nService" as push
+    component "WebSocket Service" as websocket
+}
+
+rectangle "Server" {
+    component "API Gateway" as api_gateway
+    component "Auth Service" as auth
+    component "Feed Producer" as producer
+    queue "Message Queue" as mq
+    
+    rectangle "Backend Services" {
+        component "Feed Service" as feed
+        component "Recommendation\nEngine" as rec
+        component "User Preferences\nService" as pref
+        component "Logging &\nMonitoring" as log
+    }
+    
+    database "Storage" {
+        component "Distributed Cache" as cache
+        component "NoSQL Database" as db
+    }
+}
+
+user -[#red]-> consumer : 1. Request/Publish
+consumer -[#red]-> api_gateway : 2. Send Request
+api_gateway -[#orange]-> auth : 3. Authenticate
+auth -[#orange]-> api_gateway : 4. Auth Result
+api_gateway -[#green]-> producer : 5. Forward
+producer -[#green]-> mq : 6. Send to Queue
+
+mq -[#blue]-> feed : 7. Deliver
+feed -[#blue]-> rec : 8. Personalize
+feed -[#blue]-> pref : 9. Get Preferences
+feed -[#purple]-> log : 10. Log
+feed -[#purple]-> cache : 11. Cache Hot Feeds
+feed -[#purple]-> db : 12. Store Feeds
+
+consumer -[#red]-> api_gateway : 13. Request Latest
+api_gateway -[#green]-> feed : 14. Request Feeds
+feed -[#blue]-> cache : 15. Get Hot Feeds
+feed -[#blue]-> db : 16. Get Older Feeds
+
+feed -[#orange]-> websocket : 17. Push via WebSocket
+feed -[#orange]-> push : 18. Send Push
+websocket -[#orange]-> consumer : 19. Real-time Feed
+push -[#orange]-> consumer : 20. Notify
+
+note right of cache
+  key: value pairs
+  for quick access
+end note
+
+note bottom of mq
+  Potential bottleneck
+  Consider horizontal scaling
+  or using a distributed queue system
+end note
+
+note right of rec
+  CPU intensive
+  Consider caching recommendations
+  and using machine learning models
+end note
+
+note bottom of Server
+  Scalability:
+  - Use load balancers for API Gateway and services
+  - Implement microservices architecture
+  - Use containerization (e.g., Docker) for easy scaling
+end note
+
+@enduml
diff --git a/resnet18_deep_learning_model_architecture_with_detailed_layer_structure.puml b/resnet18_deep_learning_model_architecture_with_detailed_layer_structure.puml
new file mode 100644
--- /dev/null
+++ ./resnet18_deep_learning_model_architecture_with_detailed_layer_structure.puml
@@ -0,0 +1,105 @@
+@startuml
+package "Sequential" {
+  package "Sequential_1" {
+    Conv2d_0 : 3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
+    BatchNorm2d_1 : 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+    ReLU_2 : inplace=True
+    MaxPool2d_3 : kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False
+
+    package "Block_4" {
+      package "BasicBlock_0" {
+        Conv2d_0 : 64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+      }
+      package "BasicBlock_1" {
+        Conv2d_0 : 64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+      }
+    }
+
+    package "Block_5" {
+      package "BasicBlock_0" {
+        Conv2d_0 : 64 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        package "Downsample" {
+          Conv2d_0 : 64 -> 128, kernel_size=(1, 1), stride=(2, 2), bias=False
+          BatchNorm2d_1 : 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        }
+      }
+      package "BasicBlock_1" {
+        Conv2d_0 : 128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+      }
+    }
+
+    package "Block_6" {
+      package "BasicBlock_0" {
+        Conv2d_0 : 128 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        package "Downsample" {
+          Conv2d_0 : 128 -> 256, kernel_size=(1, 1), stride=(2, 2), bias=False
+          BatchNorm2d_1 : 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        }
+      }
+      package "BasicBlock_1" {
+        Conv2d_0 : 256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+      }
+    }
+
+    package "Block_7" {
+      package "BasicBlock_0" {
+        Conv2d_0 : 256 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        package "Downsample" {
+          Conv2d_0 : 256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False
+          BatchNorm2d_1 : 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        }
+      }
+      package "BasicBlock_1" {
+        Conv2d_0 : 512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_1 : 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+        ReLU_2 : inplace=True
+        Conv2d_3 : 512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
+        BatchNorm2d_4 : 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+      }
+    }
+  }
+
+  package "Sequential_2" {
+    package "AdaptiveConcatPool2d_0" {
+      AdaptiveAvgPool2d : output_size=1
+      AdaptiveMaxPool2d : output_size=1
+    }
+    Flatten_1 : full=False
+    BatchNorm1d_2 : 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+    Dropout_3 : p=0.25, inplace=False
+    Linear_4 : in_features=1024, out_features=512, bias=False
+    ReLU_5 : inplace=True
+    BatchNorm1d_6 : 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
+    Dropout_7 : p=0.5, inplace=False
+    Linear_8 : in_features=512, out_features=2, bias=False
+  }
+}
+@enduml
diff --git a/seckill_system_architecture/database_design_nosql_sharding_caching_for_high_concurrency.puml b/seckill_system_architecture/database_design_nosql_sharding_caching_for_high_concurrency.puml
new file mode 100644
--- /dev/null
+++ ./seckill_system_architecture/database_design_nosql_sharding_caching_for_high_concurrency.puml
@@ -0,0 +1,122 @@
+@startuml
+!define TABLE(name,desc) class name as "desc" << (T,#FFAAAA) >>
+!define NOSQL(name,desc) class name as "desc" << (N,#AAFFAA) >>
+!define PK(x) <b><color:red>PK: </color></b>x
+!define FK(x) <color:green>FK: </color>x
+!define SK(x) <b><color:blue>Shard Key: </color></b>x
+hide methods
+hide stereotypes
+
+skinparam class {
+    BackgroundColor #ECECEC
+    ArrowColor #2688d4
+    BorderColor #2688d4
+}
+
+title Integrated Database Design for Flash Sale System
+
+package "Relational Database" {
+    TABLE(users, "Users") {
+        PK(user_id): BIGINT
+        username: VARCHAR(50)
+        email: VARCHAR(100)
+        password_hash: VARCHAR(255)
+        created_at: TIMESTAMP
+        updated_at: TIMESTAMP
+    }
+
+    TABLE(flash_sale_events, "Flash Sale Events") {
+        PK(event_id): BIGINT
+        FK(product_id): STRING
+        start_time: TIMESTAMP
+        end_time: TIMESTAMP
+        sale_price: DECIMAL(10,2)
+        quantity_limit: INT
+        created_at: TIMESTAMP
+        updated_at: TIMESTAMP
+    }
+}
+
+package "NoSQL Database" {
+    NOSQL(products, "Products") {
+        + _id: STRING (product_id)
+        SK(product_id)
+        name: STRING
+        description: STRING
+        price: DECIMAL
+        stock_quantity: INT
+        created_at: TIMESTAMP
+        updated_at: TIMESTAMP
+    }
+
+    NOSQL(orders, "Orders") {
+        + _id: STRING (order_id)
+        SK(user_id + created_at)
+        product_id: STRING
+        quantity: INT
+        total_price: DECIMAL
+        status: STRING
+        created_at: TIMESTAMP
+        updated_at: TIMESTAMP
+    }
+
+    NOSQL(user_activity, "User Activity") {
+        + _id: AUTO-GENERATED
+        SK(user_id + timestamp)
+        activity_type: STRING
+        details: OBJECT
+        timestamp: TIMESTAMP
+    }
+
+    NOSQL(system_logs, "System Logs") {
+        + _id: AUTO-GENERATED
+        SK(component + timestamp)
+        log_level: STRING
+        message: STRING
+        timestamp: TIMESTAMP
+    }
+}
+
+package "Cache" {
+    NOSQL(product_cache, "Product Cache") {
+        + key: product:{product_id}
+        product_id: STRING
+        name: STRING
+        price: DECIMAL
+        stock_quantity: INT
+        description: STRING
+    }
+
+    NOSQL(flash_sale_cache, "Flash Sale Cache") {
+        + key: flash_sale:{event_id}
+        event_id: STRING
+        product_id: STRING
+        start_time: TIMESTAMP
+        end_time: TIMESTAMP
+        sale_price: DECIMAL
+        quantity_limit: INT
+        remaining_quantity: INT
+    }
+
+    NOSQL(user_session_cache, "User Session Cache") {
+        + key: session:{session_id}
+        user_id: STRING
+        username: STRING
+        email: STRING
+        last_activity: TIMESTAMP
+    }
+}
+
+' Relationships
+users "1" -- "0..*" orders : places
+users "1" -- "0..*" user_activity : generates
+products "1" -- "0..*" orders : includes
+products "1" -- "0..*" flash_sale_events : featured in
+flash_sale_events "1" -- "0..*" orders : generates
+
+' Cache relationships
+products .. product_cache : caches
+flash_sale_events .. flash_sale_cache : caches
+users .. user_session_cache : caches
+
+@enduml
diff --git a/seckill_system_architecture/high_concurrency_seckill_system_backend_architecture_with_distributed_transactions_and_anti_fraud.puml b/seckill_system_architecture/high_concurrency_seckill_system_backend_architecture_with_distributed_transactions_and_anti_fraud.puml
new file mode 100644
--- /dev/null
+++ ./seckill_system_architecture/high_concurrency_seckill_system_backend_architecture_with_distributed_transactions_and_anti_fraud.puml
@@ -0,0 +1,126 @@
+@startuml High Concurrency Seckill System Backend Architecture
+!pragma layout dot
+
+skinparam defaultFontSize 19
+skinparam defaultFontName Arial
+skinparam noteFontSize 19
+skinparam noteFontName Arial
+
+rectangle "Client Layer" as ClientLayer #E1F5FE
+rectangle "Entry Layer" as EntryLayer #FFEBEE {
+    component "CDN" as CDN #FFCDD2
+    component "Load Balancer" as LB #FFCDD2
+    component "API Gateway" as APIGW #FFCDD2
+}
+rectangle "Security Layer" as SecurityLayer #E8F5E9 {
+    component "Authentication" as Auth #C8E6C9
+    component "Rate Limiter" as RateLimiter #C8E6C9
+    component "DDoS Protection" as DDoS #C8E6C9
+}
+rectangle "Service Layer" as ServiceLayer #FFF3E0 {
+    component "Command Service" as CommandService #FFE0B2
+    component "Query Service" as QueryService #DCEDC8
+}
+rectangle "Cache Layer" as CacheLayer #F3E5F5 {
+    component "Redis" as Redis #E1BEE7
+    component "Memcached" as Memcached #B3E5FC
+}
+queue "Message Queue" as MessageQueue #FAFAFA
+rectangle "Async Processing" as AsyncProcessing #FFFDE7 {
+    component "Order Processor" as OrderProcessor #FFF9C4
+    component "Inventory Manager" as InventoryManager #FFF9C4
+    component "Cache Updater" as CacheUpdater #FFF9C4
+}
+rectangle "Data Layer" as DataLayer #E0F2F1 {
+    database "Main DB" as MainDB #B2DFDB
+    database "Read Replicas" as ReadReplicas #B2DFDB
+}
+rectangle "Monitoring & Logging" as MonitoringLogging #D1C4E9
+
+ClientLayer -[#0000FF,thickness=2]-> EntryLayer : "<font size=19><color:#0000FF><b>1. Request</b></color></font>"
+EntryLayer -[#00AA00,thickness=2]-> SecurityLayer : "<font size=19><color:#00AA00><b>2. Validate</b></color></font>"
+SecurityLayer -[#FF0000,thickness=2]-> ServiceLayer : "<font size=19><color:#FF0000><b>3. Process</b></color></font>"
+CommandService -[#AAAA00,thickness=2]-> MainDB : "<font size=19><color:#AAAA00><b>4. Write to main DB</b></color></font>"
+CommandService -[#00AAAA,thickness=2]-> MessageQueue : "<font size=19><color:#00AAAA><b>5. Publish changes</b></color></font>"
+CommandService -[#FF6600,thickness=2]-> CacheLayer : "<font size=19><color:#FF6600><b>6. Direct cache update (critical data)</b></color></font>"
+AsyncProcessing <-[#AA00AA,thickness=2]- MessageQueue : "<font size=19><color:#AA00AA><b>7. Consume events</b></color></font>"
+AsyncProcessing -[#FF6600,thickness=2]-> CacheLayer : "<font size=19><color:#FF6600><b>8. Update cache</b></color></font>"
+AsyncProcessing -[#AAAA00,thickness=2]-> DataLayer : "<font size=19><color:#AAAA00><b>9. Async DB updates</b></color></font>"
+QueryService <-[#FF00FF,thickness=2]-> CacheLayer : "<font size=19><color:#FF00FF><b>10. Read from cache</b></color></font>"
+QueryService <-[#AAAA00,dashed,thickness=2]-> ReadReplicas : "<font size=19><color:#AAAA00><b>11. Read from DB (rare cases)</b></color></font>"
+
+MonitoringLogging ..> ClientLayer : "Monitor"
+MonitoringLogging ..> EntryLayer : "Monitor"
+MonitoringLogging ..> SecurityLayer : "Monitor"
+MonitoringLogging ..> ServiceLayer : "Monitor"
+MonitoringLogging ..> CacheLayer : "Monitor"
+MonitoringLogging ..> MessageQueue : "Monitor"
+MonitoringLogging ..> AsyncProcessing : "Monitor"
+MonitoringLogging ..> DataLayer : "Monitor"
+
+note right of EntryLayer
+  CDN for static content,
+  Load Balancer for traffic distribution,
+  API Gateway for request routing
+end note
+
+note right of SecurityLayer
+  Implements authentication,
+  rate limiting, and DDoS protection
+end note
+
+note bottom of CacheLayer
+  Multi-level Cache Strategy:
+  - L1: Local cache (in-memory)
+  - L2: Distributed cache (Redis)
+  - L3: Global cache (Memcached)
+  
+  Inventory Management:
+  - Pre-warm inventory data in Redis
+  - Use Lua scripts for atomic inventory deduction
+  - Implement distributed locks for concurrency control
+end note
+
+note bottom of AsyncProcessing
+  Inventory Preheating Strategy:
+  - Load inventory data into cache before sale starts
+  - Gradually increase cache capacity as sale approaches
+  - Use predictive algorithms to estimate required cache size
+
+  Distributed Transaction Handling:
+  - Implement 2PC (Two-Phase Commit) for critical operations
+  - Use Saga pattern for long-running transactions
+  - Implement compensating transactions for rollback scenarios
+
+  Rate Limiting and Circuit Breaking:
+  - Use token bucket algorithm for rate limiting
+  - Implement circuit breaker pattern to prevent system overload
+  - Dynamic adjustment of limits based on system load
+
+  User Behavior Analysis and Anti-fraud:
+  - Implement real-time user behavior monitoring
+  - Use machine learning models to detect suspicious activities
+  - Implement IP-based and device fingerprint-based restrictions
+end note
+
+note bottom of DataLayer
+  Performance optimization:
+  - Use read replicas for query operations
+  - Implement database sharding for write scalability
+  - Consider NoSQL solutions for specific data types:
+    1. Redis (Key-Value Store):
+       - Real-time inventory counts
+       - Session data and user tokens
+    2. MongoDB (Document Store):
+       - Product catalogs with flexible schemas
+       - User profiles and preferences
+    3. Cassandra (Wide-column Store):
+       - Time-series data for analytics
+       - High-volume write operations (e.g., click streams)
+    4. Neo4j (Graph Database):
+       - Social connections and recommendations
+    5. Elasticsearch (Search Engine):
+       - Full-text search for products and content
+end note
+
+@enduml
diff --git a/seckill_system_architecture/seckill_system_data_layer_architecture_with_sharding_caching_and_consistency_management.puml b/seckill_system_architecture/seckill_system_data_layer_architecture_with_sharding_caching_and_consistency_management.puml
new file mode 100644
--- /dev/null
+++ ./seckill_system_architecture/seckill_system_data_layer_architecture_with_sharding_caching_and_consistency_management.puml
@@ -0,0 +1,88 @@
+@startuml Seckill System Data Layer Architecture
+!pragma layout smetana
+skinparam backgroundColor #FFFFFF
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Seckill System Data Layer Architecture
+
+rectangle "Data Access Layer" as DataAccessLayer #FFB6C1 {
+    component "ORM" as ORM
+    component "Connection Pool" as ConnectionPool
+    component "Query Optimizer" as QueryOptimizer
+    component "Transaction Manager" as TransactionManager
+}
+
+rectangle "Database Sharding" as DatabaseSharding #FFB6C1 {
+    component "Shard 1" as Shard1
+    component "Shard 2" as Shard2
+    component "Shard 3" as Shard3
+}
+
+rectangle "Database Optimization" as DatabaseOptimization #FFB6C1 {
+    component "Indexing" as Indexing
+    component "Query Optimization" as QueryOpt
+    component "Data Partitioning" as DataPartitioning
+}
+
+rectangle "Cache" as Cache #00FFFF {
+    component "Redis" as Redis
+    component "Memcached" as Memcached
+}
+
+rectangle "Persistent Storage" as PersistentStorage #90EE90 {
+    database "Master DB" as MasterDB
+    database "Slave DB 1" as SlaveDB1
+    database "Slave DB 2" as SlaveDB2
+}
+
+rectangle "Data Consistency" as DataConsistency #FFA07A {
+    component "Distributed Transactions" as DistributedTransactions
+    component "Eventual Consistency" as EventualConsistency
+}
+
+rectangle "Backup & Recovery" as BackupRecovery #98FB98 {
+    component "Regular Backups" as RegularBackups
+    component "Point-in-Time Recovery" as PITRecovery
+}
+
+DataAccessLayer --> DatabaseSharding : Manages
+DataAccessLayer --> Cache : Utilizes
+DataAccessLayer --> PersistentStorage : Persists
+DatabaseOptimization --> DatabaseSharding : Optimizes
+DataConsistency --> PersistentStorage : Ensures
+BackupRecovery --> PersistentStorage : Protects
+
+note right of DataAccessLayer
+  Handles data access operations,
+  connection pooling, and query optimization
+end note
+
+note right of DatabaseSharding
+  Distributes data across multiple
+  database instances for scalability
+end note
+
+note right of Cache
+  Improves read performance
+  by caching frequently accessed data
+end note
+
+note right of PersistentStorage
+  Ensures data durability with
+  master-slave replication
+end note
+
+note right of DataConsistency
+  Manages data consistency across
+  distributed database instances
+end note
+
+note right of BackupRecovery
+  Implements regular backups and
+  point-in-time recovery mechanisms
+end note
+
+@enduml
diff --git a/seckill_system_architecture/seckill_system_frontend_architecture_with_cdn_and_local_caching.puml b/seckill_system_architecture/seckill_system_frontend_architecture_with_cdn_and_local_caching.puml
new file mode 100644
--- /dev/null
+++ ./seckill_system_architecture/seckill_system_frontend_architecture_with_cdn_and_local_caching.puml
@@ -0,0 +1,51 @@
+@startuml Seckill System Frontend Architecture
+!pragma layout smetana
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam roundcorner 20
+skinparam shadowing false
+
+title Seckill System Frontend Architecture
+
+rectangle "Mobile App" as MobileApp #LightBlue {
+    component "Native UI" as NativeUI
+    component "HTTP Client" as HTTPClient
+    component "Push Notification" as PushNotification
+}
+
+rectangle "Web Frontend" as WebFrontend #LightBlue {
+    component "Static Resources" as StaticResources
+    component "JavaScript SDK" as JavaScriptSDK
+    component "WebSocket Client" as WebSocketClient
+}
+
+rectangle CDN #LightGreen {
+    component "Static Content" as StaticContent
+    component "Image Caching" as ImageCaching
+}
+
+rectangle "API Gateway" as APIGateway #Orange
+
+WebFrontend --> CDN : "1. Fetch static content"
+MobileApp --> CDN : "2. Fetch static content"
+WebFrontend --> APIGateway : "3. API requests"
+MobileApp --> APIGateway : "4. API requests"
+
+note right of WebFrontend
+  Optimizations:
+  - Use Service Worker for offline support
+  - Implement progressive loading
+end note
+
+note right of MobileApp
+  Optimizations:
+  - Implement local caching
+  - Use background sync for offline actions
+end note
+
+note bottom of CDN
+  Global distribution for low latency
+end note
+
+@enduml
diff --git a/service_mesh_architecture_with_istio_components.puml b/service_mesh_architecture_with_istio_components.puml
new file mode 100644
--- /dev/null
+++ ./service_mesh_architecture_with_istio_components.puml
@@ -0,0 +1,84 @@
+@startuml service_mesh_architecture_with_istio_components
+
+!pragma layout dot
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+
+rectangle "Kubernetes Cluster" as KC {
+    rectangle "Service A Pod" as SAP #E1F5FE {
+        component "Service A" as SA
+        component "Envoy Proxy" as EP1 #B3E5FC
+    }
+
+    rectangle "Service B Pod" as SBP #E1F5FE {
+        component "Service B" as SB
+        component "Envoy Proxy" as EP2 #B3E5FC
+    }
+
+    rectangle "Service C Pod" as SCP #E1F5FE {
+        component "Service C" as SC
+        component "Envoy Proxy" as EP3 #B3E5FC
+    }
+
+    rectangle "Istio Control Plane" as ICP #81D4FA {
+        component "Istiod" as ID {
+            component "Pilot" as PI
+            component "Citadel" as CI
+            component "Galley" as GA
+        }
+    }
+
+    rectangle "Ingress Gateway" as IG #4FC3F7
+    rectangle "Egress Gateway" as EG #4FC3F7
+}
+
+cloud "External Services" as ES #E1F5FE
+
+actor "User" as US
+
+US -[#FF5722,thickness=2]-> IG : <back:#FFFFFF><color:#FF5722>1. HTTPS Request</color></back>
+IG -[#FF5722,thickness=2]-> EP1 : <back:#FFFFFF><color:#FF5722>2. Route Request</color></back>
+EP1 -[#4CAF50,thickness=2]-> SA : <back:#FFFFFF><color:#4CAF50>3. Forward Request</color></back>
+SA -[#4CAF50,thickness=2]-> EP1 : <back:#FFFFFF><color:#4CAF50>4. Response</color></back>
+EP1 -[#2196F3,thickness=2]-> EP2 : <back:#FFFFFF><color:#2196F3>5. Inter-service Call</color></back>
+EP2 -[#2196F3,thickness=2]-> SB : <back:#FFFFFF><color:#2196F3>6. Forward Request</color></back>
+EP2 -[#9C27B0,thickness=2]-> EP3 : <back:#FFFFFF><color:#9C27B0>7. Inter-service Call</color></back>
+EP3 -[#9C27B0,thickness=2]-> SC : <back:#FFFFFF><color:#9C27B0>8. Forward Request</color></back>
+EP3 -[#795548,thickness=2]-> EG : <back:#FFFFFF><color:#795548>9. External Call</color></back>
+EG -[#795548,thickness=2]-> ES : <back:#FFFFFF><color:#795548>10. Route to External</color></back>
+
+ID -[#FFC107,thickness=2,dashed]-> EP1 : <back:#FFFFFF><color:#FFC107>Config & Certs</color></back>
+ID -[#FFC107,thickness=2,dashed]-> EP2 : <back:#FFFFFF><color:#FFC107>Config & Certs</color></back>
+ID -[#FFC107,thickness=2,dashed]-> EP3 : <back:#FFFFFF><color:#FFC107>Config & Certs</color></back>
+
+note right of ICP
+  Istio Control Plane:
+  - Pilot: Service discovery and config
+  - Citadel: Key and cert management
+  - Galley: Config validation and distribution
+end note
+
+note bottom of IG
+  Ingress Gateway:
+  - L7 load balancing
+  - TLS termination
+  - HTTP/2 and gRPC proxying
+end note
+
+note bottom of EG
+  Egress Gateway:
+  - Control outbound traffic
+  - Enhance security
+end note
+
+note right of EP1
+  Envoy Proxy:
+  - Service discovery
+  - Load balancing
+  - Traffic management
+  - Observability
+end note
+
+@enduml
diff --git a/telegram/uml_diagrams/Telegram_Complete_Database_Schema.puml b/telegram/uml_diagrams/Telegram_Complete_Database_Schema.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/Telegram_Complete_Database_Schema.puml
@@ -0,0 +1,124 @@
+@startuml
+!theme plain
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName "Arial"
+skinparam defaultFontColor #333333
+skinparam classFontColor #000000
+skinparam classFontSize 14
+skinparam classAttributeFontSize 12
+skinparam classBorderThickness 1
+skinparam arrowThickness 1.5
+skinparam arrowColor #666666
+
+skinparam class {
+    BackgroundColor<<T>> #4A90E2
+    BackgroundColor<<D>> #50C878
+    BackgroundColor<<R>> #FF6B6B
+    BorderColor #333333
+}
+
+' User table
+class "User" as U << (T,#4A90E2) >> {
+  + user_id: int <<PK>>
+  --
+  username: varchar <<index>>
+  phone_number: varchar <<index>>
+  email: varchar <<index>>
+  password_hash: varchar
+  last_login: datetime
+  registration_date: datetime
+  status_timestamp: datetime
+}
+
+' PrivateChat table
+class "PrivateChat" as PC << (T,#4A90E2) >> {
+  + chat_id: int <<PK>>
+  --
+  sender_id: int <<FK>> <<index>>
+  recipient_id: int <<FK>> <<index>>
+  last_message_id: ObjectId <<FK>>
+  last_message_timestamp: datetime <<index>>
+}
+
+' GroupChat table
+class "GroupChat" as GC << (T,#4A90E2) >> {
+  + chat_id: int <<PK>>
+  --
+  chat_name: varchar
+  chat_type: enum ('group', 'channel')
+  creation_date: datetime
+  last_message_id: ObjectId <<FK>>
+  last_message_timestamp: datetime <<index>>
+}
+
+' MongoDB Message NoSQL collection
+class "MongoDB_Message_NoSQL" as MM << (D,#50C878) >> {
+  + message_id: ObjectId <<PK>>
+  --
+  chat_id: int <<FK>> <<sharding key>>
+  sender_user_id: int <<FK>>
+  content: text
+  timestamp: datetime <<index>>
+  type: enum('text', 'photo', 'video', 'document')
+}
+
+' GroupMembers table
+class "GroupMembers" as GM << (T,#4A90E2) >> {
+  + group_id: int <<FK>> <<index>>
+  + user_id: int <<FK>> <<index>>
+  --
+  join_date: datetime
+  role: enum ('member', 'admin', 'owner')
+}
+
+' UserProfile table
+class "UserProfile" as UP << (T,#4A90E2) >> {
+  + user_id: int <<FK>> <<PK>>
+  --
+  avatar: image
+  about: text
+  last_updated: datetime
+}
+
+' Stickers table
+class "Stickers" as S << (T,#4A90E2) >> {
+  + sticker_id: int <<PK>>
+  --
+  user_id: int <<FK>> <<index>>
+  image: image
+  added_date: datetime
+}
+
+' Redis Cache
+class "RedisCache" as RC << (R,#FF6B6B) >> {
+  + User Status and Last Seen
+  Key: user:{user_id}:status
+  Value: online|offline|last_seen_timestamp
+  --
+  + Unread Messages Count
+  Key: user:{user_id}:unread:{chat_id}
+  Value: count
+}
+
+' Contacts table
+class "Contacts" as C << (T,#4A90E2) >> {
+  + user_id: int <<FK>> <<index>>
+  + contact_id: int <<FK>> <<index>>
+  --
+  added_date: datetime
+}
+
+U "1" -- "1" PC : Initiates >
+U "1" -- "1" PC : < Receives
+U "1" -- "n" GC : Participates >
+U "1" -- "n" MM : Sends >
+PC "1" -- "n" MM : Contains >
+GC "1" -- "n" MM : Contains >
+GC "1" -- "n" GM : Has >
+U "1" -- "1" UP : Has >
+U "n" -- "n" GM : Joins >
+U "1" -- "n" S : Owns >
+U --> RC : Uses for\nfrequent updates >
+U "n" -- "n" C : Has >
+
+@enduml
diff --git a/telegram/uml_diagrams/Telegram_Disaster_Recovery_And_Backup.puml b/telegram/uml_diagrams/Telegram_Disaster_Recovery_And_Backup.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/Telegram_Disaster_Recovery_And_Backup.puml
@@ -0,0 +1,54 @@
+@startuml Telegram_Disaster_Recovery_And_Backup
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam monochrome true
+skinparam packageStyle rectangle
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+
+rectangle "Primary Data Center" {
+    database "Primary Databases" {
+        [User Data]
+        [Message Data]
+    }
+    [Application Servers]
+}
+
+rectangle "Backup Data Center" {
+    database "Backup Databases" {
+        [Replicated User Data]
+        [Replicated Message Data]
+    }
+    [Standby Application Servers]
+}
+
+rectangle "Disaster Recovery System" {
+    [Monitoring Service]
+    [Failover Manager]
+    [Data Sync Service]
+}
+
+rectangle "Cloud Storage" {
+    [Encrypted Backups]
+}
+
+rectangle "Backup Management" {
+    [Backup Scheduler]
+    [Encryption Service]
+    [Integrity Checker]
+}
+
+rectangle "Recovery Tools" {
+    [Data Restoration Service]
+    [System State Manager]
+}
+
+"Primary Data Center" <--> "Disaster Recovery System" : Monitor & Manage
+"Backup Data Center" <--> "Disaster Recovery System" : Sync & Activate
+"Primary Data Center" --> "Cloud Storage" : Store encrypted backups
+"Backup Management" --> "Cloud Storage" : Manage backups
+"Recovery Tools" <-- "Cloud Storage" : Restore from backups
+"Disaster Recovery System" <--> "Recovery Tools" : Coordinate recovery
+
+@enduml
diff --git a/telegram/uml_diagrams/Telegram_End_To_End_Encryption.puml b/telegram/uml_diagrams/Telegram_End_To_End_Encryption.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/Telegram_End_To_End_Encryption.puml
@@ -0,0 +1,53 @@
+@startuml Telegram_End_To_End_Encryption
+!define RECTANGLE class
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam monochrome true
+skinparam packageStyle rectangle
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+
+RECTANGLE "Sender's Device" {
+    [Message Composer]
+    [Encryption Engine]
+    [Key Management]
+}
+
+RECTANGLE "Receiver's Device" {
+    [Message Viewer]
+    [Decryption Engine]
+    [Key Management]
+}
+
+RECTANGLE "Telegram Servers" {
+    [Message Router]
+    [Encrypted Message Storage]
+}
+
+RECTANGLE "Key Exchange Service" {
+    [Diffie-Hellman Key Exchange]
+}
+
+"Sender's Device" --> "Key Exchange Service" : Initiate key exchange
+"Receiver's Device" --> "Key Exchange Service" : Complete key exchange
+"Key Exchange Service" --> "Sender's Device" : Shared secret
+"Key Exchange Service" --> "Receiver's Device" : Shared secret
+
+"Sender's Device" --> "Telegram Servers" : Send encrypted message
+"Telegram Servers" --> "Receiver's Device" : Route encrypted message
+
+note right of "Sender's Device"
+  1. Compose message
+  2. Generate session key
+  3. Encrypt message with session key
+  4. Encrypt session key with receiver's public key
+end note
+
+note left of "Receiver's Device"
+  1. Decrypt session key with private key
+  2. Decrypt message with session key
+  3. Display message
+end note
+
+@enduml
diff --git a/telegram/uml_diagrams/Telegram_File_Transfer_And_Storage.puml b/telegram/uml_diagrams/Telegram_File_Transfer_And_Storage.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/Telegram_File_Transfer_And_Storage.puml
@@ -0,0 +1,114 @@
+@startuml Telegram_File_Transfer_And_Storage
+
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 15
+skinparam ArrowFontSize 15
+skinparam NoteFontSize 17
+skinparam roundcorner 10
+skinparam shadowing false
+
+rectangle "Client Applications" as CA #E6F3FF {
+    component "Mobile App" as MA
+    component "Desktop Client" as DC
+    component "Web Client" as WC
+}
+
+rectangle "Load Balancing" as LB #F0FFF0 {
+    component "Global Load Balancer" as GLB
+    component "Regional Load Balancers" as RLB
+}
+
+rectangle "File Transfer Service Cluster" as FTSC #E6F3FF {
+    component "Chunk Manager" as CM
+    component "Metadata Handler" as MH
+    component "Encryption Service" as ES
+    component "Deduplication" as DD
+    component "Rate Limiter" as RL
+    component "Compression Service" as CS
+}
+
+rectangle "Storage" as ST #F0FFFF {
+    component "Metadata DB (Cassandra)" as MDB
+    component "Object Storage (S3-compatible)" as OS
+}
+
+rectangle "Content Delivery" as CD #F0F8FF {
+    component "CDN Edge Servers" as CES
+    component "CDN Caching Servers" as CCS
+}
+
+rectangle "File Processing" as FP #E6F3FF {
+    component "File Processing Queue (Kafka)" as FPQ
+    component "Thumbnail Generator" as TG
+    component "Video Transcoder" as VT
+    component "Virus Scanner" as VS
+    component "File Analyzer" as FA
+}
+
+rectangle "Caching" as CL #F0FFFF {
+    component "Redis Cluster" as RC
+}
+
+rectangle "Monitoring & Analytics" as MA #FFE6E6 {
+    component "Prometheus" as PM
+    component "Grafana" as GF
+    component "ELK Stack" as ELK
+}
+
+CA -[#4682B4,thickness=2]-> LB : <back:#FFFFFF><color:#4682B4>1. Request</color></back>
+LB -[#008000,thickness=2]-> FTSC : <back:#FFFFFF><color:#008000>2. Route</color></back>
+FTSC <-[#8A2BE2,thickness=2]-> ST : <back:#FFFFFF><color:#8A2BE2>3. Store/Retrieve</color></back>
+FTSC -[#1E90FF,thickness=2]-> CD : <back:#FFFFFF><color:#1E90FF>4. Distribute</color></back>
+CD -[#32CD32,thickness=2]-> CA : <back:#FFFFFF><color:#32CD32>5. Serve</color></back>
+FTSC -[#FF69B4,thickness=2]-> FP : <back:#FFFFFF><color:#FF69B4>6. Process</color></back>
+FTSC <-[#DAA520,thickness=2]-> CL : <back:#FFFFFF><color:#DAA520>7. Cache</color></back>
+MA -[#FF6347,thickness=2]-> FTSC : <back:#FFFFFF><color:#FF6347>8. Monitor</color></back>
+
+note right of MDB
+    Sharding Key: file_id
+    - Efficient query by file ID
+    - Same shard for file metadata
+    - Support file-level operations
+end note
+
+note right of OS
+    Sharding Key: chunk_id
+    - Independent access, high parallelism
+    - Support chunking and resumable transfers
+    - Even distribution, avoid hotspots
+    - Easy horizontal scaling
+end note
+
+note bottom of RC
+    Key Design:
+    1. file:{id}:meta - Hash(name,size,type,time)
+    2. file:{id}:chunks - Set(chunk_id1, chunk_id2, ...)
+    3. chunk:{id}:meta - Hash(size,checksum,location)
+    4. user:{id}:recent - SortedSet(file_id, timestamp)
+    5. file:{id}:access - String(count)
+    6. file:{id}:upload - Hash(total,uploaded,status)
+    
+    Expiration Strategy:
+    - Metadata/chunks: LRU, 1h
+    - Recent files: 7d
+    - Access count: 30d
+    - Upload status: 15m after completion
+end note
+
+note as FileChunkRelation
+    file_id and chunk_id relationship:
+    1. One file_id maps to multiple chunk_ids
+    2. Metadata DB stores the mapping
+    3. Upload: file_id first, then chunk_ids
+    4. Download: query all chunk_ids by file_id
+    5. chunk_id format: file_id + sequence number
+    6. Support large file chunked transfer
+end note
+
+FileChunkRelation .. ST
+
+@enduml
diff --git a/telegram/uml_diagrams/Telegram_Multi_Device_Sync.puml b/telegram/uml_diagrams/Telegram_Multi_Device_Sync.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/Telegram_Multi_Device_Sync.puml
@@ -0,0 +1,105 @@
+@startuml Telegram_Multi_Device_Sync_Optimized
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #E8E8E8
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam componentStyle rectangle
+skinparam rectangleFontSize 16
+skinparam noteFontSize 15
+
+rectangle "User Devices" as UserDevices #E1F5FE {
+    component "Mobile App" as MobileApp
+    component "Desktop Client" as DesktopClient
+    component "Web Client" as WebClient
+}
+
+rectangle "API Gateway" as APIGateway #FFF9C4 {
+    component "Load Balancer" as LoadBalancer
+    component "Rate Limiter" as RateLimiter
+    component "Auth Service" as AuthService
+}
+
+rectangle "Sync Service Cluster" as SyncServiceCluster #E8F5E9 {
+    component "Device Manager" as DeviceManager
+    component "Message Synchronizer" as MessageSynchronizer
+    component "State Reconciliation" as StateReconciliation
+    component "Conflict Resolution" as ConflictResolution
+}
+
+rectangle "Message Store" as MessageStore #FFEBEE {
+    component "Cassandra Cluster" as CassandraCluster
+}
+
+rectangle "Device Registry" as DeviceRegistry #F3E5F5 {
+    component "Redis Cluster" as RedisCluster
+}
+
+rectangle "Key Management Service" as KeyManagementService #FFF3E0 {
+    component "Key Distribution" as KeyDistribution
+    component "Key Rotation" as KeyRotation
+    component "HSM" as HSM
+}
+
+rectangle "Message Queue" as MessageQueue #E0F2F1 {
+    component "Kafka Cluster" as KafkaCluster
+}
+
+rectangle "Push Notification Service" as PushNotificationService #F1F8E9 {
+    component "Notification Dispatcher" as NotificationDispatcher
+}
+
+rectangle "Caching Layer" as CachingLayer #FAFAFA {
+    component "Redis Cache" as RedisCache
+}
+
+rectangle "Analytics & Monitoring" as AnalyticsMonitoring #EFEBE9 {
+    component "Prometheus" as Prometheus
+    component "Grafana" as Grafana
+    component "ELK Stack" as ELKStack
+}
+
+UserDevices -[#1E88E5,thickness=2]-> APIGateway : <back:#FFFFFF><color:#1E88E5>1. HTTPS Requests</color></back>
+APIGateway -[#FBC02D,thickness=2]-> SyncServiceCluster : <back:#FFFFFF><color:#FBC02D>2. Internal API</color></back>
+SyncServiceCluster -[#4CAF50,thickness=2]-> MessageStore : <back:#FFFFFF><color:#4CAF50>3. Read/Write Messages</color></back>
+SyncServiceCluster -[#7B1FA2,thickness=2]-> DeviceRegistry : <back:#FFFFFF><color:#7B1FA2>4. Manage Devices</color></back>
+SyncServiceCluster -[#FF7043,thickness=2]-> KeyManagementService : <back:#FFFFFF><color:#FF7043>5. Encryption Keys</color></back>
+SyncServiceCluster -[#00ACC1,thickness=2]-> MessageQueue : <back:#FFFFFF><color:#00ACC1>6. Publish Changes</color></back>
+MessageQueue -[#8BC34A,thickness=2]-> PushNotificationService : <back:#FFFFFF><color:#8BC34A>7. Consume Events</color></back>
+PushNotificationService -[#9E9E9E,thickness=2]-> UserDevices : <back:#FFFFFF><color:#9E9E9E>8. Send Notifications</color></back>
+SyncServiceCluster <-[#795548,thickness=2]-> CachingLayer : <back:#FFFFFF><color:#795548>9. Cache Data</color></back>
+AnalyticsMonitoring -[#607D8B,thickness=2]-> SyncServiceCluster : <back:#FFFFFF><color:#607D8B>10. Collect Metrics</color></back>
+
+note bottom of SyncServiceCluster
+Performance bottlenecks and optimizations:
+1. High concurrency during peak hours
+   - Implement adaptive scaling
+   - Use load shedding techniques
+2. Message synchronization delays
+   - Optimize database queries
+   - Implement efficient caching strategies
+end note
+
+note bottom of MessageStore
+Optimization suggestions:
+1. Implement read replicas for improved read performance
+2. Use time-series optimized storage for older messages
+3. Implement data partitioning based on message age
+end note
+
+note bottom of CachingLayer
+Performance enhancements:
+1. Implement multi-level caching (L1: in-memory, L2: Redis)
+2. Use cache warming for frequently accessed data
+3. Implement cache invalidation strategies
+end note
+
+note bottom of MessageQueue
+Scalability improvements:
+1. Implement topic partitioning for better parallelism
+2. Use compacted topics for efficient state management
+3. Implement dead letter queues for error handling
+end note
+
+@enduml
diff --git a/telegram/uml_diagrams/Telegram_Voice_Video_Call_WebRTC_Architecture.puml b/telegram/uml_diagrams/Telegram_Voice_Video_Call_WebRTC_Architecture.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/Telegram_Voice_Video_Call_WebRTC_Architecture.puml
@@ -0,0 +1,73 @@
+@startuml Telegram_Voice_Video_Call_Architecture_Optimized
+!theme plain
+skinparam backgroundColor #E6E6FA
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam roundcorner 10
+skinparam arrowColor #333333
+skinparam componentBackgroundColor #EEEEEE
+
+rectangle "User Devices" {
+    [Caller Device\nWebRTC Engine] as CallerDevice
+    [Callee Device\nWebRTC Engine] as CalleeDevice
+}
+
+rectangle "Telegram Servers" {
+    [Global Load Balancer] as GlobalLB
+    [Regional Load Balancer] as RegionalLB
+    rectangle "Signaling Server Cluster" {
+        [Signaling Server] as SignalingServer
+        [Cluster Manager] as SignalingManager
+    }
+    [Call Initiator] as CallInitiator
+    [Session Manager] as SessionManager
+}
+
+cloud "Optimized Network Services" {
+    rectangle "STUN/TURN Server Cluster" {
+        [STUN/TURN Server] as STUNTURNServer
+        [Cluster Manager] as STUNTURNManager
+    }
+    [Auto-scaler] as AutoScaler
+}
+
+database "Database Cluster" {
+    [Database Node] as DBNode
+    [Cluster Manager] as DBManager
+}
+
+rectangle "Caching Layer" {
+    [Redis Node] as RedisNode
+    [Cluster Manager] as RedisManager
+}
+
+CallerDevice -down-> GlobalLB : 1. Initiate call
+GlobalLB -down-> RegionalLB : 2. Route to nearest region
+RegionalLB -down-> SignalingServer : 3. Handle signaling
+SignalingServer -down-> CalleeDevice : 4. Notify incoming call
+CallerDevice <--> STUNTURNServer : 5. Get public IP / Relay
+CalleeDevice <--> STUNTURNServer : 6. Get public IP / Relay
+CallerDevice <---> CalleeDevice : 7. Establish P2P connection
+SignalingServer --> DBNode : Log calls
+SignalingServer <--> RedisNode : Cache user status
+RedisNode <--> DBNode : Sync preferences
+
+AutoScaler -up-> STUNTURNManager : Manage scaling
+
+note right of DBNode
+  Tables:
+  Users (PK: user_id, Index: username, phone_number)
+  CallLogs (PK: call_id, FK: caller_id, callee_id, Index: start_time, end_time)
+  UserPreferences (PK: user_id)
+end note
+
+note right of RedisNode
+  Keys:
+  user:online:{user_id} (TTL: 5min)
+  active_call:{call_id} (Hash)
+  user:device:{user_id} (Hash)
+  call:stats:{call_id} (Hash)
+end note
+
+@enduml
diff --git a/telegram/uml_diagrams/telegram_enhanced_group_chat_system_architecture.puml b/telegram/uml_diagrams/telegram_enhanced_group_chat_system_architecture.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/telegram_enhanced_group_chat_system_architecture.puml
@@ -0,0 +1,120 @@
+@startuml Telegram_Enhanced_Group_Chat_System_Architecture
+
+!pragma layout dot
+skinparam backgroundColor #F0F0F0
+skinparam handwritten false
+skinparam defaultFontName Arial
+skinparam defaultFontSize 18
+skinparam roundCorner 10
+skinparam shadowing false
+skinparam ArrowColor #454645
+skinparam ArrowThickness 1.5
+allowmixing
+
+title Telegram Enhanced Group Chat System Architecture
+
+rectangle "Client Layer" as ClientLayer #E1F5FE {
+    component "Mobile App"
+    component "Desktop Client"
+    component "Web Client"
+}
+
+cloud "Global Load Balancer" as GLB #D1C4E9 {
+    component "DNS-based Routing"
+}
+
+rectangle "API Gateway" as APIGateway #B39DDB {
+    component "Authentication"
+    component "Rate Limiting"
+    component "Request Routing"
+}
+
+rectangle "Group Chat Service Cluster" as GroupChatService #A5D6A7 {
+    component "Group Creation"
+    component "Membership Management"
+    component "Permission Control"
+    component "Message Handler"
+    component "File Sharing"
+    component "Real-time Updates"
+}
+
+rectangle "Supporting Services" as SupportingServices #FFE0B2 {
+    component "Notification Service"
+    component "Search Service"
+    component "Analytics Service"
+    component "Content Moderation"
+}
+
+database "Data Storage" as DataStorage #FFCDD2 {
+    component "Cassandra Cluster" as CC
+    note right of CC : Sharding Key: group_id
+    component "MongoDB Cluster" as MC
+    note right of MC : Sharding Key: user_id
+    component "PostgreSQL Cluster" as PC
+    note right of PC : For transactional data
+}
+
+queue "Message Queue" as MessageQueue #F8BBD0 {
+    component "Kafka Cluster"
+}
+
+cloud "Content Delivery Network" as CDN #B2EBF2 {
+    component "Edge Servers"
+}
+
+database "Caching Layer" as CacheLayer #E0F7FA {
+    component "Redis Cluster"
+    note right: Key: group_id:message_id
+}
+
+rectangle "Monitoring & Logging" as MonitoringLogging #DCEDC8 {
+    component "Prometheus"
+    component "ELK Stack"
+}
+
+ClientLayer --> GLB : HTTPS
+GLB --> APIGateway : Route Requests
+APIGateway --> GroupChatService : Authenticated Requests
+GroupChatService <--> DataStorage : CRUD Operations
+GroupChatService --> MessageQueue : Publish Events
+MessageQueue --> SupportingServices : Consume Events
+GroupChatService <--> CDN : Store/Retrieve Media
+SupportingServices --> DataStorage : Read Data
+ClientLayer <--> CDN : Fetch Media
+GroupChatService <--> CacheLayer : Cache Data
+APIGateway --> MonitoringLogging : Log Requests
+GroupChatService --> MonitoringLogging : Log Events
+SupportingServices --> MonitoringLogging : Log Activities
+
+note right of GroupChatService
+  Enhanced features:
+  - Scalable microservices architecture
+  - Real-time messaging with WebSocket
+  - End-to-end encryption
+  - Multi-device sync
+  - Message threading and reactions
+  - Rich media sharing (images, videos, files)
+  - Group voice and video calls
+end note
+
+note right of SupportingServices
+  - Push notifications with FCM/APNS
+  - Elasticsearch for full-text search
+  - Machine learning for content recommendations
+  - Automated content moderation
+end note
+
+note bottom of DataStorage
+  - Multi-region deployment
+  - Automatic failover
+  - Regular backups
+  - Data encryption at rest
+end note
+
+note right of CacheLayer
+  - Distributed caching
+  - Cache invalidation strategy
+  - Separate caches for different data types
+end note
+
+@enduml
diff --git a/telegram/uml_diagrams/telegram_message_prioritization_and_delivery_system_with_load_balancing.puml b/telegram/uml_diagrams/telegram_message_prioritization_and_delivery_system_with_load_balancing.puml
new file mode 100644
--- /dev/null
+++ ./telegram/uml_diagrams/telegram_message_prioritization_and_delivery_system_with_load_balancing.puml
@@ -0,0 +1,147 @@
+@startuml
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #FAFAFA
+skinparam defaultFontName Arial
+skinparam defaultFontSize 21
+skinparam arrowThickness 1.5
+skinparam noteFontSize 21 
+
+title Telegram-like System Architecture with Read/Write Splitting and Sharding
+
+rectangle "Client Side" as ClientSide #E0F7FA {
+  component "Sender's Client" as SenderClient #4DD0E1
+  component "Receiver's Client" as ReceiverClient #4DD0E1
+}
+
+rectangle "Frontend Services" as FrontendServices #FFF3E0 {
+  component "Load Balancer" as LoadBalancer #FFB74D
+  component "Service Instance" as ServiceInstance #FFB74D
+}
+
+rectangle "Backend Services" as BackendServices #E8F5E9 {
+  rectangle "Authorization Service" as AuthService #A5D6A7 {
+    component "Authorization & Permissions" as AuthPermissions #81C784
+  }
+  
+  rectangle "Message System" as MessageSystem #FFCDD2 {
+    component "Kafka (Message Queue)" as KafkaQueue #EF9A9A
+    component "Kafka (Pub/Sub Model)" as KafkaPubSub #EF9A9A
+    component "Message Worker" as MessageWorker #EF9A9A
+  }
+  
+  rectangle "Notification Service" as NotificationService #B3E5FC {
+    component "Notification Handler" as NotificationHandler #4FC3F7
+    component "Push Notification Service" as PushNotificationService #4FC3F7
+  }
+}
+
+rectangle "Database System" as DatabaseSystem #E1BEE7 {
+  rectangle "MongoDB Cluster" as MongoDBCluster #D1C4E9 {
+    component "Config Servers" as ConfigServers #B39DDB {
+      component "Metadata" as Metadata #9575CD
+    }
+    rectangle "Shard" as Shard #D1C4E9 {
+      component "Primary" as Primary #9575CD {
+        component "Write Operations" as WriteOps #7E57C2
+      }
+      component "Secondary 1" as Secondary1 #9575CD {
+        component "Read Operations" as ReadOps1 #7E57C2
+      }
+      component "Secondary 2" as Secondary2 #9575CD {
+        component "Read Operations" as ReadOps2 #7E57C2
+      }
+    }
+    component "MongoDB Router (mongos)" as MongoRouter #B39DDB
+  }
+  component "Redis Cache" as RedisCache #F8BBD0 {
+    component "Key-Value Store" as KeyValueStore #F48FB1
+  }
+}
+
+rectangle "Monitoring & Logging" as MonitoringLogging #CFD8DC {
+  component "Monitoring" as Monitoring #B0BEC5
+  component "Logging" as Logging #B0BEC5
+}
+
+' 消息流
+SenderClient -[#1ABC9C,thickness=2]-> LoadBalancer: <back:#FFFFFF><color:#1ABC9C>1. Send Message</color></back>
+LoadBalancer -[#1ABC9C,thickness=2]-> ServiceInstance
+ServiceInstance -[#1ABC9C,thickness=2]-> AuthPermissions: <back:#FFFFFF><color:#1ABC9C>2. Check Permission</color></back>
+AuthPermissions -[#1ABC9C,thickness=2]-> KafkaQueue: <back:#FFFFFF><color:#1ABC9C>3. Authorized Message</color></back>
+KafkaQueue -[#1ABC9C,thickness=2]-> MessageWorker: <back:#FFFFFF><color:#1ABC9C>4. Process Message</color></back>
+MessageWorker -[#1ABC9C,thickness=2]-> MongoRouter: <back:#FFFFFF><color:#1ABC9C>5. Store Message</color></back>
+MongoRouter -[#1ABC9C,thickness=2]-> Primary: <back:#FFFFFF><color:#1ABC9C>6. Write to Shard</color></back>
+MessageWorker -[#1ABC9C,thickness=2]-> RedisCache: <back:#FFFFFF><color:#1ABC9C>7. Update Cache</color></back>
+MessageWorker -[#1ABC9C,thickness=2]-> KafkaPubSub: <back:#FFFFFF><color:#1ABC9C>8. Publish Message Event</color></back>
+KafkaPubSub -[#1ABC9C,thickness=2]-> NotificationHandler: <back:#FFFFFF><color:#1ABC9C>9. Trigger Notification</color></back>
+NotificationHandler -[#1ABC9C,thickness=2]-> PushNotificationService: <back:#FFFFFF><color:#1ABC9C>10. Send Push Notification</color></back>
+PushNotificationService -[#1ABC9C,thickness=2]-> ReceiverClient: <back:#FFFFFF><color:#1ABC9C>11. Deliver Notification</color></back>
+
+' 读取消息流
+ReceiverClient -[#F39C12,thickness=2]-> LoadBalancer: <back:#FFFFFF><color:#F39C12>12. Request to Read Message</color></back>
+ServiceInstance -[#F39C12,thickness=2]-> RedisCache: <back:#FFFFFF><color:#F39C12>13. Check Cache</color></back>
+RedisCache -[#F39C12,thickness=2]-> ServiceInstance: <back:#FFFFFF><color:#F39C12>14. Return Cached Message (if available)</color></back>
+ServiceInstance -[#F39C12,thickness=2]-> MongoRouter: <back:#FFFFFF><color:#F39C12>15. Fetch Message (if not in cache)</color></back>
+MongoRouter -[#F39C12,thickness=2]-> Secondary1: <back:#FFFFFF><color:#F39C12>16. Read from Shard</color></back>
+MongoRouter -[#F39C12,thickness=2]-> ServiceInstance: <back:#FFFFFF><color:#F39C12>17. Return Message</color></back>
+ServiceInstance -[#F39C12,thickness=2]-> ReceiverClient: <back:#FFFFFF><color:#F39C12>18. Deliver Message</color></back>
+
+' 复制
+Primary -[#8E44AD,thickness=2]-> Secondary1: <back:#FFFFFF><color:#8E44AD>Replicate</color></back>
+Primary -[#8E44AD,thickness=2]-> Secondary2: <back:#FFFFFF><color:#8E44AD>Replicate</color></back>
+
+' 配置服务器
+ConfigServers -[#27AE60,thickness=2]-> MongoRouter: <back:#FFFFFF><color:#27AE60>Provide Cluster Metadata</color></back>
+
+' 日志和监控
+ServiceInstance -[#95A5A6,thickness=2]-> MonitoringLogging: <back:#FFFFFF><color:#95A5A6>Log Events</color></back>
+
+note top of MongoRouter
+  Sharding Key: user_id
+  Improves query performance
+  and scalability
+end note
+
+note right of RedisCache
+  Key: user_id:message_id
+  Reduces database load
+  and improves read speed
+end note
+
+note bottom of KafkaQueue
+  Ensures reliable message
+  delivery and scalability
+end note
+
+note bottom of LoadBalancer
+  Potential bottleneck
+  Consider using DNS round-robin
+  or hardware load balancing for
+  higher throughput
+end note
+
+note right of AuthPermissions
+  Cache frequently used permissions
+  to reduce latency and database load
+end note
+
+note bottom of MessageWorker
+  Optimize for parallel processing
+  Consider using a worker pool
+  to handle high message volumes
+end note
+
+note top of MongoDBCluster
+  Monitor replication lag
+  Implement read preferences
+  based on consistency requirements
+end note
+
+note bottom of PushNotificationService
+  Implement retry mechanism and
+  circuit breaker for reliability
+end note
+
+@enduml
diff --git a/test.puml b/test.puml
new file mode 100644
--- /dev/null
+++ ./test.puml
@@ -0,0 +1,2 @@
+@startuml
+@enduml
diff --git a/to_ask b/to_ask
new file mode 100644
--- /dev/null
+++ ./to_ask
@@ -0,0 +1,11 @@
+帮我在当前项目下做以下几件事情, 帮我根据事情的轻重缓急来决定先做什么. 假设我的面试很快就要开始了.
+    1: 在当前的项目文件夹下, 帮我看看有没有什么遗漏的设计, 对于常见的系统设计面试. 
+        如果有相似的或者接近的或者是相关的现有设计, 如果复杂度不足以应对系统设计面试, 就扩展现有设计，而不是新建一个设计.
+    2: 对于puml有任何修改或者生成, 要马上验证有没有语法错误, 能不能正确render.
+    3: 检查一下当前的所有设计, 看看有没有重复的或者重叠的.
+    4: 检查一下已有设计里面有需要加强优化, 或者还不完整的？ 
+    5: 检查一下已有设计里面是否有过于复杂, 需要分拆的?
+    6: 检查一下已有设计里面有含有中文需要翻译成英文的.
+    7: 帮我逐个检查文件名是否能够如实反映文件的内容, 我比较喜欢长一点的文件名，要能够完整的描述这个设计/文件里面cover了哪些东西, 我的目标是快速地用关键词来搜索定位到我想要的文件. 相当于给每个文件打label. 生成修改这些文件名的命令, 把这些命令汇集在一起.
+    8: 帮我更新 map.txt 文件, 来把文件放到对应的label下, 如果需要, 可以创建新的label, 或者修改删除现有label. 一个文件可以有多个label.
+
diff --git a/todo b/todo
new file mode 100644
--- /dev/null
+++ ./todo
@@ -0,0 +1,10 @@
+帮我在当前项目下做以下几件事情, 帮我根据事情的轻重缓急来决定先做什么. 假设我的面试很快就要开始了. 我希望.cursorrules文件的优先级最高.
+    1: 在当前的项目文件夹下, 帮我看看有没有什么遗漏的设计, 对于常见的系统设计面试. 
+        如果有相似的或者接近的或者是相关的现有设计, 如果复杂度不足以应对系统设计面试, 就扩展现有设计，而不是新建一个设计.
+    2: 检查一下当前的所有设计, 看看有没有重复的或者重叠的.
+    3: 检查一下已有设计里面有需要加强优化, 或者还不完整的？ 
+    4: 检查一下已有设计里面是否有过于复杂, 需要分拆的?
+    5: 检查一下已有设计里面有含有中文需要翻译成英文的.
+    6: 在不遗失信息的前提下, .cursorrules文件还能不能再优化, 精简? 还有帮我检查一下, 里面有没有互相矛盾的地方？
+    7: 帮我逐个检查文件名是否能够如实反映文件的内容, 我比较喜欢长一点的文件名，要能够完整的描述这个设计/文件里面cover了哪些东西, 我的目标是快速地用关键词来搜索定位到我想要的文件. 相当于给每个文件打label. 生成修改这些文件名的命令, 把这些命令汇集在一起.
+    8: 帮我更新 map.txt 文件, 来把文件放到对应的label下, 如果需要, 可以创建新的label, 或者修改删除现有label. 一个文件可以有多个label.
diff --git a/twitter/fetch_new_tweets_detailed_architecture.puml b/twitter/fetch_new_tweets_detailed_architecture.puml
new file mode 100644
--- /dev/null
+++ ./twitter/fetch_new_tweets_detailed_architecture.puml
@@ -0,0 +1,102 @@
+@startuml
+allowmixing
+
+skinparam backgroundColor #FEFEFE
+skinparam handwritten false
+skinparam shadowing false
+
+skinparam rectangle {
+    BackgroundColor E3F2FD
+    BorderColor 2196F3
+    BorderThickness 2
+    FontName Arial
+    FontSize 14
+    FontColor 000000
+    RoundCorner 10
+}
+
+skinparam note {
+    BackgroundColor FFF9C4
+    BorderColor FBC02D
+    FontSize 16
+    FontColor 000000
+}
+
+skinparam arrow {
+    Color 2196F3
+    Thickness 1.5
+}
+
+skinparam title {
+    FontSize 24
+    FontColor 000000
+    FontStyle bold
+}
+
+title Detailed Fetch New Tweets Process
+
+together {
+rectangle "Client" as Client #B3E5FC
+}
+
+package "Core Services" as Core {
+    package "Load Balancer & Web Server" as LBWS {
+        rectangle "Load Balancer" as LB
+        rectangle "Web Server" as WS
+    }
+    package "Application & Monitoring Services" as AMS {
+        rectangle "Application Server" as AS
+    }
+}
+
+package "Timeline Update Service" as TUS {
+    rectangle "Request Handler" as RH
+    rectangle "Async Queue\n(Kafka/RabbitMQ)" as AQ
+}
+
+package "Timeline Workers" as TW {
+    rectangle "Fetch New Tweets" as FNT
+    rectangle "Cache Query" as CQ
+    rectangle "Database Query" as DBQ
+    rectangle "De-duplicate & Sort" as DDS
+    rectangle "Cache Update" as CU
+    rectangle "Parallel Processing" as PP
+}
+
+package "Data Persistence Layer" as DPS {
+    package "Caching (Redis Cluster)" as Cache {
+        rectangle "Tweet Cache" as TC
+    }
+    package "Databases (Sharded & Read-Replica)" as DB {
+        rectangle "Tweet Data" as TD
+    }
+}
+
+Client -down-> LBWS : 1. Requests Timeline Update
+LBWS -down-> AMS : 2. Routes Request
+AMS -down-> TUS : 3. Passes Request to TUS
+RH -down-> AQ : 4. Queues Fetch New Tweets Task
+AQ -down-> TW : 5. Processes Tasks
+FNT -down-> CQ : 6. Query Tweet Cache
+CQ -right-> TC : 7. Fetches from Cache
+CQ -down-> DBQ : 8. Cache Miss, Query Database
+DBQ -down-> TD : 9. Fetches Tweet Data
+DBQ -right-> DDS : 10. De-duplicate & Sort Tweets
+DDS -right-> CU : 11. Updates Tweet Cache
+CU -right-> TC : 12. Stores Tweets in Cache
+PP -[hidden]right-> FNT
+
+note right of FNT
+  <b>Performance Optimization</b>:
+  • Prioritize cache queries to reduce database access
+  • Optimize database queries (indexing, caching)
+  • Implement multi-threaded parallel processing
+  • Utilize caching to speed up read operations
+end note
+
+note right of PP
+  Parallel processing of timeline updates for multiple users,
+  improving overall throughput
+end note
+
+@enduml
diff --git a/twitter/timeline_update_service_detailed_architecture.puml b/twitter/timeline_update_service_detailed_architecture.puml
new file mode 100644
--- /dev/null
+++ ./twitter/timeline_update_service_detailed_architecture.puml
@@ -0,0 +1,87 @@
+@startuml Timeline Update Service Detailed Architecture
+
+!define RECTANGLE rectangle
+allowmixing
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 18
+skinparam roundCorner 10
+skinparam componentStyle uml2
+
+skinparam component {
+  BorderColor #3C7FC0
+  BackgroundColor #D4E5F7
+}
+
+skinparam note {
+  BackgroundColor #FFFACD
+  BorderColor #DAA520
+  FontSize 18
+}
+
+title Timeline Update Service Detailed Architecture
+
+RECTANGLE "Client" as Client #LightBlue
+
+RECTANGLE "Core Services" as CoreServices {
+  component "Load Balancer" as LB #87CEFA
+  component "Web Server" as WS #87CEFA
+}
+
+RECTANGLE "Application & Monitoring Services" as AMS {
+  component "Application Server" as AS #98FB98
+  component "Service Circuit Breaker" as SCB #98FB98
+  component "Service Auto-Scaling" as SAS #98FB98
+}
+
+RECTANGLE "Timeline Update Service" as TUS {
+  component "Request Handler" as RH #FFA07A
+  component "Async Queue (Kafka/RabbitMQ)" as AQ #FFA07A
+}
+
+rectangle "Timeline Workers" as TW {
+  component "Fetch New Tweets" as FNT #FFD700
+  component "Update Timeline" as UT #FFD700
+  component "Notify Users" as NU #FFD700
+  component "Preload Content" as PC #FFD700
+}
+
+rectangle "Data Persistence Layer" as DPL {
+  rectangle "Caching (Redis Cluster)" as Cache {
+    component "Timeline Cache" as TC #F0E68C
+    component "Edge Caching (CDN)" as EC #F0E68C
+  }
+  rectangle "Databases (Sharded & Read-Replica)" as DB {
+    component "User Timelines" as UTDB #DDA0DD
+    component "Tweet Data" as TD #DDA0DD
+  }
+}
+
+Client -down-> LB : 1. Requests Timeline Update
+LB -down-> WS : 2. Routes Request
+WS -down-> AS : 3. Passes Request
+AS -down-> RH : 4. Handles Request
+RH -down-> AQ : 5. Queues Timeline Update Tasks
+AQ -down-> FNT : 6. Processes Tasks
+FNT -right-> TD : 7. Fetches New Tweets
+FNT -down-> TC : 8. Updates Timeline Cache
+UT -up-> UTDB : 9. Updates User Timelines
+NU -up-> Client : 10. Notifies Users
+TC -right-> EC : 11. Uses Edge Caching for Speed
+
+note right of RH
+  Performance Optimization:
+  - Use async queue for timeline update tasks
+  - Increase worker threads for timeline updates
+  - Utilize Redis for timeline data caching
+  - Use CDN for static resource caching
+  - Implement database sharding and read replicas
+end note
+
+note right of PC
+  Preload content likely to be viewed,
+  reducing user wait time
+end note
+
+@enduml
diff --git a/twitter/tweet_system_detailed_architecture.puml b/twitter/tweet_system_detailed_architecture.puml
new file mode 100644
--- /dev/null
+++ ./twitter/tweet_system_detailed_architecture.puml
@@ -0,0 +1,121 @@
+@startuml
+
+!pragma layout dot
+skinparam backgroundColor #E6E6FA
+skinparam defaultFontName "Arial"
+skinparam defaultFontSize 16
+skinparam note {
+  BackgroundColor #FFFACD
+  BorderColor #DAA520
+  FontSize 18
+  FontName "Arial"
+}
+
+allowmixing
+
+title Enhanced Twitter System Architecture with Advanced Scalability and Performance Strategies
+
+rectangle "Client" as Client #FFEFD5
+
+rectangle "Core Services" as Core #E0FFFF {
+    rectangle "Load Balancer & API Gateway" as LBAG #98FB98 {
+        component "Load Balancer" as LB
+        component "API Gateway" as AG
+    }
+    rectangle "Application & Monitoring Services" as AMS #87CEFA {
+        component "Application Server" as AS
+        component "Service Circuit Breaker" as SCB
+        component "Service Auto-Scaling" as SAS
+        component "Monitoring & Logging" as ML
+    }
+}
+
+rectangle "Tweet Processing Service" as TPS #FFB6C1 {
+    rectangle "Tweet & Comment Handlers" as TCH #FFA07A {
+        component "Text Processing" as TP
+        component "Sensitive Word Filtering" as SWF
+        component "Media Processing" as MP
+    }
+    rectangle "Async Processing Queue" as APQ #F0E68C {
+        component "Kafka Cluster" as KC
+    }
+    rectangle "Workers for Processing" as Workers #98FB98 {
+        component "Task Processing" as TProc
+        component "Database Update" as DBU
+        component "Cache Update" as CU
+    }
+}
+
+rectangle "Supporting Services" as SS #DDA0DD {
+    component "Search Service" as SSrv
+    component "Notification Service" as NSrv
+    component "Timeline Update Service" as TUS
+    component "Content Delivery Network" as CDN
+    component "User Authentication Service" as UAS
+}
+
+rectangle "Data Persistence Layer" as DPS #B0E0E6 {
+    rectangle "Caching" as Cache #FAFAD2 {
+        component "Redis Cluster" as RC
+        component "Edge Caching (CDN)" as EC
+    }
+    rectangle "Databases" as DB #F0FFF0 {
+        component "Tweet & Comment Data (NoSQL)" as TCD
+        component "User Data (SQL)" as UD
+        component "Data Partitioning & Sharding" as DPSH
+    }
+}
+
+Client -[#1E90FF,thickness=2]down-> LBAG : <back:#FFFFFF><color:#1E90FF>1. Sends Request</color></back>
+LBAG -[#32CD32,thickness=2]down-> AMS : <back:#FFFFFF><color:#32CD32>2. Routes Request</color></back>
+AMS -[#FF4500,thickness=2]right-> TCH : <back:#FFFFFF><color:#FF4500>3. Handles Tweet/Comment</color></back>
+TCH -[#8A2BE2,thickness=2]down-> APQ : <back:#FFFFFF><color:#8A2BE2>4. Queues Tasks</color></back>
+APQ -[#FF69B4,thickness=2]down-> Workers : <back:#FFFFFF><color:#FF69B4>5. Processes Tasks</color></back>
+Workers -[#20B2AA,thickness=2]down-> DPS : <back:#FFFFFF><color:#20B2AA>6. Updates Data</color></back>
+Workers -[#DAA520,thickness=2]left-> SSrv : <back:#FFFFFF><color:#DAA520>7. Updates Search</color></back>
+Workers -[#FF6347,thickness=2]left-> NSrv : <back:#FFFFFF><color:#FF6347>8. Notifies</color></back>
+TUS -[#4682B4,thickness=2]down-> DPS : <back:#FFFFFF><color:#4682B4>9. Updates Timelines</color></back>
+Client -[#9370DB,thickness=2]left-> LBAG : <back:#FFFFFF><color:#9370DB>10. Requests Timeline</color></back>
+AMS -[#3CB371,thickness=2]down-> TUS : <back:#FFFFFF><color:#3CB371>11. Fetches Timeline Data</color></back>
+TUS -[#CD5C5C,thickness=2]left-> Cache : <back:#FFFFFF><color:#CD5C5C>12. Checks Cache</color></back>
+Cache -[#4169E1,thickness=2]down-> TUS : <back:#FFFFFF><color:#4169E1>13. Returns Cached Data</color></back>
+TUS -[#FF8C00,thickness=2]up-> AMS : <back:#FFFFFF><color:#FF8C00>14. Returns Timeline Data</color></back>
+AMS -[#2E8B57,thickness=2]up-> LBAG : <back:#FFFFFF><color:#2E8B57>15. Sends Back Data</color></back>
+LBAG -[#8B008B,thickness=2]up-> Client : <back:#FFFFFF><color:#8B008B>16. Returns Data/Updates Timeline</color></back>
+
+note right of LBAG
+  Load Balancer & API Gateway:
+  - Use HAProxy or NGINX for load balancing
+  - Implement rate limiting and request throttling
+  - API versioning and documentation
+end note
+
+note right of AMS
+  Application & Monitoring:
+  - Implement distributed tracing (e.g., Jaeger)
+  - Use ELK stack for centralized logging
+  - Set up Prometheus & Grafana for monitoring
+end note
+
+note right of TPS
+  Tweet Processing:
+  - Implement content moderation AI
+  - Use Apache Flink for real-time stream processing
+  - Optimize media processing with FFmpeg
+end note
+
+note right of SS
+  Supporting Services:
+  - Implement Elasticsearch for efficient search
+  - Use WebSocket for real-time notifications
+  - Implement OAuth 2.0 and JWT for authentication
+end note
+
+note right of DPS
+  Data Persistence:
+  - Use Cassandra for tweet storage
+  - Implement read replicas for high availability
+  - Use database connection pooling
+end note
+
+@enduml
diff --git a/twitter/twitter_comment_system_with_moderation_threading_and_real_time_updates.puml b/twitter/twitter_comment_system_with_moderation_threading_and_real_time_updates.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_comment_system_with_moderation_threading_and_real_time_updates.puml
@@ -0,0 +1,101 @@
+@startuml
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #E8E8E8
+
+rectangle "Client Layer" as ClientLayer #FFFACD {
+    component "Web App" as WebApp #87CEFA
+    component "Mobile App" as MobileApp #98FB98
+}
+
+rectangle "API Gateway Layer" as GatewayLayer #FFE4B5 {
+    component "API Gateway" as APIGateway #F0E68C
+    component "Load Balancer" as LoadBalancer #DDA0DD
+}
+
+rectangle "Service Layer" as ServiceLayer #E6E6FA {
+    component "User Service" as UserService #ADD8E6
+    component "Tweet Service" as TweetService #90EE90
+    component "Comment Service" as CommentService #FFA07A
+    component "Notification Service" as NotificationService #F0E68C
+    component "Search Service" as SearchService #D8BFD8
+}
+
+rectangle "Caching Layer" as CachingLayer #FFE4E1 {
+    component "Redis Cache" as CacheService #FF6347
+    component "Rate Limiter" as RateLimiter #4682B4
+}
+
+rectangle "Database Layer" as DatabaseLayer #F0FFF0 {
+    component "PostgreSQL" as SQLDatabase #6495ED
+    component "Cassandra" as NoSQLDatabase #9ACD32
+    component "Elasticsearch" as ElasticsearchDB #FFA500
+}
+
+rectangle "Messaging Layer" as MessagingLayer #F5DEB3 {
+    component "Kafka" as Kafka #FF4500
+}
+
+rectangle "Monitoring Layer" as MonitoringLayer #E0FFFF {
+    component "Prometheus" as Prometheus #B0C4DE
+    component "ELK Stack" as ELKStack #20B2AA
+}
+
+ClientLayer -[#000000,thickness=2]down-> GatewayLayer : <back:#FFFFFF><color:#000000>1. HTTP/HTTPS Requests</color></back>
+
+GatewayLayer -[#0000FF,thickness=2]down-> ServiceLayer : <back:#FFFFFF><color:#0000FF>2. Route Requests</color></back>
+
+ServiceLayer -[#008000,thickness=2]right-> CachingLayer : <back:#FFFFFF><color:#008000>3. Check Cache/Rate Limit</color></back>
+
+CachingLayer -[#FF0000,thickness=2]down-> DatabaseLayer : <back:#FFFFFF><color:#FF0000>4. Cache Miss</color></back>
+
+ServiceLayer -[#800080,thickness=2]down-> DatabaseLayer : <back:#FFFFFF><color:#800080>5. CRUD Operations</color></back>
+
+ServiceLayer -[#FFA500,thickness=2]down-> MessagingLayer : <back:#FFFFFF><color:#FFA500>6. Publish Events</color></back>
+
+MessagingLayer -[#8B4513,thickness=2]up-> ServiceLayer : <back:#FFFFFF><color:#8B4513>7. Consume Events (Async)</color></back>
+
+MonitoringLayer -[#4B0082,thickness=2]up-> ServiceLayer : <back:#FFFFFF><color:#4B0082>8. Monitor & Log</color></back>
+
+ServiceLayer -[#006400,thickness=2]up-> GatewayLayer : <back:#FFFFFF><color:#006400>9. Return Response</color></back>
+
+GatewayLayer -[#8B0000,thickness=2]up-> ClientLayer : <back:#FFFFFF><color:#8B0000>10. Send Response to Client</color></back>
+
+note right of ClientLayer
+  Supports web and mobile clients
+end note
+
+note right of APIGateway
+  Handles authentication, rate limiting,
+  and request routing
+end note
+
+note right of CommentService
+  Manages CRUD operations for comments,
+  integrates with other services
+end note
+
+note top of CacheService
+  key: tweet_id
+  value: {comments, metadata}
+end note
+
+note top of NoSQLDatabase
+  Stores high-volume comment data
+end note
+
+note top of Kafka
+  Enables asynchronous communication
+  between services
+end note
+
+note right of Prometheus
+  Collects metrics for real-time
+  monitoring and alerting
+end note
+
+note right of ELKStack
+  Centralized logging and analysis
+end note
+
+@enduml
diff --git a/twitter/twitter_content_publishing_management_and_analytics_system.puml b/twitter/twitter_content_publishing_management_and_analytics_system.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_content_publishing_management_and_analytics_system.puml
@@ -0,0 +1,101 @@
+@startuml
+
+!pragma layout dot
+skinparam backgroundColor #F0F0F0
+allowmixing
+
+' 增加字体大小
+skinparam defaultFontSize 18
+skinparam noteFontSize 18
+skinparam titleFontSize 24
+
+title Twitter Content Publishing and Management System
+
+rectangle "Client Applications" as Client #E6F3FF
+rectangle "API Gateway" as AG #D6EAF8
+rectangle "Load Balancer" as LB #D6EAF8
+
+rectangle "Content Service" as CS #E8F8F5 {
+    component "Tweet Creation" as TC #ABEBC6
+    component "Tweet Editing" as TE #ABEBC6
+    component "Tweet Deletion" as TD #ABEBC6
+    component "Media Upload" as MU #ABEBC6
+    component "Content Cache" as CC #F9E79F
+}
+
+rectangle "Moderation Service" as MS #FDEDEC {
+    component "Content Moderation" as CM #F5B7B1
+    component "Report Handling" as RH #F5B7B1
+    component "Automated Filtering" as AF #F5B7B1
+}
+
+rectangle "User Service" as US #E8DAEF {
+    component "Authentication" as Auth #D2B4DE
+    component "Authorization" as Authz #D2B4DE
+    component "User Profile" as UP #D2B4DE
+}
+
+rectangle "Analytics Service" as AS #FCF3CF {
+    component "Engagement Tracking" as ET #F9E79F
+    component "Trend Analysis" as TA #F9E79F
+}
+
+rectangle "Data Persistence Layer" as DPL #D5D8DC {
+    component "Tweet Data (DB)" as TDB #ABB2B9
+    component "Media Data (Object Storage)" as MDB #ABB2B9
+    component "User Data (DB)" as UDB #ABB2B9
+    component "Analytics Data (Data Warehouse)" as ADB #ABB2B9
+}
+
+rectangle "Message Queue" as MQ #F8C471
+rectangle "CDN" as CDN #AED6F1
+
+Client -[#1E90FF,thickness=2]down-> AG : <back:#FFFFFF><color:#1E90FF>1. API Requests</color></back>
+AG -[#1E90FF,thickness=2]down-> LB : <back:#FFFFFF><color:#1E90FF>2. Route Requests</color></back>
+LB -[#1E90FF,thickness=2]down-> CS : <back:#FFFFFF><color:#1E90FF>3. Distribute Load</color></back>
+LB -[#1E90FF,thickness=2]down-> MS : <back:#FFFFFF><color:#1E90FF>3. Distribute Load</color></back>
+LB -[#1E90FF,thickness=2]down-> US : <back:#FFFFFF><color:#1E90FF>3. Distribute Load</color></back>
+
+CS -[#2ECC71,thickness=2]right-> US : <back:#FFFFFF><color:#2ECC71>4. Authenticate/Authorize</color></back>
+CS -[#2ECC71,thickness=2]down-> CC : <back:#FFFFFF><color:#2ECC71>5. Check Cache</color></back>
+CC -[#2ECC71,thickness=2]down-> TDB : <back:#FFFFFF><color:#2ECC71>6. Cache Miss</color></back>
+CS -[#2ECC71,thickness=2]down-> MQ : <back:#FFFFFF><color:#2ECC71>7. Publish Events</color></back>
+MQ -[#2ECC71,thickness=2]right-> MS : <back:#FFFFFF><color:#2ECC71>8. Consume Moderation Events</color></back>
+MQ -[#2ECC71,thickness=2]right-> AS : <back:#FFFFFF><color:#2ECC71>8. Consume Analytics Events</color></back>
+
+MS -[#E74C3C,thickness=2]down-> AF : <back:#FFFFFF><color:#E74C3C>9. Auto-filter Content</color></back>
+MS -[#E74C3C,thickness=2]down-> CM : <back:#FFFFFF><color:#E74C3C>10. Human Moderation</color></back>
+MS -[#E74C3C,thickness=2]right-> RH : <back:#FFFFFF><color:#E74C3C>11. Handle Reports</color></back>
+
+AS -[#F39C12,thickness=2]down-> ET : <back:#FFFFFF><color:#F39C12>12. Track Engagement</color></back>
+AS -[#F39C12,thickness=2]right-> TA : <back:#FFFFFF><color:#F39C12>13. Analyze Trends</color></back>
+
+MU -[#3498DB,thickness=2]down-> CDN : <back:#FFFFFF><color:#3498DB>14. Store Media</color></back>
+CDN -[#3498DB,thickness=2]down-> MDB : <back:#FFFFFF><color:#3498DB>15. Backup Media</color></back>
+
+note right of CS
+  <b>Performance Optimization:</b>
+  - Implement distributed caching (e.g., Redis)
+  - Use read replicas for database
+  - Implement database sharding for scalability
+end note
+
+note bottom of MS
+  <b>Moderation Enhancements:</b>
+  - Implement ML-based content classification
+  - Set up real-time alerting for sensitive content
+end note
+
+note right of AS
+  <b>Analytics Improvements:</b>
+  - Use stream processing for real-time analytics
+  - Implement data lake for long-term storage and analysis
+end note
+
+note bottom of DPL
+  <b>Data Management:</b>
+  - Implement data retention policies
+  - Set up regular backups and disaster recovery
+end note
+
+@enduml
diff --git a/twitter/twitter_database_schema.puml b/twitter/twitter_database_schema.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_database_schema.puml
@@ -0,0 +1,293 @@
+@startuml
+
+!define MAIN_ENTITY #E2F0D9
+!define SECONDARY_ENTITY #FFF2CC
+!define ASSOCIATION_ENTITY #DEEBF7
+
+' Skin parameters
+skinparam {
+  backgroundColor #F0F0F0
+  handwritten false
+  shadowing false
+  class {
+    BackgroundColor #FFFFFF
+    BorderColor #333333
+    ArrowColor #333333
+    FontName Arial
+    FontSize 16
+    AttributeFontSize 14
+  }
+  title {
+    FontSize 20
+    FontStyle bold
+  }
+  legend {
+    FontSize 14
+  }
+  note {
+    FontSize 16
+    FontStyle bold
+  }
+}
+
+title Twitter Database Schema - Optimized Layout
+
+legend right
+  |<#E2F0D9>| Main Entity |
+  |<#FFF2CC>| Secondary Entity |
+  |<#DEEBF7>| Association Entity |
+  |= | One-to-Many |
+  |== | Many-to-Many |
+  |..| Weak Relationship |
+endlegend
+
+together {
+  rectangle "Core" {
+    entity "User" as user MAIN_ENTITY {
+      +user_id : int <<PK>>
+      --
+      username : varchar(15)
+      email : varchar(255)
+      password : varchar(255)
+      bio : varchar(160)
+      join_date : datetime
+      ..
+      Indexes: username, email
+    }
+    
+    entity "Tweet" as tweet MAIN_ENTITY {
+      +tweet_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      text : varchar(280)
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: tweet_id
+      Indexes: user_id, created_at
+    }
+  }
+}
+
+together {
+  rectangle "User-Related" {
+    entity "UserStatus" as userstatus SECONDARY_ENTITY {
+      +user_id : int <<PK, FK>>
+      --
+      last_login : datetime
+      is_online : boolean
+      ..
+      Storage: Redis
+    }
+
+    entity "Followers" as followers SECONDARY_ENTITY {
+      +follow_id : int <<PK>>
+      --
+      follower_id : int <<FK>>
+      followed_id : int <<FK>>
+      follow_date : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: followed_id
+      Indexes: follower_id, followed_id
+    }
+  }
+}
+
+together {
+  rectangle "Tweet-Related" {
+    entity "Comments" as comments SECONDARY_ENTITY {
+      +comment_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      tweet_id : int <<FK>>
+      text : varchar(280)
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: tweet_id
+      Indexes: user_id
+    }
+
+    entity "Likes" as likes ASSOCIATION_ENTITY {
+      +like_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      tweet_id : int <<FK>>
+      like_date : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: tweet_id
+      Indexes: user_id
+    }
+
+    entity "Retweets" as retweets ASSOCIATION_ENTITY {
+      +retweet_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      tweet_id : int <<FK>>
+      retweet_date : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: tweet_id
+      Indexes: user_id
+    }
+
+    entity "MediaFiles" as media_files SECONDARY_ENTITY {
+      +media_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      tweet_id : int <<FK>>
+      file_path : varchar(255)
+      file_type : varchar(50)
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: media_id
+      Indexes: user_id, tweet_id
+    }
+  }
+}
+
+together {
+  rectangle "Hashtag & Timeline" {
+    entity "Hashtags" as hashtags SECONDARY_ENTITY {
+      +hashtag_id : int <<PK>>
+      --
+      hashtag_text : varchar(140)
+      ..
+      Indexes: hashtag_text
+    }
+
+    entity "TweetHashtags" as tweet_hashtags ASSOCIATION_ENTITY {
+      tweet_id : int <<FK, PK>>
+      hashtag_id : int <<FK, PK>>
+      ..
+      NoSQL: Yes
+      Shard Key: (tweet_id, hashtag_id)
+    }
+
+    entity "Timeline" as timeline SECONDARY_ENTITY {
+      +timeline_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      tweet_id : int <<FK>>
+      timestamp : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: user_id
+      Indexes: tweet_id
+    }
+  }
+}
+
+together {
+  rectangle "Communication" {
+    entity "Messages" as messages SECONDARY_ENTITY {
+      +message_id : int <<PK>>
+      --
+      sender_id : int <<FK>>
+      receiver_id : int <<FK>>
+      content : text
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: message_id
+      Indexes: sender_id, receiver_id
+    }
+
+    entity "Notifications" as notifications SECONDARY_ENTITY {
+      +notification_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      type : varchar(50)
+      message : text
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: notification_id
+      Indexes: user_id
+    }
+  }
+}
+
+together {
+  rectangle "Logging & Analytics" {
+    entity "ActivityLogs" as activity_logs SECONDARY_ENTITY {
+      +log_id : int <<PK>>
+      --
+      user_id : int <<FK>>
+      action : varchar(255)
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: log_id
+      Indexes: user_id, created_at
+    }
+
+    entity "Reports" as reports SECONDARY_ENTITY {
+      +report_id : int <<PK>>
+      --
+      reporter_id : int <<FK>>
+      reported_id : int <<FK>>
+      reason : text
+      created_at : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: report_id
+      Indexes: reporter_id, reported_id
+    }
+
+    entity "TrendingTopics" as trending_topics SECONDARY_ENTITY {
+      +topic_id : int <<PK>>
+      --
+      topic : varchar(140)
+      tweet_count : int
+      last_updated : datetime
+      ..
+      NoSQL: Yes
+      Shard Key: topic_id
+      Indexes: tweet_count
+    }
+  }
+}
+
+' Relationships
+user "1" -- "1" userstatus : has >
+user "1" -- "*" followers : has >
+user "1" -- "*" messages : sends >
+user "1" -- "*" notifications : receives >
+user "1" -- "*" activity_logs : logs >
+user "1" -- "*" reports : reports >
+user "1" -- "*" tweet : posts >
+user "1" -- "*" comments : writes >
+user "1" -- "*" likes : gives >
+user "1" -- "*" retweets : makes >
+user "1" -- "1" timeline : has >
+user "1" -- "*" media_files : uploads >
+
+tweet "1" -- "*" likes : has >
+tweet "1" -- "*" retweets : has >
+tweet "1" -- "*" comments : has >
+tweet "*" -- "*" hashtags : has >
+tweet "1" -- "*" media_files : includes >
+
+tweet "*" -- "*" timeline : appears in >
+
+hashtags "*" -- "*" trending_topics : may be >
+
+note as N1 #lightblue
+  <size:18>Performance bottlenecks:</size>
+  <size:18>1. User timeline generation
+  2. Tweet search and indexing
+  3. Hashtag trending calculation</size>
+endnote
+
+note as N2 #lightgreen
+  <size:18>Optimization suggestions:</size>
+  <size:18>1. Use caching for user timelines
+  2. Implement sharding for tweets
+  3. Use distributed processing for trending topics</size>
+endnote
+
+@enduml
diff --git a/twitter/twitter_push_notification_service.puml b/twitter/twitter_push_notification_service.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_push_notification_service.puml
@@ -0,0 +1,41 @@
+@startuml
+
+skinparam backgroundColor #D3D3D3
+skinparam defaultFontName "Arial"
+skinparam defaultFontSize 16
+skinparam note {
+    BackgroundColor #FFFFFF
+    BorderColor #000000
+    FontSize 16
+    FontName "Arial"
+}
+
+title Push Notification Service Architecture
+
+rectangle "Client" as Client
+
+rectangle "Notification Service" as NS {
+    rectangle "Notification Queue" as NQ
+    rectangle "Notification Workers" as NW
+    rectangle "WebSocket Server" as WSS
+}
+
+rectangle "Data Persistence Layer" as DPS {
+    rectangle "Notification Data (DB)" as ND
+    rectangle "User Device Data (DB)" as UDD
+}
+
+Client -down-> WSS : Opens WebSocket Connection
+NQ -down-> NW : Processes Notification Task
+NW -down-> UDD : Fetches User Device Info
+NW -down-> WSS : Sends Notification via WebSocket
+NW -down-> ND : Logs Notification
+
+note right of NS
+  **性能优化**:
+  - 使用消息队列处理通知任务
+  - 增加通知处理工作线程
+  - 优化WebSocket服务器性能
+end note
+
+@enduml
diff --git a/twitter/twitter_realtime_communication_service.puml b/twitter/twitter_realtime_communication_service.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_realtime_communication_service.puml
@@ -0,0 +1,109 @@
+@startuml
+
+!pragma layout dot
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam note {
+    BackgroundColor #FFFFFF
+    BorderColor #000000
+    FontSize 12
+}
+
+allowmixing
+
+title Real-time Communication Service Architecture
+
+rectangle "Client Layer" as CL #E6F3FF {
+    component "Client A" as ClientA
+    component "Client B" as ClientB
+}
+
+rectangle "Load Balancer" as LB #D1E8FF
+
+rectangle "Communication Service" as CS #FFE6CC {
+    component "WebSocket Server Cluster" as WSS #FFA07A
+    component "Message Queue (Kafka)" as MQ #98FB98
+    component "Message Processor Cluster" as MP #87CEFA
+}
+
+rectangle "Caching Layer" as CL #FFD700 {
+    component "Redis Cluster" as RC
+}
+
+rectangle "Data Persistence Layer" as DPS #E6E6FA {
+    component "Message Database (Cassandra)" as MD
+    component "User Status Database (Redis)" as USD
+}
+
+rectangle "Monitoring and Logging" as ML #F0E68C {
+    component "ELK Stack" as ELK
+    component "Prometheus" as Prom
+}
+
+ClientA -[#FF6347]-> LB : "1. Connect"
+ClientB -[#FF6347]-> LB : "2. Connect"
+LB -[#FF6347]-> WSS : "3. Route"
+ClientA -[#4169E1]-> LB : "4. Send Message"
+LB -[#4169E1]-> WSS : "5. Route Message"
+WSS -[#4169E1]-> MQ : "6. Queue Message"
+MQ -[#4169E1]-> MP : "7. Process Message"
+MP -[#4169E1]-> MD : "8. Store Message"
+MP -[#32CD32]-> RC : "9. Update Cache"
+MP -[#32CD32]-> USD : "10. Update User Status"
+MP -[#4169E1]-> WSS : "11. Deliver Message"
+WSS -[#4169E1]-> ClientB : "12. Receive Message"
+
+WSS -[#A9A9A9]-> ELK : "Log"
+MP -[#A9A9A9]-> ELK : "Log"
+WSS -[#A9A9A9]-> Prom : "Metrics"
+MP -[#A9A9A9]-> Prom : "Metrics"
+
+note right of LB
+  Load Balancer distributes traffic
+  across WebSocket server instances
+end note
+
+note bottom of WSS
+  WebSocket Server Cluster:
+  - Horizontal scaling for high concurrency
+  - Sticky sessions for connection persistence
+end note
+
+note right of MQ
+  Kafka:
+  - High throughput message queue
+  - Partitioning for parallel processing
+end note
+
+note bottom of MP
+  Message Processor Cluster:
+  - Stateless for easy scaling
+  - Handles message routing and processing
+end note
+
+note right of RC
+  Redis Cluster:
+  - In-memory caching for fast data access
+  - Distributed caching for scalability
+end note
+
+note bottom of MD
+  Cassandra:
+  - Distributed NoSQL database
+  - Optimized for write-heavy workloads
+end note
+
+note bottom of USD
+  Redis for User Status:
+  - Fast read/write operations
+  - TTL support for temporary data
+end note
+
+note right of ML
+  Monitoring and Logging:
+  - Real-time system health monitoring
+  - Log aggregation and analysis
+end note
+
+@enduml
diff --git a/twitter/twitter_recommendation_service_with_ml_personalization_and_real_time_processing.puml b/twitter/twitter_recommendation_service_with_ml_personalization_and_real_time_processing.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_recommendation_service_with_ml_personalization_and_real_time_processing.puml
@@ -0,0 +1,105 @@
+@startuml Twitter_Recommendation_Service_Detailed
+
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #E6F3FF
+skinparam shadowing false
+skinparam RoundCorner 8
+skinparam ArrowColor 454645
+skinparam DefaultFontName Arial
+skinparam DefaultFontSize 20
+
+rectangle "Recommendation Service" as RecommendationService #90EE90 {
+    component "Batch Recommendation Engine" as BRE #D0F0C0
+    component "Real-time Recommendation Engine" as RRE #D0F0C0
+    component "Result Caching" as REC #D0F0C0
+    component "Push Notification Service" as PNS #D0F0C0
+    component "A/B Testing Module" as ABT #D0F0C0
+    component "Personalization Module" as PM #D0F0C0
+}
+
+rectangle "Data Processing" as DataProcessing #FFD700 {
+    component "Data Analysis" as DataAnalysis #FFEC8B
+    component "Machine Learning Pipeline" as MLP #FFEC8B
+    component "Feature Engineering" as FE #FFEC8B
+}
+
+rectangle "Storage" as Storage #87CEFA {
+    component "Cache" as Cache #B0E0E6 {
+        component "Recommendation Cache" as RCache #ADD8E6
+        component "User Profile Cache" as UPCache #ADD8E6
+    }
+    component "Database" as Database #B0E0E6 {
+        component "User Interaction Data" as UID #ADD8E6
+        component "Content Data" as CD #ADD8E6
+        component "User Profile Data" as UPD #ADD8E6
+    }
+}
+
+rectangle "External Services" as ExternalServices #FFA07A {
+    component "API Gateway" as APIGateway #FFB6C1
+    component "User Authentication Service" as UAS #FFB6C1
+    component "Content Moderation Service" as CMS #FFB6C1
+}
+
+rectangle "Monitoring and Logging" as MonitoringLogging #98FB98 {
+    component "Performance Monitoring" as PM #90EE90
+    component "Error Logging" as EL #90EE90
+    component "User Feedback Collection" as UFC #90EE90
+}
+
+' Connections
+BRE -[#blue,thickness=2]down-> UID : 1. Fetch user data
+BRE -[#blue,thickness=2]down-> CD : 2. Fetch content data
+BRE -[#green,thickness=2]up-> REC : 3. Cache recommendations
+REC -[#green,thickness=2]right-> RCache : 4. Store recommendations
+PNS -[#orange,thickness=2]down-> RCache : 5. Fetch recommendations
+PNS -[#orange,thickness=2]right-> APIGateway : 6. Push recommendations
+RRE -[#red,thickness=2]down-> UID : 7. Analyze real-time data
+RRE -[#red,thickness=2]up-> REC : 8. Update recommendations
+DataAnalysis -[#purple,thickness=2]down-> Database : 9. Analyze user behavior
+MLP -[#purple,thickness=2]right-> FE : 10. Generate features
+FE -[#purple,thickness=2]up-> BRE : 11. Provide features
+ABT -[#brown,thickness=2]left-> BRE : 12. Test recommendation strategies
+PM -[#brown,thickness=2]left-> RRE : 13. Personalize recommendations
+UAS -[#gray,thickness=2]up-> APIGateway : 14. Authenticate users
+CMS -[#gray,thickness=2]up-> CD : 15. Moderate content
+
+note right of RecommendationService
+  Performance optimization:
+  - Use caching to reduce recommendation calculations
+  - Optimize recommendation algorithms
+  - Increase computational resources for recommendation engines
+end note
+
+note right of RRE
+  Real-time recommendations based on user behavior,
+  improving timeliness and accuracy
+end note
+
+note bottom of Database
+  Scalability:
+  - Implement sharding
+  - Use read replicas for heavy read operations
+end note
+
+note bottom of DataProcessing
+  Continuous improvement:
+  - Regular model retraining
+  - Feature importance analysis
+end note
+
+note right of ExternalServices
+  Security:
+  - Implement rate limiting
+  - Use OAuth 2.0 for authentication
+end note
+
+note left of MonitoringLogging
+  System health:
+  - Set up alerts for anomalies
+  - Implement distributed tracing
+end note
+
+@enduml
diff --git a/twitter/twitter_search_recommendation_overview.puml b/twitter/twitter_search_recommendation_overview.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_search_recommendation_overview.puml
@@ -0,0 +1,84 @@
+@startuml Twitter_Search_Recommendation_Overview
+
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #E6F3FF
+skinparam shadowing false
+skinparam RoundCorner 8
+skinparam ArrowColor 454645
+skinparam DefaultFontName Arial
+skinparam DefaultFontSize 11
+
+rectangle "Load Balancing Layer" as LoadBalancingLayer #F0F8FF {
+    component "Geo-Load Balancer" as GeoLoadBalancer #FFFACD
+    component "API Gateway" as APIGateway #FFFACD
+}
+
+rectangle "Service Layer" as ServiceLayer #F0FFF0 {
+    component "Auth Service" as AuthService #90EE90
+    component "Search Service" as SearchService #90EE90
+    component "Recommendation Service" as RecommendationService #90EE90
+    component "Data Analysis Service" as DataAnalysis #90EE90
+    component "Logging Service" as LoggingService #90EE90
+    component "Monitoring Service" as MonitoringService #90EE90
+    component "Batch Processing Service" as BatchProcessing #90EE90
+}
+
+rectangle "Data Layer" as DataLayer #FFF0F5 {
+    component "Search Engine" as SearchEngine #FFB6C1
+    component "Cache" as Cache #FFFACD
+    component "Database" as Database #FFFACD
+    component "Message Queue" as MessageQueue #FFFACD
+}
+
+GeoLoadBalancer -[#blue,thickness=2]down-> APIGateway : <back:#FFFFFF><color:#blue>1. Global routing</color></back>
+APIGateway -[#green,thickness=2]down-> AuthService : <back:#FFFFFF><color:#green>2. Auth check</color></back>
+APIGateway -[#red,thickness=2]down-> SearchService : <back:#FFFFFF><color:#red>3. Search request</color></back>
+APIGateway -[#purple,thickness=2]down-> RecommendationService : <back:#FFFFFF><color:#purple>4. Recommendation request</color></back>
+
+SearchService -[#orange,thickness=2]right-> Cache : <back:#FFFFFF><color:#orange>5. Cache access</color></back>
+SearchService -[#brown,thickness=2]right-> SearchEngine : <back:#FFFFFF><color:#brown>6. Query/Index</color></back>
+SearchService -[#pink,thickness=2]down-> LoggingService : <back:#FFFFFF><color:#pink>13. Log searches</color></back>
+
+RecommendationService -[#darkgreen,thickness=2]left-> DataAnalysis : <back:#FFFFFF><color:#darkgreen>7. Generate</color></back>
+RecommendationService -[#darkblue,thickness=2]right-> Cache : <back:#FFFFFF><color:#darkblue>8. Cache access</color></back>
+RecommendationService -[#gray,thickness=2]down-> BatchProcessing : <back:#FFFFFF><color:#gray>12. Trigger batch</color></back>
+RecommendationService -[#lightblue,thickness=2]down-> LoggingService : <back:#FFFFFF><color:#lightblue>14. Log recommendations</color></back>
+
+DataAnalysis -[#darkred,thickness=2]down-> Database : <back:#FFFFFF><color:#darkred>9. User data</color></back>
+DataAnalysis -[#darkorange,thickness=2]down-> MessageQueue : <back:#FFFFFF><color:#darkorange>10. Async tasks</color></back>
+
+MessageQueue -[#darkpurple,thickness=2]right-> RecommendationService : <back:#FFFFFF><color:#darkpurple>11. Process tasks</color></back>
+
+MonitoringService -[#lightgreen,thickness=2]up-> SearchService : <back:#FFFFFF><color:#lightgreen>15. Monitor</color></back>
+MonitoringService -[#lightyellow,thickness=2]up-> RecommendationService : <back:#FFFFFF><color:#lightyellow>16. Monitor</color></back>
+MonitoringService -[#lightpink,thickness=2]up-> MessageQueue : <back:#FFFFFF><color:#lightpink>17. Monitor queue</color></back>
+MonitoringService -[#lightgray,thickness=2]up-> BatchProcessing : <back:#FFFFFF><color:#lightgray>18. Monitor jobs</color></back>
+
+note top of Cache
+  Key: ProductID/TweetID
+  Value: ProductDetails/TweetDetails
+end note
+
+note top of Database
+  Indexed for fast retrieval
+  Read-optimized
+end note
+
+note top of MessageQueue
+  Async processing
+  Horizontal scaling
+end note
+
+note bottom of SearchEngine
+  Performance bottleneck:
+  Optimize indexing and query processing
+end note
+
+note top of RecommendationService
+  Scalability concern:
+  Implement caching and load balancing
+end note
+
+@enduml
diff --git a/twitter/twitter_search_service_architecture_with_caching_and_elasticsearch_integration.puml b/twitter/twitter_search_service_architecture_with_caching_and_elasticsearch_integration.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_search_service_architecture_with_caching_and_elasticsearch_integration.puml
@@ -0,0 +1,63 @@
+@startuml Twitter_Search_Service_Detailed
+
+!define LIGHTYELLOW #FFFACD
+!define LIGHTGREEN #90EE90
+!define LIGHTRED #FFB6C1
+
+skinparam backgroundColor #E6F3FF
+skinparam shadowing false
+skinparam RoundCorner 8
+skinparam ArrowColor 454645
+skinparam DefaultFontName Arial
+skinparam DefaultFontSize 11
+
+allowmixing
+
+component "Search Service" as SearchService #90EE90 {
+    component "Request Handler" as RH
+    component "Query Parser" as QP
+    component "Spell Check & Auto-complete" as SCAC
+    component "Cache Query" as CQ
+    component "Index Query" as IQ
+    component "Result Handler" as RHdl {
+        component "Result Sorting & Filtering" as RSF
+        component "Result Caching" as RC
+    }
+}
+
+component "Search Engine" as SearchEngine #FFB6C1
+component "Cache" as Cache #FFFACD {
+    component "Search Result Cache" as SRC
+}
+
+' Search Service Flow
+RH -down-> QP : Parse query
+QP -down-> SCAC : Check spelling & suggest
+SCAC -down-> CQ : Query cache
+CQ -right-> SRC : Read from cache
+SRC -left-> CQ : Return results (Hit/Miss)
+CQ -down-> IQ : Query index (if Cache Miss)
+IQ -right-> SearchEngine : Query search engine
+SearchEngine -left-> IQ : Return search results
+IQ -up-> RSF : Sort & filter results
+RSF -right-> RC : Cache results
+RC -up-> SRC : Store in cache
+
+note right of SearchService
+  性能优化:
+  - 使用缓存（Redis）减少重复查询
+  - 使用Elasticsearch提高搜索查询速度
+  - 查询解析和结果排序优化
+end note
+
+note right of SCAC
+  提供拼写纠正和自动补全功能，
+  改善用户搜索体验
+end note
+
+note right of SearchEngine
+  Performance bottleneck:
+  Optimize query processing
+end note
+
+@enduml
diff --git a/twitter/twitter_social_interaction_service.puml b/twitter/twitter_social_interaction_service.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_social_interaction_service.puml
@@ -0,0 +1,47 @@
+@startuml
+
+skinparam backgroundColor #D3D3D3
+skinparam class {
+  BackgroundColor #FFFFFF
+  BorderColor #222
+  ArrowColor #222
+  FontName "Arial"
+  FontSize 14
+}
+
+title Twitter Social Interaction
+
+rectangle "Client" as Client
+
+rectangle "Interaction Service" as IS {
+  rectangle "Like Handler" as LH
+  rectangle "Comment Handler" as CH
+  rectangle "Retweet Handler" as RH
+  rectangle "Direct Message Handler" as DMH
+  rectangle "Follow Handler" as FH
+}
+
+rectangle "Data Persistence Layer" as DPS {
+  rectangle "Interaction Data (DB)" as ID
+  rectangle "User Data (DB)" as UD
+}
+
+Client -down-> LH : Like Tweet
+LH -down-> ID : Stores Like Data
+Client -down-> CH : Comment on Tweet
+CH -down-> ID : Stores Comment Data
+Client -down-> RH : Retweet
+RH -down-> ID : Stores Retweet Data
+Client -down-> DMH : Send Direct Message
+DMH -down-> ID : Stores Message Data
+Client -down-> FH : Follow User
+FH -down-> UD : Updates Follow Data
+
+note right of IS
+  **Performance Optimization**:
+  - Use Redis for real-time interaction data caching
+  - Asynchronous processing of interactions
+  - Optimize database schema for fast reads and writes
+end note
+
+@enduml
diff --git a/twitter/twitter_system_performance_bottlenecks_and_optimizations_for_high_concurrency.puml b/twitter/twitter_system_performance_bottlenecks_and_optimizations_for_high_concurrency.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_system_performance_bottlenecks_and_optimizations_for_high_concurrency.puml
@@ -0,0 +1,116 @@
+@startuml
+skinparam backgroundColor #F0F8FF
+skinparam defaultFontName "Arial"
+skinparam defaultFontSize 16
+skinparam rectangle {
+    BackgroundColor #FFFFFF
+    BorderColor #4682B4
+    RoundCorner 20
+}
+skinparam note {
+    BackgroundColor #E6E6FA
+    BorderColor #483D8B
+    FontSize 14
+}
+
+title <font size=24>Performance Bottlenecks and Optimizations in Twitter System Architecture</font>
+
+' Front-end Components
+rectangle "Front-end" {
+    rectangle "Load Balancer" as LB #E0FFFF
+    rectangle "Web Server" as WS #E0FFFF
+}
+
+' Back-end Components
+rectangle "Back-end" {
+    rectangle "Application Server" as AS #FFE4E1
+    rectangle "Async Processing Queue\n(Kafka/RabbitMQ)" as APQ #FFE4E1
+    rectangle "Search Service" as SSrv #FFE4E1
+    rectangle "Notification Service" as NSrv #FFE4E1
+    rectangle "Timeline Update Service" as TUS #FFE4E1
+}
+
+' Data Storage
+rectangle "Data Layer" {
+    rectangle "Tweet & Comment Data" as TCD #F0FFF0
+    rectangle "Distributed Transaction Management" as DTM #F0FFF0
+}
+
+' Global Deployment
+rectangle "Infrastructure" {
+    rectangle "Global Deployment" as GD #FFF5EE
+}
+
+' Notes
+note right of LB
+  <font size=16>**负载均衡器性能瓶颈**:</font>
+  - 原因：处理能力不足
+  - 解决方法：使用高性能负载均衡器，增加实例
+end note
+
+note right of WS
+  <font size=16>**Web服务器性能瓶颈**:</font>
+  - 原因：处理能力不足
+  - 解决方法：增加Web服务器实例
+end note
+
+note right of AS
+  <font size=16>**应用服务器性能瓶颈**:</font>
+  - 原因：高并发处理
+  - 解决方法：优化代码，增加服务器实例
+end note
+
+note right of APQ
+  <font size=16>**异步处理队列性能瓶颈**:</font>
+  - 原因：容量和处理速度不足
+  - 解决方法：使用高性能消息队列，增加分区和消费者
+end note
+
+note right of SSrv
+  <font size=16>**搜索服务性能瓶颈**:</font>
+  - 原因：数据库查询效率低
+  - 解决方法：优化查询，使用索引，分布式搜索引擎
+end note
+
+note right of NSrv
+  <font size=16>**通知服务性能瓶颈**:</font>
+  - 原因：推送延迟高
+  - 解决方法：优化推送算法，使用实时推送服务
+end note
+
+note right of TUS
+  <font size=16>**时间线更新服务性能瓶颈**:</font>
+  - 原因：实时数据读取和写入
+  - 解决方法：优化算法，使用缓存，数据库分片和复制
+end note
+
+note right of TCD
+  <font size=16>**数据库性能瓶颈**:</font>
+  - 原因：高并发读写
+  - 解决方法：分片，读写分离，优化索引
+end note
+
+note right of DTM
+  <font size=16>**分布式事务管理性能瓶颈**:</font>
+  - 原因：跨服务操作协调
+  - 解决方法：减少分布式事务，采用最终一致性
+end note
+
+note right of GD
+  <font size=16>**全球化部署优化**:</font>
+  - 原因：跨地区访问延迟高
+  - 解决方法：多地区部署，就近访问
+end note
+
+' Layout
+LB -[hidden]down-> WS
+WS -[hidden]down-> AS
+AS -[hidden]down-> APQ
+APQ -[hidden]down-> SSrv
+SSrv -[hidden]down-> NSrv
+NSrv -[hidden]down-> TUS
+TUS -[hidden]down-> TCD
+TCD -[hidden]down-> DTM
+DTM -[hidden]down-> GD
+
+@enduml
diff --git a/twitter/twitter_user_auth_service_with_oauth_jwt_2fa_and_rate_limiting.puml b/twitter/twitter_user_auth_service_with_oauth_jwt_2fa_and_rate_limiting.puml
new file mode 100644
--- /dev/null
+++ ./twitter/twitter_user_auth_service_with_oauth_jwt_2fa_and_rate_limiting.puml
@@ -0,0 +1,57 @@
+@startuml
+
+skinparam backgroundColor #D3D3D3
+skinparam defaultFontName "Arial"
+skinparam defaultFontSize 16
+skinparam note {
+  BackgroundColor #FFFFFF
+  BorderColor #000000
+  FontSize 16
+  FontName "Arial"
+}
+
+title User Authentication and Authorization Service Architecture
+
+rectangle "Client" as Client
+
+rectangle "Auth Service" as Auth {
+    rectangle "Login Handler" as LH
+    rectangle "Registration Handler" as RegH
+    rectangle "Token Service" as TS
+    rectangle "Permission Checker" as PC
+}
+
+rectangle "Data Persistence Layer" as DPS {
+    rectangle "User Data (DB)" as UD
+    rectangle "Session Data (Redis)" as SD
+    rectangle "Credential Cache (Redis)" as CC
+}
+
+Client -down-> LH : Sends Login Request
+LH -down-> CC : Checks Credential Cache
+CC -down-> UD : Cache Miss, Verifies Credentials in DB
+UD -up-> CC : Stores Verified Credentials in Cache
+LH -down-> TS : Generates Token
+TS -down-> SD : Stores Session Data
+TS -up-> LH : Returns Token
+LH -up-> Client : Returns Login Response with Token
+
+Client -down-> RegH : Sends Registration Request
+RegH -down-> UD : Stores User Data
+RegH -up-> Client : Returns Registration Response
+
+Client -down-> PC : Sends Request with Token
+PC -down-> TS : Verifies Token
+TS -down-> SD : Verifies Session Data
+TS -up-> PC : Returns Token Status
+PC -up-> Client : Returns Permission Check Response
+
+note right of Auth
+  **性能优化**:
+  - 使用Redis缓存会话数据
+  - 优化数据库查询
+  - 使用JWT进行轻量级权限验证
+  - 缓存用户凭证验证结果
+end note
+
+@enduml
diff --git a/twitter_comment_system_with_ml_moderation_and_real_time_processing.puml b/twitter_comment_system_with_ml_moderation_and_real_time_processing.puml
new file mode 100644
--- /dev/null
+++ ./twitter_comment_system_with_ml_moderation_and_real_time_processing.puml
@@ -0,0 +1,119 @@
+@startuml Twitter Comment System Design
+
+!define FONTNAME Roboto
+!define FONTSIZE 12
+!define TITLEFONTSIZE 18
+
+skinparam backgroundColor #E0E8F0
+skinparam defaultFontName FONTNAME
+skinparam defaultFontSize FONTSIZE
+skinparam roundcorner 10
+skinparam shadowing false
+skinparam ArrowColor #2C3E50
+skinparam ArrowThickness 1.2
+skinparam rectangleBorderColor #34495E
+skinparam rectangleBackgroundColor #ECF0F1
+skinparam databaseBorderColor #16A085
+skinparam databaseBackgroundColor #D1F2EB
+skinparam queueBorderColor #8E44AD
+skinparam queueBackgroundColor #E8DAEF
+skinparam noteBorderColor #F39C12
+skinparam noteBackgroundColor #FCF3CF
+
+title <font size=TITLEFONTSIZE>Twitter Comment System: Comment Flow</font>
+
+actor User
+
+rectangle "User Interface" as UI {
+    [Web & Mobile Apps]
+}
+
+cloud "CDN" as CDN
+
+rectangle "API Gateway" as APIGateway {
+    [Load Balancer]
+    [Auth & Rate Limiting]
+}
+
+rectangle "Comment Service" as CommentService {
+    [Comment CRUD]
+    [Tree Structure]
+    [Sorting & Pagination]
+}
+
+database "Data Storage" as DataStorage {
+    [Comment DB]
+}
+
+rectangle "Cache Layer" as CacheLayer {
+    [Redis Cluster]
+}
+
+queue "Message Queue" as MessageQueue {
+    [Kafka]
+}
+
+rectangle "ML & Analytics" as MLAnalytics {
+    [Sentiment Analysis]
+    [Content Moderation]
+}
+
+rectangle "Notification Service" as NotificationService
+
+User -[#3498DB]right-> UI : "1. Post comment"
+UI -[#3498DB]down-> CDN : "2. Send request"
+CDN -[#3498DB]down-> APIGateway : "3. Route request"
+APIGateway -[#3498DB]down-> CommentService : "4. Process comment"
+CommentService -[#3498DB]right-> CacheLayer : "5. Check cache"
+CommentService -[#3498DB]down-> DataStorage : "6. Store comment"
+CommentService -[#3498DB]left-> MessageQueue : "7. Publish event"
+MessageQueue -[#3498DB]-> MLAnalytics : "8. Analyze content"
+MLAnalytics -[#3498DB]-> CommentService : "9. Return analysis"
+MessageQueue -[#3498DB]-> NotificationService : "10. Send notifications"
+CommentService -[#3498DB]up-> APIGateway : "11. Return response"
+APIGateway -[#3498DB]up-> CDN : "12. Send response"
+CDN -[#3498DB]up-> UI : "13. Update UI"
+UI -[#3498DB]left-> User : "14. Display comment"
+
+note right of UI
+  Features:
+  - Real-time updates
+  - Infinite scrolling
+  - Rich text editor
+end note
+
+note right of APIGateway
+  - Rate limiting: 100 req/min
+  - JWT Authentication
+end note
+
+note right of CommentService
+  - Tree structure for replies
+  - Sorting: Latest, Most liked
+  - Pagination: 20 comments/page
+end note
+
+note bottom of DataStorage
+  - Sharding by tweet_id
+  - Multi-region replication
+end note
+
+note bottom of CacheLayer
+  key: tweet_id
+  value: {
+    comments: [...],
+    user_info: {...}
+  }
+  TTL: 1 hour
+end note
+
+note bottom of MLAnalytics
+  Performance bottleneck:
+  High volume of comments
+  
+  Optimization:
+  - Batch processing
+  - Asynchronous analysis
+end note
+
+@enduml
\ No newline at end of file
diff --git a/uber_system_architecture/uber_driver_rider_matching_algorithm_detailed_flow.puml b/uber_system_architecture/uber_driver_rider_matching_algorithm_detailed_flow.puml
new file mode 100644
--- /dev/null
+++ ./uber_system_architecture/uber_driver_rider_matching_algorithm_detailed_flow.puml
@@ -0,0 +1,80 @@
+@startuml Uber Matching Algorithm System
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #F0F0F0
+
+title Uber Driver-Rider Matching Algorithm System
+
+rectangle "Input Data Sources" as InputSources #E1F5FE {
+    component "Real-time Location Data" as LocationData #B3E5FC
+    component "User Preferences" as UserPreferences #B3E5FC
+    component "Historical Trip Data" as HistoricalData #B3E5FC
+}
+
+rectangle "Preprocessing" as Preprocessing #E8F5E9 {
+    component "Data Cleansing" as DataCleansing #C8E6C9
+    component "Feature Extraction" as FeatureExtraction #C8E6C9
+}
+
+rectangle "Demand Prediction" as DemandPrediction #FFF3E0 {
+    component "Time Series Analysis" as TimeSeriesAnalysis #FFE0B2
+    component "Machine Learning Models" as MLModels #FFE0B2
+}
+
+rectangle "Pricing Engine" as PricingEngine #F3E5F5 {
+    component "Dynamic Pricing Algorithm" as DynamicPricing #E1BEE7
+    component "Surge Pricing Calculator" as SurgePricing #E1BEE7
+}
+
+rectangle "Matching Algorithm" as MatchingAlgorithm #FFEBEE {
+    component "Spatial Index" as SpatialIndex #FFCDD2
+    component "ETA Calculator" as ETACalculator #FFCDD2
+    component "Driver Ranking" as DriverRanking #FFCDD2
+    component "Rider-Driver Matcher" as Matcher #FFCDD2
+}
+
+rectangle "Optimization Engine" as OptimizationEngine #E0F2F1 {
+    component "Route Optimizer" as RouteOptimizer #B2DFDB
+    component "Supply-Demand Balancer" as SupplyDemandBalancer #B2DFDB
+}
+
+rectangle "Output Handlers" as OutputHandlers #ECEFF1 {
+    component "Notification Service" as NotificationService #CFD8DC
+    component "Booking Service" as BookingService #CFD8DC
+}
+
+database "Matching Results DB" as MatchingResultsDB #FFCCBC
+
+InputSources -[#0000FF,thickness=2]down-> Preprocessing : <back:#FFFFFF><color:#0000FF>1. Feed data</color></back>
+Preprocessing -[#006400,thickness=2]right-> DemandPrediction : <back:#FFFFFF><color:#006400>2. Processed data</color></back>
+Preprocessing -[#8B4513,thickness=2]down-> MatchingAlgorithm : <back:#FFFFFF><color:#8B4513>3. Cleaned data</color></back>
+DemandPrediction -[#4B0082,thickness=2]down-> PricingEngine : <back:#FFFFFF><color:#4B0082>4. Demand forecasts</color></back>
+PricingEngine -[#FF1493,thickness=2]right-> MatchingAlgorithm : <back:#FFFFFF><color:#FF1493>5. Price information</color></back>
+MatchingAlgorithm -[#FF4500,thickness=2]right-> OptimizationEngine : <back:#FFFFFF><color:#FF4500>6. Initial matches</color></back>
+OptimizationEngine -[#1E90FF,thickness=2]down-> OutputHandlers : <back:#FFFFFF><color:#1E90FF>7. Optimized matches</color></back>
+OutputHandlers -[#FF8C00,thickness=2]down-> MatchingResultsDB : <back:#FFFFFF><color:#FF8C00>8. Store results</color></back>
+
+note top of SpatialIndex
+  Efficiently indexes driver 
+  and rider locations
+end note
+
+note top of DriverRanking
+  Ranks drivers based on 
+  proximity, ratings, and 
+  other factors
+end note
+
+note bottom of RouteOptimizer
+  Optimizes routes for 
+  multiple pickups and 
+  drop-offs (e.g., UberPool)
+end note
+
+note bottom of SupplyDemandBalancer
+  Adjusts driver supply 
+  to meet rider demand 
+  across different areas
+end note
+
+@enduml
diff --git a/uber_system_architecture/uber_real_time_location_tracking.puml b/uber_system_architecture/uber_real_time_location_tracking.puml
new file mode 100644
--- /dev/null
+++ ./uber_system_architecture/uber_real_time_location_tracking.puml
@@ -0,0 +1,81 @@
+@startuml Uber Real-time Location Tracking System
+!pragma layout dot
+allowmixing
+skinparam backgroundColor #F0F0F0
+
+title Uber Real-time Location Tracking System Architecture
+
+rectangle "Mobile Devices" as MobileDevices #E1F5FE {
+    component "Uber Driver App" as DriverApp #B3E5FC
+    component "Uber Rider App" as RiderApp #B3E5FC
+}
+
+rectangle "Location Services" as LocationServices #E8F5E9 {
+    component "GPS Data Processor" as GPSProcessor #C8E6C9
+    component "Geocoding Service" as GeocodingService #C8E6C9
+    component "Geofencing Service" as GeofencingService #C8E6C9
+}
+
+rectangle "Real-time Processing" as RealTimeProcessing #FFF3E0 {
+    component "Location Update Stream" as LocationStream #FFE0B2
+    component "Apache Kafka" as Kafka #FFE0B2
+    component "Apache Flink" as Flink #FFE0B2
+}
+
+rectangle "Data Storage" as DataStorage #F3E5F5 {
+    database "Location History DB" as LocationHistoryDB #E1BEE7
+    database "Real-time Location Cache" as LocationCache #E1BEE7
+}
+
+rectangle "Location-based Services" as LocationBasedServices #FFEBEE {
+    component "ETA Calculator" as ETACalculator #FFCDD2
+    component "Route Optimizer" as RouteOptimizer #FFCDD2
+    component "Nearby Drivers Finder" as NearbyDriversFinder #FFCDD2
+}
+
+rectangle "Map Services" as MapServices #E0F2F1 {
+    component "Map Rendering" as MapRendering #B2DFDB
+    component "Traffic Data Integration" as TrafficData #B2DFDB
+}
+
+cloud "External Services" as ExternalServices #ECEFF1 {
+    component "Google Maps API" as GoogleMapsAPI
+    component "OpenStreetMap" as OpenStreetMap
+}
+
+MobileDevices -[#0000FF,thickness=2]down-> LocationServices : <back:#FFFFFF><color:#0000FF>1. Send GPS data</color></back>
+LocationServices -[#006400,thickness=2]right-> RealTimeProcessing : <back:#FFFFFF><color:#006400>2. Process & stream data</color></back>
+RealTimeProcessing -[#8B4513,thickness=2]down-> DataStorage : <back:#FFFFFF><color:#8B4513>3. Store & cache data</color></back>
+RealTimeProcessing -[#4B0082,thickness=2]right-> LocationBasedServices : <back:#FFFFFF><color:#4B0082>4. Provide real-time data</color></back>
+LocationBasedServices -[#FF1493,thickness=2]up-> MapServices : <back:#FFFFFF><color:#FF1493>5. Update map data</color></back>
+MapServices -[#FF4500,thickness=2]right-> ExternalServices : <back:#FFFFFF><color:#FF4500>6. Fetch map & traffic data</color></back>
+MapServices -[#1E90FF,thickness=2]up-> MobileDevices : <back:#FFFFFF><color:#1E90FF>7. Render map & location info</color></back>
+
+note top of GPSProcessor
+  Handles raw GPS data
+  from mobile devices
+end note
+
+note top of GeocodingService
+  Converts GPS coordinates
+  to human-readable addresses
+end note
+
+note top of GeofencingService
+  Manages virtual geographic
+  boundaries for specific areas
+end note
+
+note bottom of LocationCache
+  Redis cluster for
+  high-performance caching
+  of current locations
+end note
+
+note bottom of Flink
+  Processes location updates
+  in real-time for immediate
+  use in the system
+end note
+
+@enduml
diff --git a/uber_system_architecture/uber_system_architecture_with_matching_routing_payment_and_realtime_tracking.puml b/uber_system_architecture/uber_system_architecture_with_matching_routing_payment_and_realtime_tracking.puml
new file mode 100644
--- /dev/null
+++ ./uber_system_architecture/uber_system_architecture_with_matching_routing_payment_and_realtime_tracking.puml
@@ -0,0 +1,90 @@
+@startuml
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #FAFAFA
+skinparam componentStyle uml2
+skinparam defaultFontSize 16
+skinparam noteFontSize 14
+skinparam arrowFontSize 14
+
+rectangle "Uber System" as UberSystem {
+    rectangle "Client Layer" as ClientLayer #E6E6FA {
+        component "User App\n(Passenger)" as UserApp #D8BFD8
+        component "Driver App\n(Driver)" as DriverApp #D8BFD8
+        component "Web Interface" as WebInterface #D8BFD8
+    }
+
+    rectangle "API Gateway & Load Balancing" as APIGatewayLB #98FB98 {
+        component "Global Load Balancer" as GLB #90EE90
+        component "Regional Load Balancers" as RLB #90EE90
+        component "API Gateway" as APIGateway #90EE90
+        component "Rate Limiter" as RateLimiter #90EE90
+        component "Authentication" as Auth #90EE90
+    }
+
+    rectangle "Microservices" as Microservices #ADD8E6 {
+        component "User Service" as UserService #87CEFA
+        component "Driver Service" as DriverService #87CEFA
+        component "Trip Service" as TripService #87CEFA
+        component "Payment Service" as PaymentService #87CEFA
+        component "Notification Service" as NotificationService #87CEFA
+        component "Analytics Service" as AnalyticsService #87CEFA
+    }
+
+    rectangle "Core Services" as CoreServices #FFA07A {
+        component "Geolocation Service" as Geolocation #FA8072
+        component "Trip Matcher" as Matcher #FA8072
+        component "Real-time Location Tracker" as LocationTracker #FA8072
+        component "Pricing Engine" as PricingEngine #FA8072
+    }
+
+    rectangle "Data Storage Layer" as DataStorageLayer #DDA0DD {
+        database "User DB\n(Sharded)" as UserDB #D8BFD8
+        database "Driver DB\n(Sharded)" as DriverDB #D8BFD8
+        database "Trip DB\n(Sharded)" as TripDB #D8BFD8
+        database "Payment DB\n(Sharded)" as PaymentDB #D8BFD8
+        database "Analytics DB" as AnalyticsDB #D8BFD8
+        queue "Kafka Cluster" as KafkaCluster #D8BFD8
+        database "Redis Cluster" as RedisCluster #D8BFD8
+    }
+
+    rectangle "Infrastructure" as Infrastructure #F0E68C {
+        component "Service Discovery" as ServiceDiscovery #EEE8AA
+        component "Config Management" as ConfigManagement #EEE8AA
+        component "Logging & Monitoring" as LoggingMonitoring #EEE8AA
+        component "CI/CD Pipeline" as CICD #EEE8AA
+    }
+}
+
+ClientLayer -[#000000,thickness=2]down-> APIGatewayLB : <back:#FFFFFF><color:#000000>1. Requests</color></back>
+APIGatewayLB -[#000000,thickness=2]down-> Microservices : <back:#FFFFFF><color:#000000>2. Route</color></back>
+Microservices -[#0000FF,thickness=2]right-> CoreServices : <back:#FFFFFF><color:#0000FF>3. Core ops</color></back>
+Microservices -[#008000,thickness=2]down-> DataStorageLayer : <back:#FFFFFF><color:#008000>4. Data ops</color></back>
+CoreServices -[#008000,thickness=2]down-> DataStorageLayer : <back:#FFFFFF><color:#008000>5. R/W data</color></back>
+Microservices -[#FF0000,thickness=2]left-> Infrastructure : <back:#FFFFFF><color:#FF0000>6. Use infra</color></back>
+CoreServices -[#FF0000,thickness=2]left-> Infrastructure : <back:#FFFFFF><color:#FF0000>7. Use infra</color></back>
+
+note right of DataStorageLayer
+  Sharding:
+  - User DB: by user_id
+  - Driver DB: by driver_id
+  - Trip DB: by trip_id
+  - Payment DB: by payment_id
+  Redis: Caching, Real-time data
+  Kafka: Event streaming
+endnote
+
+note bottom of UberSystem
+Optimization suggestions:
+1. DB read replicas
+2. CDN for static assets
+3. Query optimization
+4. Horizontal scaling
+5. Multi-level caching
+6. Geospatial query optimization
+7. Async processing
+8. Circuit breakers
+endnote
+
+@enduml
diff --git a/uber_system_architecture/uber_system_database_design_with_sharding_and_replication.puml b/uber_system_architecture/uber_system_database_design_with_sharding_and_replication.puml
new file mode 100644
--- /dev/null
+++ ./uber_system_architecture/uber_system_database_design_with_sharding_and_replication.puml
@@ -0,0 +1,207 @@
+@startuml
+!define RECTANGLE class
+!define DATABASE database
+
+allowmixing
+scale 2
+
+skinparam backgroundColor #FFFFFF
+skinparam packageStyle rectangle
+skinparam linetype ortho
+
+skinparam defaultFontName Arial
+skinparam defaultFontSize 14
+skinparam defaultFontColor #000000
+skinparam noteFontSize 12
+skinparam arrowColor #454545
+skinparam defaultFontStyle bold
+
+skinparam rectangle {
+    BackgroundColor #E0E0E0
+    BorderColor #454545
+    FontColor #000000
+}
+
+skinparam database {
+    BackgroundColor #D0D0D0
+    BorderColor #454545
+    FontColor #000000
+}
+
+skinparam note {
+    BackgroundColor #FFFACD
+    BorderColor #454545
+    FontColor #000000
+}
+
+skinparam package {
+    BackgroundColor #F0F0F0
+    BorderColor #454545
+    FontColor #000000
+}
+
+package "Uber System" {
+    RECTANGLE "Backend Server\n(APIs & Core Logic)" as Backend
+
+    package "Data Storage Layer" {
+        package "Orders Database Cluster" {
+            DATABASE "Orders DB" as OrdersDB {
+                entity "Orders" {
+                    + PK: order_id (UUID)
+                    --
+                    FK: user_id
+                    FK: driver_id
+                    order_status
+                    pickup_location
+                    dropoff_location
+                    created_at
+                    updated_at
+                }
+                note bottom of Orders
+                    Indexes: 
+                    - user_id
+                    - driver_id
+                    - order_status
+                    - created_at
+                    --
+                    Shard Key: order_id (hash)
+                    Partition: Range by created_at
+                end note
+            }
+        }
+        
+        package "User Database Cluster" {
+            DATABASE "Users DB" as UsersDB {
+                entity "Users" {
+                    + PK: user_id (UUID)
+                    --
+                    username
+                    email
+                    phone_number
+                    password_hash
+                    account_status
+                    created_at
+                    last_login
+                }
+                note bottom of Users
+                    Indexes:
+                    - email
+                    - phone_number
+                    - account_status
+                    --
+                    Shard Key: user_id (hash)
+                    Partition: List by account_status
+                end note
+            }
+        }
+        
+        package "Driver Database Cluster" {
+            DATABASE "Drivers DB" as DriversDB {
+                entity "Drivers" {
+                    + PK: driver_id (UUID)
+                    --
+                    name
+                    email
+                    phone_number
+                    license_number
+                    vehicle_info
+                    current_status
+                    current_location
+                    last_updated
+                }
+                note bottom of Drivers
+                    Indexes:
+                    - email
+                    - phone_number
+                    - current_status
+                    - last_updated
+                    --
+                    Shard Key: driver_id (hash)
+                    Partition: List by current_status
+                end note
+            }
+        }
+        
+        package "Trip Records Database Cluster" {
+            DATABASE "Trips DB" as TripsDB {
+                entity "Trips" {
+                    + PK: trip_id (UUID)
+                    --
+                    FK: user_id
+                    FK: driver_id
+                    start_location
+                    end_location
+                    start_time
+                    end_time
+                    fare
+                    payment_status
+                }
+                note bottom of Trips
+                    Indexes:
+                    - user_id
+                    - driver_id
+                    - start_time
+                    - payment_status
+                    --
+                    Shard Key: trip_id (hash)
+                    Partition: Range by start_time
+                end note
+            }
+        }
+        
+        RECTANGLE "Redis Cluster" as RedisCluster {
+            User_Sessions
+            Driver_Locations
+            Active_Trips
+            Surge_Pricing
+        }
+        
+        note bottom of RedisCluster
+            User_Sessions: {user_id: session_data}
+            Driver_Locations: {driver_id: geo_coordinates}
+            Active_Trips: {trip_id: trip_status}
+            Surge_Pricing: {area_id: multiplier}
+            --
+            Shard Key: Key prefix
+            Partitioning: Hash-based sharding
+        end note
+    }
+    
+    Backend -down-> "Data Storage Layer" : Queries/Updates
+}
+
+note right of Backend
+    <b>Performance Optimizations:</b>
+    • Read replicas for read-heavy ops
+    • Redis caching for frequent data
+    • Database indexing
+    • Database sharding
+    • Asynchronous processing
+end note
+
+note left of "Data Storage Layer"
+    <b>Potential Bottlenecks:</b>
+    • Peak hour write operations
+    • Real-time location updates
+    • Complex cross-database queries
+    • High-volume data ingestion
+end note
+
+note bottom of "Data Storage Layer"
+    <b>Scalability Strategies:</b>
+    • Horizontal database scaling
+    • Caching layer implementation
+    • Message queues for async processing
+    • Microservices architecture
+    • CDNs for static content
+end note
+
+note right of RedisCluster
+    Redis Cluster Role:
+    • High-speed data access
+    • Reduce database load
+    • Real-time data processing
+    • Temporary data storage
+end note
+
+@enduml
diff --git a/url_shortener_system_architecture/url_shortener_system_architecture_with_caching_and_analytics.puml b/url_shortener_system_architecture/url_shortener_system_architecture_with_caching_and_analytics.puml
new file mode 100644
--- /dev/null
+++ ./url_shortener_system_architecture/url_shortener_system_architecture_with_caching_and_analytics.puml
@@ -0,0 +1,132 @@
+@startuml
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #F0F0F0
+skinparam padding 8
+skinparam roundcorner 15
+skinparam defaultFontSize 20
+skinparam noteFontSize 20
+skinparam arrowFontSize 20
+
+rectangle "Client" as Client #E1F5FE
+rectangle "CDN" as CDN #E1F5FE
+rectangle "DNS" as DNS #E1F5FE
+
+rectangle "Load Balancer" as LoadBalancer #D1C4E9 {
+    component "API Gateway" as APIGateway #B39DDB
+    component "Rate Limiter" as RateLimiter #B39DDB
+}
+
+rectangle "Security Layer" as SecurityLayer #FFCDD2 {
+    component "WAF" as WAF #EF9A9A
+    component "OAuth2 Server" as OAuth2Server #EF9A9A
+}
+
+rectangle "Service Layer" as ServiceLayer #BBDEFB {
+    component "URL Shortener Service" as URLShortenerService #90CAF9
+    component "Analytics Service" as AnalyticsService #90CAF9
+    component "User Management Service" as UserManagementService #90CAF9
+}
+
+rectangle "Cache Layer" as CacheLayer #FFCDD2 {
+    component "Redis Cluster" as RedisCluster #EF9A9A
+    component "Memcached" as Memcached #EF9A9A
+}
+
+rectangle "Database Layer" as DatabaseLayer #C8E6C9 {
+    component "MongoDB Cluster" as MongoDBCluster #81C784
+    component "Cassandra Cluster" as CassandraCluster #81C784
+    component "TimescaleDB" as TimescaleDB #81C784
+}
+
+rectangle "Message Queue" as MessageQueue #FFE0B2
+rectangle "Background Workers" as BackgroundWorkers #FFCC80
+
+rectangle "Service Discovery & Config" as ServiceDiscoveryConfig #E1BEE7 {
+    component "Consul" as Consul #CE93D8
+    component "Vault" as Vault #CE93D8
+}
+
+rectangle "Monitoring & Logging" as MonitoringLogging #E1BEE7 {
+    component "ELK Stack" as ELKStack #CE93D8
+    component "Prometheus" as Prometheus #CE93D8
+    component "Grafana" as Grafana #CE93D8
+    component "Jaeger" as Jaeger #CE93D8
+}
+
+rectangle "Backup & DR" as BackupDR #DCEDC8 {
+    component "Backup Service" as BackupService #AED581
+    component "Disaster Recovery" as DisasterRecovery #AED581
+}
+
+Client -[#1E88E5,thickness=2]down-> DNS : <back:#FFFFFF><color:#1E88E5>1. DNS lookup</color></back>
+DNS -[#1E88E5,thickness=2]down-> LoadBalancer : <back:#FFFFFF><color:#1E88E5>2. Route to Load Balancer</color></back>
+Client -[#43A047,thickness=2]down-> CDN : <back:#FFFFFF><color:#43A047>3. Static resource request</color></back>
+Client -[#FBC02D,thickness=2]down-> LoadBalancer : <back:#FFFFFF><color:#FBC02D>4. API request</color></back>
+LoadBalancer -[#FBC02D,thickness=2]down-> SecurityLayer : <back:#FFFFFF><color:#FBC02D>5. Security check</color></back>
+SecurityLayer -[#FBC02D,thickness=2]down-> ServiceLayer : <back:#FFFFFF><color:#FBC02D>6. Route request</color></back>
+ServiceLayer -[#D81B60,thickness=2]right-> CacheLayer : <back:#FFFFFF><color:#D81B60>7. Cache read/write</color></back>
+ServiceLayer -[#8E24AA,thickness=2]down-> DatabaseLayer : <back:#FFFFFF><color:#8E24AA>8. DB read/write</color></back>
+ServiceLayer -[#F4511E,thickness=2]left-> MessageQueue : <back:#FFFFFF><color:#F4511E>9. Publish events</color></back>
+BackgroundWorkers -[#F4511E,thickness=2]up-> MessageQueue : <back:#FFFFFF><color:#F4511E>10. Consume events</color></back>
+BackgroundWorkers -[#00897B,thickness=2]right-> DatabaseLayer : <back:#FFFFFF><color:#00897B>11. Process data</color></back>
+ServiceLayer -[#5E35B1,thickness=2]-> MonitoringLogging : <back:#FFFFFF><color:#5E35B1>12. Log & metrics</color></back>
+ServiceLayer -[#3949AB,thickness=2]-> ServiceDiscoveryConfig : <back:#FFFFFF><color:#3949AB>13. Service discovery & config</color></back>
+BackupDR -[#00ACC1,thickness=2]-> DatabaseLayer : <back:#FFFFFF><color:#00ACC1>14. Backup & DR</color></back>
+
+note right of LoadBalancer
+    - Implements SSL termination
+    - Distributes traffic across API servers
+    - Integrates with API Gateway for routing
+    - Rate limiting to prevent abuse
+end note
+
+note right of SecurityLayer
+    - WAF for protection against common web attacks
+    - OAuth2 for secure authentication and authorization
+end note
+
+note right of ServiceLayer
+    - Microservices architecture
+    - Each service is independently scalable
+    - Implements business logic and data access
+end note
+
+note bottom of CacheLayer
+    - Redis for short URL mappings and analytics
+    - Memcached for user sessions and rate limiting
+    - Implements cache aside pattern and write-through caching
+end note
+
+note right of DatabaseLayer
+    - MongoDB for URL mappings and user data
+    - Cassandra for time-series analytics data
+    - TimescaleDB for high-performance time-series data
+    - Sharding for horizontal scalability
+end note
+
+note left of MessageQueue
+    - Handles asynchronous processing
+    - Decouples services for better scalability
+    - Used for analytics, cleanup, and notifications
+end note
+
+note bottom of ServiceDiscoveryConfig
+    - Consul for service discovery and health checking
+    - Vault for secrets management and dynamic credentials
+end note
+
+note bottom of MonitoringLogging
+    - Centralized logging with ELK Stack
+    - Real-time metrics with Prometheus
+    - Visualization and alerting with Grafana
+    - Distributed tracing with Jaeger
+end note
+
+note bottom of BackupDR
+    - Regular backups of all databases
+    - Disaster recovery plan with multi-region failover
+end note
+
+@enduml
diff --git a/url_shortener_system_architecture/url_shortener_system_database_schema.puml b/url_shortener_system_architecture/url_shortener_system_database_schema.puml
new file mode 100644
--- /dev/null
+++ ./url_shortener_system_architecture/url_shortener_system_database_schema.puml
@@ -0,0 +1,81 @@
+@startuml
+!define MONGO(x) class x << (M,#3498db) >>
+
+skinparam class {
+    BackgroundColor #ecf0f1
+    BorderColor #34495e
+    ArrowColor #7f8c8d
+    FontColor #2c3e50
+}
+skinparam note {
+    BackgroundColor #f9e79f
+    BorderColor #f39c12
+    FontColor #34495e
+}
+skinparam linetype ortho
+skinparam backgroundColor #FAFAFA
+
+package "MongoDB Collections" #e8f6f3 {
+    together {
+        MONGO(urls) {
+            _id: ObjectId (Primary Key)
+            short_code: String (Shard Key)
+            original_url: String
+            created_at: Date (Index)
+            user_id: ObjectId (Index)
+            visits_count: Int
+            is_active: Boolean
+        }
+
+        MONGO(users) {
+            _id: ObjectId (Primary Key)
+            username: String
+            email: String
+            password_hash: String
+            created_at: Date
+            last_login: Date
+        }
+
+        MONGO(visits) {
+            _id: ObjectId (Primary Key)
+            url_id: ObjectId (Shard Key)
+            visitor_ip: String
+            user_agent: String
+            timestamp: Date (Index)
+        }
+    }
+
+    note bottom of urls
+        urls collection:
+        - Stores mapping of short codes to original URLs
+        - Uses short_code as shard key for optimized query performance
+        - Tracks visit count to reduce queries to visits collection
+    end note
+
+    note bottom of users
+        users collection:
+        - Stores user information
+        - Used for user authentication and profile management
+    end note
+
+    note bottom of visits
+        visits collection:
+        - Records detailed information for each short URL visit
+        - Used for generating visit statistics and analytics reports
+        - Uses url_id as shard key to keep related visit records on the same shard
+    end note
+}
+
+note as N1 #d5f5e3
+    NoSQL Design Notes:
+    1. This is a MongoDB document model, not a traditional relational database table design
+    2. No foreign key constraints; data consistency is maintained at the application level
+    3. Shard Keys are used for data distribution and query optimization
+    4. Indexes are used to improve query performance
+    5. The _id field serves as the Primary Key for each collection
+end note
+
+urls -[#7f8c8d]-> users : "belongs to"
+urls -[#7f8c8d]-> visits : "has many"
+
+@enduml
diff --git a/youtube_video_metadata_and_user_interaction_data_schema.puml b/youtube_video_metadata_and_user_interaction_data_schema.puml
new file mode 100644
--- /dev/null
+++ ./youtube_video_metadata_and_user_interaction_data_schema.puml
@@ -0,0 +1,126 @@
+@startuml YouTube Data Model
+
+' Color scheme
+skinparam backgroundColor #2E3440
+skinparam defaultFontColor #ECEFF4
+skinparam class {
+  BackgroundColor #3B4252
+  ArrowColor #88C0D0
+  BorderColor #81A1C1
+  FontColor #E5E9F0
+}
+skinparam packageBackgroundColor #2E3440
+skinparam packageBorderColor #5E81AC
+skinparam stereotypeCBackgroundColor #EBCB8B
+skinparam stereotypeCBorderColor #D08770
+
+' Layout
+left to right direction
+skinparam nodesep 60
+skinparam ranksep 60
+
+' Main entities
+package "Core Entities" {
+  class User {
+    +id : int <<PK>>
+    +username : varchar
+    +password : varchar
+    +email : varchar
+    +created_at : datetime
+  }
+
+  class Video <<NoSQL>> {
+    +id : string <<PK>>
+    +user_id : string <<Shard-Key>>
+    +title : string
+    +description : text
+    +upload_date : timestamp
+    +video_path : string
+    +status : string
+  }
+
+  class Tag {
+    +id : int <<PK>>
+    +name : varchar
+  }
+}
+
+' Interaction entities
+package "Interaction Entities" {
+  class VideoTag {
+    +video_id : string <<FK>>
+    +tag_id : int <<FK>>
+    +created_at : timestamp
+  }
+
+  class Comment <<NoSQL>> {
+    +id : string <<PK>>
+    +video_id : string <<Shard-Key>>
+    +user_id : string
+    +text : text
+    +created_at : timestamp
+  }
+
+  class VideoView <<NoSQL>> {
+    +video_id : string <<Composite Shard-Key>>
+    +user_id : string
+    +view_date : timestamp <<Composite Shard-Key>>
+  }
+
+  class VideoRating <<NoSQL>> {
+    +video_id : string <<Composite Shard-Key>>
+    +user_id : string <<Composite Shard-Key>>
+    +rating : int
+    +created_at : timestamp
+  }
+}
+
+' Cache entities
+package "Cache Entities" {
+  class SessionToken <<Redis>> {
+    +user_id : string <<FK>>
+    +token : string
+  }
+
+  class VideoStatusCache <<Redis>> {
+    +video_id : string <<FK>>
+    +status : string
+  }
+
+  class CommentCountCache <<Redis>> {
+    +video_id : string <<FK>>
+    +count : int
+  }
+
+  class RecentViewsCache <<Redis>> {
+    +user_id : string <<FK>>
+    +video_ids : string
+  }
+
+  class RatingCountCache <<Redis>> {
+    +video_id : string <<FK>>
+    +count : int
+  }
+}
+
+' Relationships
+User "1" -- "N" Video : Owns >
+User "1" -- "N" Comment : Creates >
+User "1" -- "N" VideoView : Records User >
+User "1" -- "N" VideoRating : Rates >
+
+Video "1" -- "N" VideoTag
+Video "1" -- "N" Comment : Includes >
+Video "1" -- "N" VideoView : Records View >
+Video "1" -- "N" VideoRating : Includes Rating >
+
+Tag "1" -- "N" VideoTag
+
+' Cache relationships
+Video ..> VideoStatusCache : Cache >
+User ..> SessionToken : Cache >
+Comment ..> CommentCountCache : Counts >
+VideoView ..> RecentViewsCache : Cache >
+VideoRating ..> RatingCountCache : Cache >
+
+@enduml
diff --git a/youtube_video_playback_and_recommendation_system_with_cdn_analytics_ml_personalization_and_content_delivery_optimization.puml b/youtube_video_playback_and_recommendation_system_with_cdn_analytics_ml_personalization_and_content_delivery_optimization.puml
new file mode 100644
--- /dev/null
+++ ./youtube_video_playback_and_recommendation_system_with_cdn_analytics_ml_personalization_and_content_delivery_optimization.puml
@@ -0,0 +1,94 @@
+@startuml YouTube_Video_Playback_Architecture
+!pragma layout smetana
+allowmixing
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 12
+skinparam defaultFontColor #333333
+skinparam padding 5
+skinparam roundCorner 10
+skinparam ArrowColor #0066CC
+skinparam ArrowThickness 2
+skinparam linetype ortho
+
+' 增加组件边框的可见度
+skinparam rectangle {
+    BorderColor #555555
+    BorderThickness 2
+}
+skinparam component {
+    BorderColor #555555
+    BorderThickness 2
+    BackgroundColor #E1F5FE
+}
+skinparam database {
+    BorderColor #555555
+    BorderThickness 2
+    BackgroundColor #FFCDD2
+}
+
+' Components
+rectangle "Client Side" as ClientSide {
+    component "User Interface" as UI
+    component "Video Player" as VideoPlayer
+}
+
+rectangle "Content Delivery" as ContentDelivery {
+    component "CDN" as CDN
+    component "Edge Servers" as EdgeServers
+}
+
+rectangle "Backend Services" as BackendServices {
+    component "Streaming Server" as StreamingServer
+    component "Analytics Service" as AnalyticsService
+    component "Recommendation Engine" as RecommendationEngine
+}
+
+rectangle "Data Storage" as DataStorage {
+    database "Video Storage" as VideoStorage
+    database "User Preferences DB" as UserPrefsDB
+    database "Analytics DB" as AnalyticsDB
+}
+
+actor "User" as User
+
+' Connections
+User -right-> UI : <color:#0066CC>1. Interact</color>
+UI -right-> VideoPlayer : <color:#0066CC>2. Request video</color>
+VideoPlayer -right-> CDN : <color:#0066CC>3. Request video chunks</color>
+CDN -right-> EdgeServers : <color:#0066CC>4. Fetch from nearest</color>
+EdgeServers -down-> VideoStorage : <color:#0066CC>5. Retrieve video data</color>
+VideoStorage -up-> EdgeServers : <color:#0066CC>6. Return video data</color>
+EdgeServers -left-> CDN : <color:#0066CC>7. Cache and return</color>
+CDN -left-> VideoPlayer : <color:#0066CC>8. Stream video chunks</color>
+VideoPlayer -down-> StreamingServer : <color:#0066CC>9. Request adaptive bitrate</color>
+StreamingServer -up-> VideoPlayer : <color:#0066CC>10. Provide optimal bitrate</color>
+VideoPlayer -down-> UserPrefsDB : <color:#0066CC>11. Update watch history</color>
+VideoPlayer -down-> AnalyticsService : <color:#0066CC>12. Send playback analytics</color>
+AnalyticsService -right-> AnalyticsDB : <color:#0066CC>13. Store analytics data</color>
+UI -down-> RecommendationEngine : <color:#0066CC>14. Get recommendations</color>
+RecommendationEngine -up-> UI : <color:#0066CC>15. Provide recommendations</color>
+
+' Notes
+note right of CDN #E0E0E0
+  Globally distributed for
+  low-latency content delivery
+end note
+
+note bottom of StreamingServer #E0E0E0
+  Handles adaptive bitrate streaming
+  for optimal viewing experience
+end note
+
+note bottom of AnalyticsService #E0E0E0
+  Processes user engagement
+  and performance metrics
+end note
+
+note bottom of RecommendationEngine #E0E0E0
+  Suggests videos based on
+  user history and preferences
+end note
+
+@enduml
\ No newline at end of file
diff --git a/youtube_video_upload_processing_and_transcoding_schema_with_queues.puml b/youtube_video_upload_processing_and_transcoding_schema_with_queues.puml
new file mode 100644
--- /dev/null
+++ ./youtube_video_upload_processing_and_transcoding_schema_with_queues.puml
@@ -0,0 +1,85 @@
+@startuml YouTube_Video_Upload_Architecture
+
+!pragma layout dot
+allowmixing
+
+skinparam backgroundColor #F0F0F0
+skinparam defaultFontName Arial
+skinparam defaultFontSize 15
+skinparam padding 5
+skinparam roundCorner 10
+
+rectangle "Client Side" as ClientSide #E6F3FF {
+    component "User Device" as UserDevice #87CEFA
+    component "YouTube App/Web Interface" as Frontend #FF6347
+}
+
+rectangle "Server Side" as ServerSide #FFFAF0 {
+    rectangle "Web Layer" as WebLayer #E6FFE6 {
+        component "Load Balancer" as LoadBalancer #FFD700
+        component "Web Server" as WebServer #98FB98
+        component "Authentication Service" as AuthService #DDA0DD
+    }
+    
+    rectangle "Application Layer" as AppLayer #FFF0F5 {
+        component "Upload Service" as UploadService #FF69B4
+        component "Transcoding Service" as TranscodingService #20B2AA
+        component "Content Delivery Network (CDN)" as CDN #87CEEB
+        component "Analytics Service" as AnalyticsService #FFA07A
+    }
+    
+    rectangle "Queue Layer" as QueueLayer #E6E6FA {
+        component "Upload Queue" as UploadQueue #FFDAB9
+    }
+    
+    rectangle "Storage Layer" as StorageLayer #FFE4E1 {
+        component "Original Video Storage" as OriginalStorage #FF69B4
+        component "Transcoded Video Storage" as TranscodedStorage #FF69B4
+        component "Metadata DB" as MetadataDB #FF69B4
+        component "User DB" as UserDB #FF69B4
+    }
+}
+
+UserDevice -[#000000,thickness=2]-> Frontend : <back:#FFFFFF><color:#000000>1. Initiate upload</color></back>
+Frontend -[#000000,thickness=2]-> LoadBalancer : <back:#FFFFFF><color:#000000>2. Send request</color></back>
+LoadBalancer -[#000000,thickness=2]-> WebServer : <back:#FFFFFF><color:#000000>3. Route request</color></back>
+WebServer -[#000000,thickness=2]-> AuthService : <back:#FFFFFF><color:#000000>4. Authenticate user</color></back>
+AuthService -[#000000,thickness=2]-> UserDB : <back:#FFFFFF><color:#000000>5. Verify credentials</color></back>
+WebServer -[#000000,thickness=2]-> UploadService : <back:#FFFFFF><color:#000000>6. Process upload</color></back>
+UploadService -[#000000,thickness=2]-> UploadQueue : <back:#FFFFFF><color:#000000>7. Queue for processing</color></back>
+UploadService -[#000000,thickness=2]-> OriginalStorage : <back:#FFFFFF><color:#000000>8. Store original</color></back>
+UploadService -[#000000,thickness=2]-> MetadataDB : <back:#FFFFFF><color:#000000>9. Store metadata</color></back>
+UploadQueue -[#000000,thickness=2]-> TranscodingService : <back:#FFFFFF><color:#000000>10. Dequeue for transcoding</color></back>
+TranscodingService -[#000000,thickness=2]-> TranscodedStorage : <back:#FFFFFF><color:#000000>11. Store transcoded versions</color></back>
+TranscodingService -[#000000,thickness=2]-> MetadataDB : <back:#FFFFFF><color:#000000>12. Update metadata</color></back>
+TranscodedStorage -[#000000,thickness=2]-> CDN : <back:#FFFFFF><color:#000000>13. Distribute content</color></back>
+CDN -[#000000,thickness=2]-> Frontend : <back:#FFFFFF><color:#000000>14. Serve video</color></back>
+AnalyticsService -[#000000,thickness=2]-> MetadataDB : <back:#FFFFFF><color:#000000>15. Collect and store analytics</color></back>
+
+note right of TranscodingService
+  Generates multiple versions
+  (different resolutions and formats)
+  Parallel processing for faster transcoding
+end note
+
+note bottom of CDN
+  Improves video delivery speed
+  and reduces server load
+end note
+
+note bottom of AnalyticsService
+  Tracks user engagement, views,
+  and other important metrics
+end note
+
+note right of UploadService
+  Potential bottleneck:
+  Handle large file uploads efficiently
+end note
+
+note left of TranscodingService
+  Performance consideration:
+  Load balancing for distributed transcoding
+end note
+
+@enduml
