@startuml High Traffic Calendar System - Architecture Overview
' ==================== Enhanced Metadata ====================
' === 基础分类 ===
' @category: distributed-systems
' @subcategory: message-queue
' @tags: #kafka, #message-queue, #event-streaming, #distributed-systems
' @description: High Traffic Calendar System - Complete Architecture Overview with Optimized Data Flow
'
' === 应用场景 ===
' @application: Calendar System
' @industry: Media & Entertainment, Education
' @use-cases: Data Analytics, Search & Discovery, Log Aggregation
' @business-value: Improved scalability, High availability, Better performance, Cost efficiency
'
' === 技术栈 ===
' @tech-stack: Kafka, ZooKeeper, Redis, Elasticsearch, MongoDB, Nginx
' @programming-languages: Go, JavaScript/Node.js, Scala
' @frameworks: Framework-agnostic
' @protocols: HTTP/REST, TCP
' @apis: REST API
'
' === 架构模式 ===
' @pattern: Event-Driven Architecture, CQRS, API Gateway
' @design-pattern: Observer, Producer-Consumer, Repository, Factory
' @data-flow: Bidirectional, Request-Response
' @communication-style: Asynchronous, Synchronous, Event-driven
'
' === 分布式特性 ===
' @cap-focus: AP (Availability + Partition Tolerance)
' @consistency-model: Eventual Consistency (configurable)
' @consensus-algorithm: Raft, Leader Election
' @partition-strategy: Hash-based, Key-based, Custom partitioning
'
' === 性能与扩展 ===
' @scale: Large (100K+ users, 1K+ QPS)
' @scalability: Horizontal scaling, Auto-scaling, Load balancing
' @performance-metrics: Throughput: 1M+ msg/s, Latency: <10ms p99
' @optimization-techniques: Caching, Indexing, Sharding/Partitioning
' @throughput: Very High (1M+ messages/second)
' @latency: Low (<10ms p99)
'
' === 可靠性 ===
' @reliability: Replication, Redundancy, Health checks, Automatic recovery
' @fault-tolerance: Replication, Retry mechanism
' @disaster-recovery: Multi-datacenter replication, Backup strategies, RPO/RTO management
' @availability: 99.99% (4 nines)
' @data-durability: 99.999999999% (11 nines) with proper replication
'
' === 安全性 ===
' @security-features: Authentication, Authorization, Encryption, Audit logging
' @authentication: OAuth 2.0, JWT, API Keys, SASL
' @authorization: RBAC (Role-Based Access Control), ACLs, Policy-based
' @encryption: TLS (in-transit), Optional encryption at rest
' @compliance: GDPR-ready, SOC2, HIPAA-compatible, PCI-DSS
'
' === 存储 ===
' @storage-type: Log Storage
' @database-type: NoSQL, In-Memory, Search Engine
' @caching-strategy: Cache-aside
' @data-persistence: Disk-based with WAL, Configurable durability, Snapshot backups
'
' === 监控运维 ===
' @monitoring: ELK Stack
' @logging: Centralized logging (ELK/Splunk), Structured logs, Log aggregation
' @alerting: Prometheus Alertmanager, PagerDuty, Custom alerts, SLA monitoring
' @observability: Metrics (RED/USE), Logs, Distributed tracing (Jaeger/Zipkin)
'
' === 部署 ===
' @deployment: Kubernetes, Docker, Cloud-native, Blue-Green deployment
' @infrastructure: Cloud, On-premise, Hybrid, Multi-cloud
' @cloud-provider: AWS, Azure, GCP, Cloud-agnostic
' @containerization: Docker-ready, Container-friendly
'
' === 成本 ===
' @cost-factors: Compute instances, Storage costs, Network bandwidth, Licensing
' @cost-optimization: Reserved instances, Auto-scaling, Storage tiering, Compression, Resource right-sizing
' @resource-usage: CPU: Medium-High, Memory: Medium-High, Disk I/O: High, Network: Medium
'
' === 复杂度 ===
' @complexity: Medium
' @implementation-difficulty: Medium
' @maintenance-complexity: Medium
'
' === 学习 ===
' @difficulty-level: Intermediate
' @learning-value: Medium to High (practical system design)
' @prerequisites: Message queues, Pub-Sub pattern, Database fundamentals, SQL/NoSQL
' @related-concepts: Replication strategies, Data partitioning, Event sourcing, CQRS, Caching strategies, Cache invalidation
'
' === 数据特征 ===
' @data-volume: Medium to Large (GBs to TBs)
' @data-velocity: Real-time, High-speed streaming
' @data-variety: Structured, Semi-structured (JSON, Avro)
' @data-model: Document, Key-Value, Relational, Time-series
'
' === 集成 ===
' @integration-points: REST APIs, Message queues, Database connectors, Webhooks
' @third-party-services: Cloud storage, CDN, Payment processors, Analytics services
' @external-dependencies: Minimal external dependencies
'
' === 测试 ===
' @testing-strategy: Unit tests, Integration tests, Load tests, Chaos engineering
' @quality-assurance: CI/CD pipelines, Code review, Static analysis, Performance testing
'
' === 版本 ===
' @version: 1.0 (current design)
' @maturity: Production-ready, Battle-tested
' @evolution-stage: Active development, Continuous improvement
'
' === 关联 ===
' @related-files: See other architecture diagrams in the same directory
' @alternatives: Multiple implementation approaches available
' @comparison-with: Traditional monolithic vs distributed approaches
'
' === 实战 ===
' @real-world-examples: LinkedIn, Netflix, Uber, Airbnb
' @companies-using: LinkedIn, Netflix, Uber, Airbnb
' @production-readiness: Production-ready, Battle-tested at scale, Enterprise-grade
' ==================================================


!define PRIMARY_COLOR #4CAF50
!define SECONDARY_COLOR #2196F3
!define CACHE_COLOR #FF9800
!define STORAGE_COLOR #9C27B0
!define WORKER_COLOR #FFC107

skinparam backgroundColor #F5F5F5
skinparam defaultFontSize 14
skinparam roundcorner 15
skinparam shadowing false
allowmixing

title High Traffic Calendar System - Architecture Overview

' ========================================
' User Layer
' ========================================
actor User PRIMARY_COLOR

' ========================================
' Front End Layer
' ========================================
rectangle "Front End" as FrontEnd SECONDARY_COLOR {
    rectangle "Load Balancer" {
        component "Nginx/HAProxy" as LB
    }
    rectangle "API Gateway" as APIGateway {
        component "Create Event" as CreateAPI
        component "Update Event" as UpdateAPI
        component "Delete Event" as DeleteAPI
        component "Get Events" as GetAPI
    }
}

' ========================================
' Back End Layer
' ========================================
rectangle "Back End" as BackEnd PRIMARY_COLOR {
    rectangle "Web Server" as WebServer {
        component "Business Logic" as BizLogic
        component "Read Handler" as ReadHandler
        component "Write Handler" as WriteHandler
    }

    queue "Message Queue" as MQ WORKER_COLOR {
        component "Kafka" as Kafka
        queue "Write Events" as WriteEvents
        queue "Cache Invalidation" as CacheInvalidation
    }

    rectangle "Workers" as Workers WORKER_COLOR {
        component "Send Notification" as NotifyWorker
        component "Process Recurring Events" as RecurWorker
        component "Data Analysis & Reporting" as AnalyticsWorker
        component "Cache Update" as CacheWorker
        component "Search Index Update" as SearchWorker
    }

    rectangle "Worker Nodes" as WorkerNodes WORKER_COLOR {
        component "Kafka Consumer" as KafkaConsumer
    }

    rectangle "Prefetch Service" as PrefetchService #E1BEE7 {
        component "Data Analyzer" as DataAnalyzer
        component "Prefetch Worker" as PrefetchWorker
    }
}

' ========================================
' Data Storage Layer
' ========================================
package "Data Storage" as DataStorage STORAGE_COLOR {
    rectangle "Cache" as Cache CACHE_COLOR {
        component "Redis" as Redis
    }

    database "Databases" as Databases STORAGE_COLOR {
        component "Primary DB: MongoDB" as PrimaryDB
    }

    rectangle "MongoDB Cluster" as MongoCluster #7B1FA2 {
        component "Query Routers (mongos)" as Mongos
        component "Primary Node" as PrimaryNode
        component "Secondary Node 1" as Secondary1
        component "Secondary Node 2" as Secondary2
    }

    database "Search Engine" as SearchEngine #673AB7 {
        component "Elasticsearch" as ES
    }
}

' ========================================
' User to Front End Flow
' ========================================
User -[PRIMARY_COLOR,thickness=2]-> LB : <back:#FFFFFF><color:PRIMARY_COLOR>1. Request</color></back>
LB -[SECONDARY_COLOR,thickness=2]-> APIGateway : <back:#FFFFFF><color:SECONDARY_COLOR>2. Route</color></back>

' ========================================
' API Gateway to Web Server
' ========================================
APIGateway -[PRIMARY_COLOR,thickness=2]-> WebServer : <back:#FFFFFF><color:PRIMARY_COLOR>3. Forward Request</color></back>

' ========================================
' Read Path (Optimized)
' ========================================
ReadHandler -[CACHE_COLOR,thickness=2]-> Redis : <back:#FFFFFF><color:CACHE_COLOR>4a. Check Cache</color></back>
Redis -[CACHE_COLOR,thickness=2]-> ReadHandler : <back:#FFFFFF><color:CACHE_COLOR>Cache Hit/Miss</color></back>
ReadHandler -[STORAGE_COLOR,thickness=2]-> Mongos : <back:#FFFFFF><color:STORAGE_COLOR>4b. Read Request (on miss)</color></back>
Mongos -[STORAGE_COLOR,thickness=2]-> Secondary1 : <back:#FFFFFF><color:STORAGE_COLOR>Distribute Read</color></back>
Mongos -[STORAGE_COLOR,thickness=2]-> Secondary2 : <back:#FFFFFF><color:STORAGE_COLOR>Distribute Read</color></back>

' ========================================
' Write Path (Async)
' ========================================
WriteHandler -[WORKER_COLOR,thickness=2]-> Kafka : <back:#FFFFFF><color:WORKER_COLOR>5a. Publish Write Event</color></back>
Kafka -[STORAGE_COLOR,thickness=2]-> PrimaryNode : <back:#FFFFFF><color:STORAGE_COLOR>5b. Consume Write Event</color></back>
PrimaryNode -[STORAGE_COLOR,thickness=1]-> Secondary1 : <back:#FFFFFF><color:STORAGE_COLOR>Replicate</color></back>
PrimaryNode -[STORAGE_COLOR,thickness=1]-> Secondary2 : <back:#FFFFFF><color:STORAGE_COLOR>Replicate</color></back>

' ========================================
' Cache Invalidation Flow
' ========================================
WriteHandler -[CACHE_COLOR,thickness=2]-> Kafka : <back:#FFFFFF><color:CACHE_COLOR>6. Publish Cache Invalidation</color></back>
Kafka -[CACHE_COLOR,thickness=2]-> Redis : <back:#FFFFFF><color:CACHE_COLOR>Consume and Invalidate</color></back>

' ========================================
' Worker Pool Flow
' ========================================
Kafka -[WORKER_COLOR,thickness=2]-> Workers : <back:#FFFFFF><color:WORKER_COLOR>7. Distribute Tasks</color></back>
Workers -[WORKER_COLOR,thickness=1]-> Kafka : <back:#FFFFFF><color:WORKER_COLOR>Results</color></back>

' ========================================
' Worker Nodes Flow
' ========================================
Kafka --> WorkerNodes : <back:#FFFFFF>Tasks</back>
WorkerNodes <--> PrimaryDB : <back:#FFFFFF>Query/Update</back>
WorkerNodes --> Redis : <back:#FFFFFF>Invalidate/Update</back>
WorkerNodes --> ES : <back:#FFFFFF>Update Index</back>

' ========================================
' Prefetch Service Flow
' ========================================
PrefetchService -[#E1BEE7,thickness=2]-> MongoCluster : <back:#FFFFFF><color:#E1BEE7>8a. Analyze Data Patterns</color></back>
PrefetchService -[#E1BEE7,thickness=2]-> Redis : <back:#FFFFFF><color:#E1BEE7>8b. Prefetch Likely Needed Data</color></back>

' ========================================
' Search Integration
' ========================================
BizLogic --> ES : <back:#FFFFFF>Search</back>

' ========================================
' Additional Connections
' ========================================
BizLogic <--> Redis : <back:#FFFFFF>Check/Update</back>
BizLogic <--> PrimaryDB : <back:#FFFFFF>Query/Update</back>

' ========================================
' Feature Notes
' ========================================
note right of WebServer
  <b>Handles core business logic:</b>
  - Read/Write separation (CQRS)
  - Interacts with cache first
  - Publishes messages to Kafka
  - Asynchronous write operations
end note

note right of Kafka
  <b>Enables event-driven architecture:</b>
  - Loose coupling
  - High-throughput event streaming
  - Asynchronous processing
  - Message persistence
  - Retry mechanism
end note

note right of Redis
  <b>Caching Strategy:</b>
  - Read-through cache
  - Cache-aside pattern
  - Automatic invalidation via Kafka
  - TTL-based expiration
  - Hot data prefetching
end note

note right of MongoCluster
  <b>Read/Write Optimization:</b>
  - Reads from secondary nodes
  - Writes to primary node
  - Automatic replication
  - Horizontal scalability
  - Sharding support
end note

note bottom of PrefetchService
  <b>Intelligent Prefetching:</b>
  - Analyzes user access patterns
  - Prefetches likely needed data
  - Reduces cache misses
  - Improves response time
  - Machine learning based
end note

note left of Workers
  <b>Worker Pool Functions:</b>
  - Notification sending
  - Recurring event processing
  - Analytics and reporting
  - Cache updates
  - Search index updates
  - All asynchronous
end note

' ========================================
' Legend
' ========================================
legend right
<b>Architecture Highlights:</b>
==
<b>Request Flow:</b>
1. User request → Load Balancer
2. Load Balancer → API Gateway
3. API Gateway → Web Server
4. Read: Cache → DB (if miss)
5. Write: Kafka → MongoDB → Cache Invalidation

<b>Optimization Strategies:</b>
- CQRS: Read/Write separation
- Cache-aside pattern
- Asynchronous writes via Kafka
- Read from secondary nodes
- Write to primary node only
- Intelligent prefetching

<b>Scalability Features:</b>
- Load balancing
- MongoDB sharding & replication
- Redis clustering
- Kafka partitioning
- Worker pool scaling
- Horizontal scaling

<b>Performance Features:</b>
- Multi-layer caching
- Prefetch service
- Async processing
- Read replicas
- Search indexing

<b>Components:</b>
- <color:PRIMARY_COLOR>Primary Flow</color>
- <color:SECONDARY_COLOR>API Gateway</color>
- <color:CACHE_COLOR>Cache Layer</color>
- <color:STORAGE_COLOR>Storage Layer</color>
- <color:WORKER_COLOR>Worker Pool</color>
end legend

@enduml
